{
  "metadata": {
    "date": "2025-10-23T01:51:52.144102",
    "project": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local",
    "index_size": 1124,
    "model": "BAAI/bge-m3",
    "k": 10,
    "multi_hop_config": {
      "hops": 2,
      "expansion": 0.3
    }
  },
  "queries": [
    {
      "query": "search algorithm implementation",
      "single_hop": {
        "time_ms": 2327.45,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "search\\searcher.py:123-165:method:_semantic_search",
            "score": 6.087,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\searcher.py",
              "relative_path": "search\\searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 123,
              "end_line": 165,
              "name": "_semantic_search",
              "parent_name": "IntelligentSearcher",
              "docstring": "Pure semantic search implementation.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def _semantic_search(\n        self,\n        query: str,\n        k: int = 5,\n        context_depth: int = 1,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Pure semantic search implementation.\"\"\"\n\n        # Detect query intent and optimize\n        optimized_query = self._optimize_query(query)\n        intent_tags = self._detect_query_intent(query)\n\n        self._logger.info(\n            f\"Searching for: '{optimized_query}' with intent: {intent_tags}\"\n        )\n\n        # Generate query embedding\n        query_embedding = self.embedder.embed_query(optimized_query)\n\n        # Search with expanded result set for better filtering and recall\n        search_k = min(k * 10, 200)  # Increased from k*3 to k*10 for better recall\n        self._logger.info(\n            f\"Query embedding shape: {query_embedding.shape if hasattr(query_embedding, 'shape') else 'unknown'}\"\n        )\n        self._logger.info(f\"Using original filters: {filters}\")\n        self._logger.info(f\"Calling index_manager.search with k={search_k}\")\n\n        raw_results = self.index_manager.search(query_embedding, search_k, filters)\n        self._logger.info(f\"Index manager returned {len(raw_results)} raw results\")\n\n        # Convert to rich search results\n        search_results = []\n        for chunk_id, similarity, metadata in raw_results:\n            result = self._create_search_result(\n                chunk_id, similarity, metadata, context_depth\n            )\n            search_results.append(result)\n\n        # Post-process and rank results\n        ranked_results = self._rank_results(search_results, query, intent_tags)\n\n        return ranked_results[:k]",
              "content_preview": "def _semantic_search(\n        self,\n        query: str,\n        k: int = 5,\n        context_depth: int = 1,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Pu...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015989911727616647,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "search\\searcher.py:33-478:class:IntelligentSearcher",
            "score": 4.105,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\searcher.py",
              "relative_path": "search\\searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "class",
              "start_line": 33,
              "end_line": 478,
              "name": "IntelligentSearcher",
              "parent_name": null,
              "docstring": "Intelligent code search with query optimization and context awareness.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class IntelligentSearcher:\n    \"\"\"Intelligent code search with query optimization and context awareness.\"\"\"\n\n    def __init__(self, index_manager: CodeIndexManager, embedder: CodeEmbedder):\n        self.index_manager = index_manager\n        self.embedder = embedder\n        self._logger = logging.getLogger(__name__)\n\n        # Query patterns for intent detection\n        self.query_patterns = {\n            \"function_search\": [\n                r\"\\bfunction\\b\",\n                r\"\\bdef\\b\",\n                r\"\\bmethod\\b\",\n                r\"\\bclass\\b\",\n                r\"how.*work\",\n                r\"implement.*\",\n                r\"algorithm.*\",\n            ],\n            \"error_handling\": [\n                r\"\\berror\\b\",\n                r\"\\bexception\\b\",\n                r\"\\btry\\b\",\n                r\"\\bcatch\\b\",\n                r\"handle.*error\",\n                r\"exception.*handling\",\n            ],\n            \"database\": [\n                r\"\\bdatabase\\b\",\n                r\"\\bdb\\b\",\n                r\"\\bquery\\b\",\n                r\"\\bsql\\b\",\n                r\"\\bmodel\\b\",\n                r\"\\btable\\b\",\n                r\"connection\",\n            ],\n            \"api\": [\n                r\"\\bapi\\b\",\n                r\"\\bendpoint\\b\",\n                r\"\\broute\\b\",\n                r\"\\brequest\\b\",\n                r\"\\bresponse\\b\",\n                r\"\\bhttp\\b\",\n                r\"rest.*api\",\n            ],\n            \"authentication\": [\n                r\"\\bauth\\b\",\n                r\"\\blogin\\b\",\n                r\"\\btoken\\b\",\n                r\"\\bpassword\\b\",\n                r\"\\bsession\\b\",\n                r\"authenticate\",\n                r\"permission\",\n            ],\n            \"testing\": [\n                r\"\\btest\\b\",\n                r\"\\bmock\\b\",\n                r\"\\bassert\\b\",\n                r\"\\bfixture\\b\",\n                r\"unit.*test\",\n                r\"integration.*test\",\n            ],\n        }\n\n    def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"semantic\",\n        context_depth: int = 1,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Semantic search for code understanding.\n\n        This provides semantic search capabilities. For complete search coverage:\n        - Use this tool for conceptual/functionality queries\n        - Use Claude Code's Grep for exact term matching\n        - Combine both for comprehensive results\n\n        Args:\n            query: Natural language query\n            k: Number of results\n            search_mode: Currently \"semantic\" only\n            context_depth: Include related chunks\n            filters: Optional filters\n        \"\"\"\n\n        # Focus on semantic search - our specialty\n        return self._semantic_search(query, k, context_depth, filters)\n\n    def _semantic_search(\n        self,\n        query: str,\n        k: int = 5,\n        context_depth: int = 1,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Pure semantic search implementation.\"\"\"\n\n        # Detect query intent and optimize\n        optimized_query = self._optimize_query(query)\n        intent_tags = self._detect_query_intent(query)\n\n        self._logger.info(\n            f\"Searching for: '{optimized_query}' with intent: {intent_tags}\"\n        )\n\n        # Generate query embedding\n        query_embedding = self.embedder.embed_query(optimized_query)\n\n        # Search with expanded result set for better filtering and recall\n        search_k = min(k * 10, 200)  # Increased from k*3 to k*10 for better recall\n        self._logger.info(\n            f\"Query embedding shape: {query_embedding.shape if hasattr(query_embedding, 'shape') else 'unknown'}\"\n        )\n        self._logger.info(f\"Using original filters: {filters}\")\n        self._logger.info(f\"Calling index_manager.search with k={search_k}\")\n\n        raw_results = self.index_manager.search(query_embedding, search_k, filters)\n        self._logger.info(f\"Index manager returned {len(raw_results)} raw results\")\n\n        # Convert to rich search results\n        search_results = []\n        for chunk_id, similarity, metadata in raw_results:\n            result = self._create_search_result(\n                chunk_id, similarity, metadata, context_depth\n            )\n            search_results.append(result)\n\n        # Post-process and rank results\n        ranked_results = self._rank_results(search_results, query, intent_tags)\n\n        return ranked_results[:k]\n\n    def _optimize_query(self, query: str) -> str:\n        \"\"\"Optimize query for better embedding generation.\"\"\"\n        # Basic query cleaning only - avoid expanding technical terms\n        # that might distort code-specific queries\n        return query.strip()\n\n    def _detect_query_intent(self, query: str) -> List[str]:\n        \"\"\"Detect the intent/domain of the search query.\"\"\"\n        query_lower = query.lower()\n        detected_intents = []\n\n        for intent, patterns in self.query_patterns.items():\n            for pattern in patterns:\n                if re.search(pattern, query_lower):\n                    detected_intents.append(intent)\n                    break\n\n        return detected_intents\n\n    def _create_search_result(\n        self,\n        chunk_id: str,\n        similarity: float,\n        metadata: Dict[str, Any],\n        context_depth: int,\n    ) -> SearchResult:\n        \"\"\"Create a rich search result with context information.\"\"\"\n\n        # Basic metadata extraction\n        content_preview = metadata.get(\"content_preview\", \"\")\n        file_path = metadata.get(\"file_path\", \"\")\n        relative_path = metadata.get(\"relative_path\", \"\")\n        folder_structure = metadata.get(\"folder_structure\", [])\n\n        # Context information\n        context_info = {}\n\n        if context_depth > 0:\n            # Add related chunks context\n            similar_chunks = self.index_manager.get_similar_chunks(chunk_id, k=3)\n            context_info[\"similar_chunks\"] = [\n                {\n                    \"chunk_id\": cid,\n                    \"similarity\": sim,\n                    \"name\": meta.get(\"name\"),\n                    \"chunk_type\": meta.get(\"chunk_type\"),\n                }\n                for cid, sim, meta in similar_chunks[:2]  # Top 2 similar\n            ]\n\n            # Add file context\n            context_info[\"file_context\"] = {\n                \"total_chunks_in_file\": self._count_chunks_in_file(relative_path),\n                \"folder_path\": \"/\".join(folder_structure) if folder_structure else None,\n            }\n\n        return SearchResult(\n            chunk_id=chunk_id,\n            similarity_score=similarity,\n            content_preview=content_preview,\n            file_path=file_path,\n            relative_path=relative_path,\n            folder_structure=folder_structure,\n            chunk_type=metadata.get(\"chunk_type\", \"unknown\"),\n            name=metadata.get(\"name\"),\n            parent_name=metadata.get(\"parent_name\"),\n            start_line=metadata.get(\"start_line\", 0),\n            end_line=metadata.get(\"end_line\", 0),\n            docstring=metadata.get(\"docstring\"),\n            tags=metadata.get(\"tags\", []),\n            context_info=context_info,\n        )\n\n    def _count_chunks_in_file(self, relative_path: str) -> int:\n        \"\"\"Count total chunks in a specific file.\"\"\"\n        stats = self.index_manager.get_stats()\n\n        # This is a simplified implementation\n        # In a real scenario, you might want to maintain this as a separate index\n        return stats.get(\"files_indexed\", 0)\n\n    def _rank_results(\n        self, results: List[SearchResult], original_query: str, intent_tags: List[str]\n    ) -> List[SearchResult]:\n        \"\"\"Advanced ranking based on multiple factors.\"\"\"\n\n        def calculate_rank_score(result: SearchResult) -> float:\n            score = result.similarity_score\n\n            # Detect if query looks like an entity/class name\n            query_tokens = self._normalize_to_tokens(original_query.lower())\n            is_entity_query = self._is_entity_like_query(original_query, query_tokens)\n            has_class_keyword = \"class\" in original_query.lower()\n\n            # Dynamic chunk type boosts based on query type\n            if has_class_keyword:\n                # Strong preference for classes when \"class\" is mentioned\n                type_boosts = {\n                    \"class\": 1.3,\n                    \"function\": 1.05,\n                    \"method\": 1.05,\n                    \"module\": 0.9,\n                }\n            elif is_entity_query:\n                # Moderate preference for classes on entity-like queries\n                type_boosts = {\n                    \"class\": 1.15,\n                    \"function\": 1.1,\n                    \"method\": 1.1,\n                    \"module\": 0.92,\n                }\n            else:\n                # Default boosts for general queries\n                type_boosts = {\n                    \"function\": 1.1,\n                    \"method\": 1.1,\n                    \"class\": 1.05,\n                    \"module\": 0.95,\n                }\n\n            score *= type_boosts.get(result.chunk_type, 1.0)\n\n            # Enhanced name matching with token-based comparison\n            name_boost = self._calculate_name_boost(\n                result.name, original_query, query_tokens\n            )\n            score *= name_boost\n\n            # Path/filename relevance boost\n            path_boost = self._calculate_path_boost(result.relative_path, query_tokens)\n            score *= path_boost\n\n            # Boost based on tag matches\n            if intent_tags and result.tags:\n                tag_overlap = len(set(intent_tags) & set(result.tags))\n                score *= 1.0 + tag_overlap * 0.1\n\n            # Boost based on docstring presence (but less for module chunks on entity queries)\n            if result.docstring:\n                if is_entity_query and result.chunk_type == \"module\":\n                    score *= (\n                        1.02  # Smaller boost for module docstrings on entity queries\n                    )\n                else:\n                    score *= 1.05\n\n            # Slight penalty for very complex chunks (might be too specific)\n            if len(result.content_preview) > 1000:\n                score *= 0.98\n\n            return score\n\n        # Sort by calculated rank score\n        ranked_results = sorted(results, key=calculate_rank_score, reverse=True)\n        return ranked_results\n\n    def _normalize_to_tokens(self, text: str) -> List[str]:\n        \"\"\"Convert text to normalized tokens, handling CamelCase.\"\"\"\n        import re\n\n        # Split CamelCase and snake_case\n        text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text)\n        text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n\n        # Extract alphanumeric tokens\n        tokens = re.findall(r\"\\w+\", text.lower())\n        return tokens\n\n    def _is_entity_like_query(self, query: str, query_tokens: List[str]) -> bool:\n        \"\"\"Detect if query looks like an entity/type name.\"\"\"\n        # Short queries with 1-3 tokens that don't contain action words\n        if len(query_tokens) > 3:\n            return False\n\n        action_words = {\n            \"find\",\n            \"search\",\n            \"get\",\n            \"show\",\n            \"list\",\n            \"how\",\n            \"what\",\n            \"where\",\n            \"when\",\n            \"create\",\n            \"build\",\n            \"make\",\n            \"handle\",\n            \"process\",\n            \"manage\",\n            \"implement\",\n        }\n\n        # If any token is an action word, it's not an entity query\n        if any(token in action_words for token in query_tokens):\n            return False\n\n        # If original query has CamelCase or looks like a class name, it's entity-like\n        import re\n\n        if re.search(r\"[A-Z][a-z]+[A-Z]\", query):  # CamelCase pattern\n            return True\n\n        return len(query_tokens) <= 2  # Short noun phrases\n\n    def _calculate_name_boost(\n        self, name: Optional[str], original_query: str, query_tokens: List[str]\n    ) -> float:\n        \"\"\"Calculate boost based on name matching with robust token comparison.\"\"\"\n        if not name:\n            return 1.0\n\n        name_tokens = self._normalize_to_tokens(name)\n\n        # Exact match (case insensitive)\n        if original_query.lower() == name.lower():\n            return 1.4\n\n        # Token overlap calculation\n        query_set = set(query_tokens)\n        name_set = set(name_tokens)\n\n        if not query_set or not name_set:\n            return 1.0\n\n        overlap = len(query_set & name_set)\n        total_query_tokens = len(query_set)\n\n        if overlap == 0:\n            return 1.0\n\n        # Strong boost for high overlap\n        overlap_ratio = overlap / total_query_tokens\n        if overlap_ratio >= 0.8:  # 80%+ of query tokens match\n            return 1.3\n        elif overlap_ratio >= 0.5:  # 50%+ match\n            return 1.2\n        elif overlap_ratio >= 0.3:  # 30%+ match\n            return 1.1\n        else:\n            return 1.05\n\n    def _calculate_path_boost(\n        self, relative_path: str, query_tokens: List[str]\n    ) -> float:\n        \"\"\"Calculate boost based on path/filename relevance.\"\"\"\n        if not relative_path or not query_tokens:\n            return 1.0\n\n        # Extract path components and filename\n        path_parts = relative_path.lower().replace(\"/\", \" \").replace(\"\\\\\", \" \")\n        path_tokens = self._normalize_to_tokens(path_parts)\n\n        # Check for token overlap with path\n        query_set = set(query_tokens)\n        path_set = set(path_tokens)\n\n        overlap = len(query_set & path_set)\n        if overlap > 0:\n            # Modest boost for path relevance\n            return 1.0 + (overlap * 0.05)  # 5% boost per matching token\n\n        return 1.0\n\n    def search_by_file_pattern(\n        self, query: str, file_patterns: List[str], k: int = 5\n    ) -> List[SearchResult]:\n        \"\"\"Search within specific file patterns.\"\"\"\n        filters = {\"file_pattern\": file_patterns}\n        return self.search(query, k=k, filters=filters)\n\n    def search_by_chunk_type(\n        self, query: str, chunk_type: str, k: int = 5\n    ) -> List[SearchResult]:\n        \"\"\"Search for specific types of code chunks.\"\"\"\n        filters = {\"chunk_type\": chunk_type}\n        return self.search(query, k=k, filters=filters)\n\n    def find_similar_to_chunk(self, chunk_id: str, k: int = 5) -> List[SearchResult]:\n        \"\"\"Find chunks similar to a given chunk.\"\"\"\n        similar_chunks = self.index_manager.get_similar_chunks(chunk_id, k)\n\n        results = []\n        for chunk_id, similarity, metadata in similar_chunks:\n            result = self._create_search_result(\n                chunk_id, similarity, metadata, context_depth=1\n            )\n            results.append(result)\n\n        return results\n\n    def get_search_suggestions(self, partial_query: str) -> List[str]:\n        \"\"\"Generate search suggestions based on indexed content.\"\"\"\n        # This is a simplified implementation\n        # In a full system, you might maintain a separate suggestions index\n\n        suggestions = []\n        stats = self.index_manager.get_stats()\n\n        # Suggest based on top tags\n        top_tags = stats.get(\"top_tags\", {})\n        for tag in top_tags:\n            if partial_query.lower() in tag.lower():\n                suggestions.append(f\"Find {tag} related code\")\n\n        # Suggest based on chunk types\n        chunk_types = stats.get(\"chunk_types\", {})\n        for chunk_type in chunk_types:\n            if partial_query.lower() in chunk_type.lower():\n                suggestions.append(f\"Show all {chunk_type}s\")\n\n        return suggestions[:5]",
              "content_preview": "class IntelligentSearcher:\n    \"\"\"Intelligent code search with query optimization and context awareness.\"\"\"\n\n    def __init__(self, index_manager: CodeIndexManager, embedder: CodeEmbedder):\n        se...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014457332228666114,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\integration\\test_hybrid_search_integration.py:28-556:class:TestHybridSearchIntegration",
            "score": 2.676,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_hybrid_search_integration.py",
              "relative_path": "tests\\integration\\test_hybrid_search_integration.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "class",
              "start_line": 28,
              "end_line": 556,
              "name": "TestHybridSearchIntegration",
              "parent_name": null,
              "docstring": "Integration tests for hybrid search system.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestHybridSearchIntegration:\n    \"\"\"Integration tests for hybrid search system.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment with real components.\"\"\"\n        self.temp_dir = Path(tempfile.mkdtemp())\n        self.project_dir = self.temp_dir / \"test_project\"\n        self.storage_dir = self.temp_dir / \"storage\"\n\n        # Create test project structure\n        self.project_dir.mkdir(parents=True)\n        self.storage_dir.mkdir(parents=True)\n\n        # Create test files\n        self.create_test_files()\n\n        # Initialize components\n        self.embedder = None\n        self.chunker = None\n        self.hybrid_searcher = None\n        self.incremental_indexer = None\n\n    def create_test_files(self):\n        \"\"\"Create test Python files for indexing.\"\"\"\n        test_files = {\n            \"calculator.py\": '''\ndef add_numbers(a, b):\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n\ndef multiply_numbers(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\nclass Calculator:\n    \"\"\"A simple calculator class.\"\"\"\n\n    def __init__(self):\n        self.history = []\n\n    def calculate_sum(self, numbers):\n        \"\"\"Calculate sum of a list of numbers.\"\"\"\n        result = sum(numbers)\n        self.history.append(f\"sum({numbers}) = {result}\")\n        return result\n''',\n            \"user_manager.py\": '''\nclass UserManager:\n    \"\"\"Manages user operations.\"\"\"\n\n    def __init__(self):\n        self.users = {}\n\n    def get_user(self, user_id):\n        \"\"\"Retrieve user by ID.\"\"\"\n        return self.users.get(user_id)\n\n    def authenticate_user(self, username, password):\n        \"\"\"Authenticate a user with username and password.\"\"\"\n        user = self.find_user_by_username(username)\n        if user and user.check_password(password):\n            return user\n        return None\n\n    def find_user_by_username(self, username):\n        \"\"\"Find user by username.\"\"\"\n        for user in self.users.values():\n            if user.username == username:\n                return user\n        return None\n''',\n            \"api_client.py\": '''\nimport asyncio\nfrom typing import Optional, Dict, Any\n\nasync def fetch_data(url: str, headers: Optional[Dict] = None) -> Dict[Any, Any]:\n    \"\"\"Fetch data from API endpoint.\"\"\"\n    # Simulate API call\n    await asyncio.sleep(0.1)\n    return {\"status\": \"success\", \"data\": {}}\n\ndef handle_api_error(error: Exception) -> Dict[str, Any]:\n    \"\"\"Handle API errors gracefully.\"\"\"\n    return {\n        \"error\": str(error),\n        \"status\": \"failed\",\n        \"retry\": True\n    }\n\nclass APIClient:\n    \"\"\"HTTP API client for external services.\"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n        self.session = None\n\n    async def get(self, endpoint: str) -> Dict[Any, Any]:\n        \"\"\"GET request to API endpoint.\"\"\"\n        url = f\"{self.base_url}{endpoint}\"\n        return await fetch_data(url)\n''',\n            \"database.py\": '''\nimport sqlite3\nfrom typing import List, Dict, Any\n\nclass DatabaseConnection:\n    \"\"\"Database connection manager.\"\"\"\n\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.connection = None\n\n    def connect(self):\n        \"\"\"Establish database connection.\"\"\"\n        self.connection = sqlite3.connect(self.db_path)\n        return self.connection\n\n    def execute_query(self, query: str, params: tuple = ()) -> List[Dict[str, Any]]:\n        \"\"\"Execute SQL query and return results.\"\"\"\n        if not self.connection:\n            self.connect()\n\n        cursor = self.connection.cursor()\n        cursor.execute(query, params)\n\n        columns = [description[0] for description in cursor.description]\n        results = []\n        for row in cursor.fetchall():\n            results.append(dict(zip(columns, row)))\n\n        return results\n''',\n        }\n\n        for filename, content in test_files.items():\n            file_path = self.project_dir / filename\n            file_path.write_text(content)\n\n    def initialize_components(self):\n        \"\"\"Initialize all components for testing.\"\"\"\n        try:\n            # Initialize embedder\n            self.embedder = CodeEmbedder()\n\n            # Initialize chunker\n            self.chunker = MultiLanguageChunker(str(self.project_dir))\n\n            # Initialize hybrid searcher\n            self.hybrid_searcher = HybridSearcher(\n                storage_dir=str(self.storage_dir), bm25_weight=0.4, dense_weight=0.6\n            )\n\n            # Initialize incremental indexer\n            # This should work with HybridSearcher as indexer\n            self.incremental_indexer = IncrementalIndexer(\n                indexer=self.hybrid_searcher,\n                embedder=self.embedder,\n                chunker=self.chunker,\n            )\n\n        except Exception as e:\n            pytest.skip(f\"Could not initialize components: {e}\")\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_searcher_has_add_embeddings_method(self):\n        \"\"\"Test that HybridSearcher has add_embeddings method.\"\"\"\n        self.initialize_components()\n\n        # This should not fail\n        assert hasattr(\n            self.hybrid_searcher, \"add_embeddings\"\n        ), \"HybridSearcher missing add_embeddings method required by incremental indexer\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_incremental_indexing_with_hybrid_search(self):\n        \"\"\"Test that incremental indexing works with hybrid search.\"\"\"\n        self.initialize_components()\n\n        # Attempt to index the project\n        try:\n            result = self.incremental_indexer.incremental_index(\n                str(self.project_dir), project_name=\"test_project\", force_full=True\n            )\n\n            assert result.success, f\"Indexing failed: {result.error}\"\n            assert result.chunks_added > 0, \"No chunks were added to the index\"\n\n        except AttributeError as e:\n            if \"add_embeddings\" in str(e):\n                pytest.fail(\"HybridSearcher missing add_embeddings method\")\n            else:\n                raise\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_indices_are_populated(self):\n        \"\"\"Test that both BM25 and dense indices are populated after indexing.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n\n        assert result.success, \"Indexing must succeed for this test\"\n\n        # Check that HybridSearcher is ready (both indices populated)\n        assert (\n            self.hybrid_searcher.is_ready\n        ), \"HybridSearcher should be ready after indexing (both BM25 and dense indices populated)\"\n\n        # Check BM25 index specifically\n        assert (\n            not self.hybrid_searcher.bm25_index.is_empty\n        ), \"BM25 index should not be empty after indexing\"\n\n        # Check dense index specifically\n        assert (\n            self.hybrid_searcher.dense_index.index is not None\n        ), \"Dense index should exist after indexing\"\n        assert (\n            self.hybrid_searcher.dense_index.index.ntotal > 0\n        ), \"Dense index should contain vectors after indexing\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_search_returns_results(self):\n        \"\"\"Test that hybrid search returns results from both indices.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        # Test search queries that should favor different indices\n        queries_to_test = [\n            # Should favor BM25 (exact text matches)\n            \"def add_numbers\",\n            \"UserManager\",\n            \"sqlite3\",\n            # Should favor dense/semantic (conceptual matches)\n            \"database connection\",\n            \"user authentication\",\n            \"API client\",\n            \"error handling\",\n            \"calculate numbers\",\n        ]\n\n        for query in queries_to_test:\n            results = self.hybrid_searcher.search(query, k=5)\n            assert len(results) > 0, f\"No results found for query: '{query}'\"\n\n            # Check that results have the expected format\n            for result in results:\n                assert hasattr(result, \"doc_id\"), \"Result missing doc_id\"\n                assert hasattr(result, \"score\"), \"Result missing score\"\n                assert hasattr(result, \"metadata\"), \"Result missing metadata\"\n                assert (\n                    result.score > 0\n                ), f\"Result score should be positive: {result.score}\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_bm25_vs_dense_results_differ(self):\n        \"\"\"Test that BM25-only and dense-only searches return different results.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        # Test a query that should show different results for BM25 vs dense\n        query = \"user authentication\"\n\n        # Get BM25-only results\n        bm25_results = self.hybrid_searcher._search_bm25(query, k=5, min_score=0.0)\n\n        # Get dense-only results\n        dense_results = self.hybrid_searcher._search_dense(query, k=5, filters=None)\n\n        # Both should return results\n        assert len(bm25_results) > 0, \"BM25 search should return results\"\n        assert len(dense_results) > 0, \"Dense search should return results\"\n\n        # Extract doc IDs for comparison\n        bm25_doc_ids = [doc_id for doc_id, _, _ in bm25_results]\n        dense_doc_ids = [doc_id for doc_id, _, _ in dense_results]\n\n        # The results should be different (different ranking/selection)\n        # This tests that both indices are contributing to the search\n        assert (\n            bm25_doc_ids != dense_doc_ids\n        ), \"BM25 and dense search should return different results for semantic queries\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_reranking_combines_results(self):\n        \"\"\"Test that hybrid reranking properly combines BM25 and dense results.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        query = \"calculate sum\"\n\n        # Get results from both searches\n        bm25_results = self.hybrid_searcher._search_bm25(query, k=10, min_score=0.0)\n        dense_results = self.hybrid_searcher._search_dense(query, k=10, filters=None)\n\n        # Get hybrid results\n        hybrid_results = self.hybrid_searcher.search(query, k=5, use_parallel=False)\n\n        # Hybrid results should exist\n        assert len(hybrid_results) > 0, \"Hybrid search should return results\"\n\n        # Check that hybrid results contain documents from both searches\n        hybrid_doc_ids = {result.doc_id for result in hybrid_results}\n        bm25_doc_ids = {doc_id for doc_id, _, _ in bm25_results}\n        dense_doc_ids = {doc_id for doc_id, _, _ in dense_results}\n\n        # At least some hybrid results should come from BM25 or dense\n        assert (\n            len(hybrid_doc_ids & bm25_doc_ids) > 0\n            or len(hybrid_doc_ids & dense_doc_ids) > 0\n        ), \"Hybrid results should include documents from BM25 or dense searches\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_parallel_vs_sequential_search(self):\n        \"\"\"Test that parallel and sequential search modes work and return similar results.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        query = \"database connection\"\n\n        # Get results with both modes\n        parallel_results = self.hybrid_searcher.search(query, k=5, use_parallel=True)\n        sequential_results = self.hybrid_searcher.search(query, k=5, use_parallel=False)\n\n        # Both should return results\n        assert len(parallel_results) > 0, \"Parallel search should return results\"\n        assert len(sequential_results) > 0, \"Sequential search should return results\"\n\n        # Results should be similar (same reranking algorithm)\n        parallel_doc_ids = [r.doc_id for r in parallel_results]\n        sequential_doc_ids = [r.doc_id for r in sequential_results]\n\n        # Allow for some differences due to threading, but expect significant overlap\n        overlap = len(set(parallel_doc_ids) & set(sequential_doc_ids))\n        assert (\n            overlap >= len(parallel_results) // 2\n        ), \"Parallel and sequential search should have significant overlap in results\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_index_persistence(self):\n        \"\"\"Test that hybrid indices persist across searcher instances.\"\"\"\n        self.initialize_components()\n\n        # Index with first searcher instance\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Initial indexing must succeed\"\n\n        # Save indices\n        self.hybrid_searcher.save_indices()\n\n        # Create new searcher instance\n        new_searcher = HybridSearcher(\n            storage_dir=str(self.storage_dir), bm25_weight=0.4, dense_weight=0.6\n        )\n\n        # Load indices\n        load_success = new_searcher.load_indices()\n        assert load_success, \"Loading indices should succeed\"\n\n        # Verify the new searcher is ready\n        assert (\n            new_searcher.is_ready\n        ), \"New searcher should be ready after loading indices\"\n\n        # Test that search works with loaded indices\n        query = \"calculate sum\"\n        results = new_searcher.search(query, k=3)\n        assert len(results) > 0, \"Search should work with loaded indices\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_incremental_updates(self):\n        \"\"\"Test that incremental updates work with hybrid search.\"\"\"\n        self.initialize_components()\n\n        # Initial indexing\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Initial indexing must succeed\"\n\n        # Add a new file\n        new_file = self.project_dir / \"new_module.py\"\n        new_file.write_text(\n            '''\ndef process_data(data_list):\n    \"\"\"Process a list of data items.\"\"\"\n    processed = []\n    for item in data_list:\n        if validate_item(item):\n            processed.append(transform_item(item))\n    return processed\n\ndef validate_item(item):\n    \"\"\"Validate a single data item.\"\"\"\n    return item is not None and len(str(item)) > 0\n'''\n        )\n\n        # Incremental update\n        update_result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=False\n        )\n\n        assert update_result.success, \"Incremental update must succeed\"\n        assert update_result.files_added > 0, \"New file should be detected\"\n        assert update_result.chunks_added > 0, \"New chunks should be added\"\n\n        # Verify that new content is searchable\n        results = self.hybrid_searcher.search(\"process data\", k=3)\n        assert len(results) > 0, \"New content should be searchable\"\n\n        # Check that at least one result is from the new file\n        new_file_results = [r for r in results if \"new_module.py\" in r.doc_id]\n        assert len(new_file_results) > 0, \"Should find results from new file\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_search_mode_configuration(self):\n        \"\"\"Test search mode configuration and switching.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        # Test weight adjustment\n        original_bm25_weight = self.hybrid_searcher.bm25_weight\n        original_dense_weight = self.hybrid_searcher.dense_weight\n\n        # Change weights\n        self.hybrid_searcher.bm25_weight = 0.8\n        self.hybrid_searcher.dense_weight = 0.2\n\n        # Search should still work\n        results = self.hybrid_searcher.search(\"user authentication\", k=3)\n        assert len(results) > 0, \"Search should work with different weights\"\n\n        # Restore weights\n        self.hybrid_searcher.bm25_weight = original_bm25_weight\n        self.hybrid_searcher.dense_weight = original_dense_weight\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_error_handling(self):\n        \"\"\"Test error handling in hybrid search system.\"\"\"\n        self.initialize_components()\n\n        # Test search on empty index\n        results = self.hybrid_searcher.search(\"test query\", k=5)\n        assert results == [], \"Search on empty index should return empty results\"\n\n        # Test with invalid query\n        results = self.hybrid_searcher.search(\"\", k=5)\n        assert isinstance(results, list), \"Empty query should return list\"\n\n        # Test with zero k\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        results = self.hybrid_searcher.search(\"test\", k=0)\n        assert len(results) == 0, \"k=0 should return no results\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_statistics_and_monitoring(self):\n        \"\"\"Test statistics collection and monitoring features.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        # Get initial stats\n        stats = self.hybrid_searcher.stats\n        assert \"bm25_stats\" in stats, \"Stats should include BM25 information\"\n        assert \"dense_stats\" in stats, \"Stats should include dense information\"\n        assert \"gpu_memory\" in stats, \"Stats should include GPU memory information\"\n\n        # Perform some searches to generate performance stats\n        queries = [\"calculate\", \"user\", \"database\", \"API\"]\n        for query in queries:\n            self.hybrid_searcher.search(query, k=3)\n\n        # Get search mode stats\n        search_stats = self.hybrid_searcher.get_search_mode_stats()\n        assert search_stats[\"total_searches\"] == len(\n            queries\n        ), \"Should track search count\"\n        assert \"average_times\" in search_stats, \"Should include timing information\"\n\n    def teardown_method(self):\n        \"\"\"Clean up test environment.\"\"\"\n        try:\n            if hasattr(self, \"hybrid_searcher\") and self.hybrid_searcher:\n                self.hybrid_searcher.shutdown()\n        except Exception:\n            pass\n\n        try:\n            shutil.rmtree(self.temp_dir, ignore_errors=True)\n        except Exception:\n            pass",
              "content_preview": "class TestHybridSearchIntegration:\n    \"\"\"Integration tests for hybrid search system.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment with real components.\"\"\"\n        self.temp_dir ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012955465587044534,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "search\\config.py:30-98:decorated_definition:SearchConfig",
            "score": 0.613,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\config.py",
              "relative_path": "search\\config.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 30,
              "end_line": 98,
              "name": "SearchConfig",
              "parent_name": null,
              "docstring": "Configuration for search behavior.",
              "decorators": [
                "@dataclass"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@dataclass\nclass SearchConfig:\n    \"\"\"Configuration for search behavior.\"\"\"\n\n    # Embedding Model Configuration\n    embedding_model_name: str = \"google/embeddinggemma-300m\"\n    model_dimension: int = 768\n    embedding_batch_size: int = 128  # Dynamic based on model, see MODEL_REGISTRY\n\n    # Search Mode Configuration\n    default_search_mode: str = \"hybrid\"  # hybrid, semantic, bm25, auto\n    enable_hybrid_search: bool = True\n\n    # Hybrid Search Weights\n    bm25_weight: float = 0.4\n    dense_weight: float = 0.6\n\n    # Performance Settings\n    use_parallel_search: bool = True\n    max_parallel_workers: int = 2\n\n    # BM25 Configuration\n    bm25_k_parameter: int = 100\n    bm25_use_stopwords: bool = True\n    min_bm25_score: float = 0.1\n\n    # Reranking Configuration\n    rrf_k_parameter: int = 100\n    enable_result_reranking: bool = True\n\n    # GPU Configuration\n    prefer_gpu: bool = True\n    gpu_memory_threshold: float = 0.8\n\n    # Auto-reindexing\n    enable_auto_reindex: bool = True\n    max_index_age_minutes: float = 5.0\n\n    # Multi-hop Search Configuration\n    enable_multi_hop: bool = True\n    multi_hop_count: int = 2\n    multi_hop_expansion: float = 0.3\n\n    # Search Result Limits\n    default_k: int = 5\n    max_k: int = 50\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"SearchConfig\":\n        \"\"\"Create from dictionary.\"\"\"\n        # Auto-update dimension and batch size if model is in registry\n        if \"embedding_model_name\" in data:\n            model_config = get_model_config(data[\"embedding_model_name\"])\n            if model_config:\n                data[\"model_dimension\"] = model_config[\"dimension\"]\n                # Only auto-set batch size if not explicitly provided\n                if \"embedding_batch_size\" not in data:\n                    data[\"embedding_batch_size\"] = model_config.get(\n                        \"recommended_batch_size\", 128\n                    )\n\n        # Filter only known fields to avoid TypeError\n        valid_fields = {f.name for f in cls.__dataclass_fields__.values()}\n        filtered_data = {k: v for k, v in data.items() if k in valid_fields}\n        return cls(**filtered_data)",
              "content_preview": "@dataclass\nclass SearchConfig:\n    \"\"\"Configuration for search behavior.\"\"\"\n\n    # Embedding Model Configuration\n    embedding_model_name: str = \"google/embeddinggemma-300m\"\n    model_dimension: int =...",
              "project_name": "claude-context-local",
              "rrf_score": 0.009677419354838708,
              "appears_in_lists": 1,
              "final_rank": 4
            }
          },
          {
            "doc_id": "search\\__init__.py:1-2:module",
            "score": 0.599,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\__init__.py",
              "relative_path": "search\\__init__.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "module",
              "start_line": 1,
              "end_line": 2,
              "name": null,
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "\"\"\"Search and indexing module.\"\"\"\n",
              "content_preview": "\"\"\"Search and indexing module.\"\"\"\n",
              "project_name": "claude-context-local",
              "rrf_score": 0.009523809523809523,
              "appears_in_lists": 1,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "tools\\auto_tune_search.py:83-266:function:main",
          "tools\\search_helper.py:184-236:method:search",
          "tests\\integration\\test_full_flow.py:174-214:method:test_real_search_scenarios",
          "search\\searcher.py:123-165:method:_semantic_search",
          "search\\hybrid_searcher.py:727-769:method:_search_dense",
          "search\\__init__.py:1-2:module",
          "tests\\integration\\test_hybrid_search_integration.py:559-621:class:TestHybridSearchConfigIntegration",
          "search\\config.py:30-98:decorated_definition:SearchConfig",
          "search\\searcher.py:33-478:class:IntelligentSearcher",
          "tests\\integration\\test_hybrid_search_integration.py:28-556:class:TestHybridSearchIntegration"
        ]
      },
      "multi_hop": {
        "time_ms": 74.34,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "search\\searcher.py:123-165:method:_semantic_search",
            "score": 6.087,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\searcher.py",
              "relative_path": "search\\searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 123,
              "end_line": 165,
              "name": "_semantic_search",
              "parent_name": "IntelligentSearcher",
              "docstring": "Pure semantic search implementation.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def _semantic_search(\n        self,\n        query: str,\n        k: int = 5,\n        context_depth: int = 1,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Pure semantic search implementation.\"\"\"\n\n        # Detect query intent and optimize\n        optimized_query = self._optimize_query(query)\n        intent_tags = self._detect_query_intent(query)\n\n        self._logger.info(\n            f\"Searching for: '{optimized_query}' with intent: {intent_tags}\"\n        )\n\n        # Generate query embedding\n        query_embedding = self.embedder.embed_query(optimized_query)\n\n        # Search with expanded result set for better filtering and recall\n        search_k = min(k * 10, 200)  # Increased from k*3 to k*10 for better recall\n        self._logger.info(\n            f\"Query embedding shape: {query_embedding.shape if hasattr(query_embedding, 'shape') else 'unknown'}\"\n        )\n        self._logger.info(f\"Using original filters: {filters}\")\n        self._logger.info(f\"Calling index_manager.search with k={search_k}\")\n\n        raw_results = self.index_manager.search(query_embedding, search_k, filters)\n        self._logger.info(f\"Index manager returned {len(raw_results)} raw results\")\n\n        # Convert to rich search results\n        search_results = []\n        for chunk_id, similarity, metadata in raw_results:\n            result = self._create_search_result(\n                chunk_id, similarity, metadata, context_depth\n            )\n            search_results.append(result)\n\n        # Post-process and rank results\n        ranked_results = self._rank_results(search_results, query, intent_tags)\n\n        return ranked_results[:k]",
              "content_preview": "def _semantic_search(\n        self,\n        query: str,\n        k: int = 5,\n        context_depth: int = 1,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Pu...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015989911727616647,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "search\\searcher.py:33-478:class:IntelligentSearcher",
            "score": 4.105,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\searcher.py",
              "relative_path": "search\\searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "class",
              "start_line": 33,
              "end_line": 478,
              "name": "IntelligentSearcher",
              "parent_name": null,
              "docstring": "Intelligent code search with query optimization and context awareness.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class IntelligentSearcher:\n    \"\"\"Intelligent code search with query optimization and context awareness.\"\"\"\n\n    def __init__(self, index_manager: CodeIndexManager, embedder: CodeEmbedder):\n        self.index_manager = index_manager\n        self.embedder = embedder\n        self._logger = logging.getLogger(__name__)\n\n        # Query patterns for intent detection\n        self.query_patterns = {\n            \"function_search\": [\n                r\"\\bfunction\\b\",\n                r\"\\bdef\\b\",\n                r\"\\bmethod\\b\",\n                r\"\\bclass\\b\",\n                r\"how.*work\",\n                r\"implement.*\",\n                r\"algorithm.*\",\n            ],\n            \"error_handling\": [\n                r\"\\berror\\b\",\n                r\"\\bexception\\b\",\n                r\"\\btry\\b\",\n                r\"\\bcatch\\b\",\n                r\"handle.*error\",\n                r\"exception.*handling\",\n            ],\n            \"database\": [\n                r\"\\bdatabase\\b\",\n                r\"\\bdb\\b\",\n                r\"\\bquery\\b\",\n                r\"\\bsql\\b\",\n                r\"\\bmodel\\b\",\n                r\"\\btable\\b\",\n                r\"connection\",\n            ],\n            \"api\": [\n                r\"\\bapi\\b\",\n                r\"\\bendpoint\\b\",\n                r\"\\broute\\b\",\n                r\"\\brequest\\b\",\n                r\"\\bresponse\\b\",\n                r\"\\bhttp\\b\",\n                r\"rest.*api\",\n            ],\n            \"authentication\": [\n                r\"\\bauth\\b\",\n                r\"\\blogin\\b\",\n                r\"\\btoken\\b\",\n                r\"\\bpassword\\b\",\n                r\"\\bsession\\b\",\n                r\"authenticate\",\n                r\"permission\",\n            ],\n            \"testing\": [\n                r\"\\btest\\b\",\n                r\"\\bmock\\b\",\n                r\"\\bassert\\b\",\n                r\"\\bfixture\\b\",\n                r\"unit.*test\",\n                r\"integration.*test\",\n            ],\n        }\n\n    def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"semantic\",\n        context_depth: int = 1,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Semantic search for code understanding.\n\n        This provides semantic search capabilities. For complete search coverage:\n        - Use this tool for conceptual/functionality queries\n        - Use Claude Code's Grep for exact term matching\n        - Combine both for comprehensive results\n\n        Args:\n            query: Natural language query\n            k: Number of results\n            search_mode: Currently \"semantic\" only\n            context_depth: Include related chunks\n            filters: Optional filters\n        \"\"\"\n\n        # Focus on semantic search - our specialty\n        return self._semantic_search(query, k, context_depth, filters)\n\n    def _semantic_search(\n        self,\n        query: str,\n        k: int = 5,\n        context_depth: int = 1,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Pure semantic search implementation.\"\"\"\n\n        # Detect query intent and optimize\n        optimized_query = self._optimize_query(query)\n        intent_tags = self._detect_query_intent(query)\n\n        self._logger.info(\n            f\"Searching for: '{optimized_query}' with intent: {intent_tags}\"\n        )\n\n        # Generate query embedding\n        query_embedding = self.embedder.embed_query(optimized_query)\n\n        # Search with expanded result set for better filtering and recall\n        search_k = min(k * 10, 200)  # Increased from k*3 to k*10 for better recall\n        self._logger.info(\n            f\"Query embedding shape: {query_embedding.shape if hasattr(query_embedding, 'shape') else 'unknown'}\"\n        )\n        self._logger.info(f\"Using original filters: {filters}\")\n        self._logger.info(f\"Calling index_manager.search with k={search_k}\")\n\n        raw_results = self.index_manager.search(query_embedding, search_k, filters)\n        self._logger.info(f\"Index manager returned {len(raw_results)} raw results\")\n\n        # Convert to rich search results\n        search_results = []\n        for chunk_id, similarity, metadata in raw_results:\n            result = self._create_search_result(\n                chunk_id, similarity, metadata, context_depth\n            )\n            search_results.append(result)\n\n        # Post-process and rank results\n        ranked_results = self._rank_results(search_results, query, intent_tags)\n\n        return ranked_results[:k]\n\n    def _optimize_query(self, query: str) -> str:\n        \"\"\"Optimize query for better embedding generation.\"\"\"\n        # Basic query cleaning only - avoid expanding technical terms\n        # that might distort code-specific queries\n        return query.strip()\n\n    def _detect_query_intent(self, query: str) -> List[str]:\n        \"\"\"Detect the intent/domain of the search query.\"\"\"\n        query_lower = query.lower()\n        detected_intents = []\n\n        for intent, patterns in self.query_patterns.items():\n            for pattern in patterns:\n                if re.search(pattern, query_lower):\n                    detected_intents.append(intent)\n                    break\n\n        return detected_intents\n\n    def _create_search_result(\n        self,\n        chunk_id: str,\n        similarity: float,\n        metadata: Dict[str, Any],\n        context_depth: int,\n    ) -> SearchResult:\n        \"\"\"Create a rich search result with context information.\"\"\"\n\n        # Basic metadata extraction\n        content_preview = metadata.get(\"content_preview\", \"\")\n        file_path = metadata.get(\"file_path\", \"\")\n        relative_path = metadata.get(\"relative_path\", \"\")\n        folder_structure = metadata.get(\"folder_structure\", [])\n\n        # Context information\n        context_info = {}\n\n        if context_depth > 0:\n            # Add related chunks context\n            similar_chunks = self.index_manager.get_similar_chunks(chunk_id, k=3)\n            context_info[\"similar_chunks\"] = [\n                {\n                    \"chunk_id\": cid,\n                    \"similarity\": sim,\n                    \"name\": meta.get(\"name\"),\n                    \"chunk_type\": meta.get(\"chunk_type\"),\n                }\n                for cid, sim, meta in similar_chunks[:2]  # Top 2 similar\n            ]\n\n            # Add file context\n            context_info[\"file_context\"] = {\n                \"total_chunks_in_file\": self._count_chunks_in_file(relative_path),\n                \"folder_path\": \"/\".join(folder_structure) if folder_structure else None,\n            }\n\n        return SearchResult(\n            chunk_id=chunk_id,\n            similarity_score=similarity,\n            content_preview=content_preview,\n            file_path=file_path,\n            relative_path=relative_path,\n            folder_structure=folder_structure,\n            chunk_type=metadata.get(\"chunk_type\", \"unknown\"),\n            name=metadata.get(\"name\"),\n            parent_name=metadata.get(\"parent_name\"),\n            start_line=metadata.get(\"start_line\", 0),\n            end_line=metadata.get(\"end_line\", 0),\n            docstring=metadata.get(\"docstring\"),\n            tags=metadata.get(\"tags\", []),\n            context_info=context_info,\n        )\n\n    def _count_chunks_in_file(self, relative_path: str) -> int:\n        \"\"\"Count total chunks in a specific file.\"\"\"\n        stats = self.index_manager.get_stats()\n\n        # This is a simplified implementation\n        # In a real scenario, you might want to maintain this as a separate index\n        return stats.get(\"files_indexed\", 0)\n\n    def _rank_results(\n        self, results: List[SearchResult], original_query: str, intent_tags: List[str]\n    ) -> List[SearchResult]:\n        \"\"\"Advanced ranking based on multiple factors.\"\"\"\n\n        def calculate_rank_score(result: SearchResult) -> float:\n            score = result.similarity_score\n\n            # Detect if query looks like an entity/class name\n            query_tokens = self._normalize_to_tokens(original_query.lower())\n            is_entity_query = self._is_entity_like_query(original_query, query_tokens)\n            has_class_keyword = \"class\" in original_query.lower()\n\n            # Dynamic chunk type boosts based on query type\n            if has_class_keyword:\n                # Strong preference for classes when \"class\" is mentioned\n                type_boosts = {\n                    \"class\": 1.3,\n                    \"function\": 1.05,\n                    \"method\": 1.05,\n                    \"module\": 0.9,\n                }\n            elif is_entity_query:\n                # Moderate preference for classes on entity-like queries\n                type_boosts = {\n                    \"class\": 1.15,\n                    \"function\": 1.1,\n                    \"method\": 1.1,\n                    \"module\": 0.92,\n                }\n            else:\n                # Default boosts for general queries\n                type_boosts = {\n                    \"function\": 1.1,\n                    \"method\": 1.1,\n                    \"class\": 1.05,\n                    \"module\": 0.95,\n                }\n\n            score *= type_boosts.get(result.chunk_type, 1.0)\n\n            # Enhanced name matching with token-based comparison\n            name_boost = self._calculate_name_boost(\n                result.name, original_query, query_tokens\n            )\n            score *= name_boost\n\n            # Path/filename relevance boost\n            path_boost = self._calculate_path_boost(result.relative_path, query_tokens)\n            score *= path_boost\n\n            # Boost based on tag matches\n            if intent_tags and result.tags:\n                tag_overlap = len(set(intent_tags) & set(result.tags))\n                score *= 1.0 + tag_overlap * 0.1\n\n            # Boost based on docstring presence (but less for module chunks on entity queries)\n            if result.docstring:\n                if is_entity_query and result.chunk_type == \"module\":\n                    score *= (\n                        1.02  # Smaller boost for module docstrings on entity queries\n                    )\n                else:\n                    score *= 1.05\n\n            # Slight penalty for very complex chunks (might be too specific)\n            if len(result.content_preview) > 1000:\n                score *= 0.98\n\n            return score\n\n        # Sort by calculated rank score\n        ranked_results = sorted(results, key=calculate_rank_score, reverse=True)\n        return ranked_results\n\n    def _normalize_to_tokens(self, text: str) -> List[str]:\n        \"\"\"Convert text to normalized tokens, handling CamelCase.\"\"\"\n        import re\n\n        # Split CamelCase and snake_case\n        text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text)\n        text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n\n        # Extract alphanumeric tokens\n        tokens = re.findall(r\"\\w+\", text.lower())\n        return tokens\n\n    def _is_entity_like_query(self, query: str, query_tokens: List[str]) -> bool:\n        \"\"\"Detect if query looks like an entity/type name.\"\"\"\n        # Short queries with 1-3 tokens that don't contain action words\n        if len(query_tokens) > 3:\n            return False\n\n        action_words = {\n            \"find\",\n            \"search\",\n            \"get\",\n            \"show\",\n            \"list\",\n            \"how\",\n            \"what\",\n            \"where\",\n            \"when\",\n            \"create\",\n            \"build\",\n            \"make\",\n            \"handle\",\n            \"process\",\n            \"manage\",\n            \"implement\",\n        }\n\n        # If any token is an action word, it's not an entity query\n        if any(token in action_words for token in query_tokens):\n            return False\n\n        # If original query has CamelCase or looks like a class name, it's entity-like\n        import re\n\n        if re.search(r\"[A-Z][a-z]+[A-Z]\", query):  # CamelCase pattern\n            return True\n\n        return len(query_tokens) <= 2  # Short noun phrases\n\n    def _calculate_name_boost(\n        self, name: Optional[str], original_query: str, query_tokens: List[str]\n    ) -> float:\n        \"\"\"Calculate boost based on name matching with robust token comparison.\"\"\"\n        if not name:\n            return 1.0\n\n        name_tokens = self._normalize_to_tokens(name)\n\n        # Exact match (case insensitive)\n        if original_query.lower() == name.lower():\n            return 1.4\n\n        # Token overlap calculation\n        query_set = set(query_tokens)\n        name_set = set(name_tokens)\n\n        if not query_set or not name_set:\n            return 1.0\n\n        overlap = len(query_set & name_set)\n        total_query_tokens = len(query_set)\n\n        if overlap == 0:\n            return 1.0\n\n        # Strong boost for high overlap\n        overlap_ratio = overlap / total_query_tokens\n        if overlap_ratio >= 0.8:  # 80%+ of query tokens match\n            return 1.3\n        elif overlap_ratio >= 0.5:  # 50%+ match\n            return 1.2\n        elif overlap_ratio >= 0.3:  # 30%+ match\n            return 1.1\n        else:\n            return 1.05\n\n    def _calculate_path_boost(\n        self, relative_path: str, query_tokens: List[str]\n    ) -> float:\n        \"\"\"Calculate boost based on path/filename relevance.\"\"\"\n        if not relative_path or not query_tokens:\n            return 1.0\n\n        # Extract path components and filename\n        path_parts = relative_path.lower().replace(\"/\", \" \").replace(\"\\\\\", \" \")\n        path_tokens = self._normalize_to_tokens(path_parts)\n\n        # Check for token overlap with path\n        query_set = set(query_tokens)\n        path_set = set(path_tokens)\n\n        overlap = len(query_set & path_set)\n        if overlap > 0:\n            # Modest boost for path relevance\n            return 1.0 + (overlap * 0.05)  # 5% boost per matching token\n\n        return 1.0\n\n    def search_by_file_pattern(\n        self, query: str, file_patterns: List[str], k: int = 5\n    ) -> List[SearchResult]:\n        \"\"\"Search within specific file patterns.\"\"\"\n        filters = {\"file_pattern\": file_patterns}\n        return self.search(query, k=k, filters=filters)\n\n    def search_by_chunk_type(\n        self, query: str, chunk_type: str, k: int = 5\n    ) -> List[SearchResult]:\n        \"\"\"Search for specific types of code chunks.\"\"\"\n        filters = {\"chunk_type\": chunk_type}\n        return self.search(query, k=k, filters=filters)\n\n    def find_similar_to_chunk(self, chunk_id: str, k: int = 5) -> List[SearchResult]:\n        \"\"\"Find chunks similar to a given chunk.\"\"\"\n        similar_chunks = self.index_manager.get_similar_chunks(chunk_id, k)\n\n        results = []\n        for chunk_id, similarity, metadata in similar_chunks:\n            result = self._create_search_result(\n                chunk_id, similarity, metadata, context_depth=1\n            )\n            results.append(result)\n\n        return results\n\n    def get_search_suggestions(self, partial_query: str) -> List[str]:\n        \"\"\"Generate search suggestions based on indexed content.\"\"\"\n        # This is a simplified implementation\n        # In a full system, you might maintain a separate suggestions index\n\n        suggestions = []\n        stats = self.index_manager.get_stats()\n\n        # Suggest based on top tags\n        top_tags = stats.get(\"top_tags\", {})\n        for tag in top_tags:\n            if partial_query.lower() in tag.lower():\n                suggestions.append(f\"Find {tag} related code\")\n\n        # Suggest based on chunk types\n        chunk_types = stats.get(\"chunk_types\", {})\n        for chunk_type in chunk_types:\n            if partial_query.lower() in chunk_type.lower():\n                suggestions.append(f\"Show all {chunk_type}s\")\n\n        return suggestions[:5]",
              "content_preview": "class IntelligentSearcher:\n    \"\"\"Intelligent code search with query optimization and context awareness.\"\"\"\n\n    def __init__(self, index_manager: CodeIndexManager, embedder: CodeEmbedder):\n        se...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014457332228666114,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\integration\\test_hybrid_search_integration.py:28-556:class:TestHybridSearchIntegration",
            "score": 2.676,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_hybrid_search_integration.py",
              "relative_path": "tests\\integration\\test_hybrid_search_integration.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "class",
              "start_line": 28,
              "end_line": 556,
              "name": "TestHybridSearchIntegration",
              "parent_name": null,
              "docstring": "Integration tests for hybrid search system.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestHybridSearchIntegration:\n    \"\"\"Integration tests for hybrid search system.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment with real components.\"\"\"\n        self.temp_dir = Path(tempfile.mkdtemp())\n        self.project_dir = self.temp_dir / \"test_project\"\n        self.storage_dir = self.temp_dir / \"storage\"\n\n        # Create test project structure\n        self.project_dir.mkdir(parents=True)\n        self.storage_dir.mkdir(parents=True)\n\n        # Create test files\n        self.create_test_files()\n\n        # Initialize components\n        self.embedder = None\n        self.chunker = None\n        self.hybrid_searcher = None\n        self.incremental_indexer = None\n\n    def create_test_files(self):\n        \"\"\"Create test Python files for indexing.\"\"\"\n        test_files = {\n            \"calculator.py\": '''\ndef add_numbers(a, b):\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n\ndef multiply_numbers(a, b):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\nclass Calculator:\n    \"\"\"A simple calculator class.\"\"\"\n\n    def __init__(self):\n        self.history = []\n\n    def calculate_sum(self, numbers):\n        \"\"\"Calculate sum of a list of numbers.\"\"\"\n        result = sum(numbers)\n        self.history.append(f\"sum({numbers}) = {result}\")\n        return result\n''',\n            \"user_manager.py\": '''\nclass UserManager:\n    \"\"\"Manages user operations.\"\"\"\n\n    def __init__(self):\n        self.users = {}\n\n    def get_user(self, user_id):\n        \"\"\"Retrieve user by ID.\"\"\"\n        return self.users.get(user_id)\n\n    def authenticate_user(self, username, password):\n        \"\"\"Authenticate a user with username and password.\"\"\"\n        user = self.find_user_by_username(username)\n        if user and user.check_password(password):\n            return user\n        return None\n\n    def find_user_by_username(self, username):\n        \"\"\"Find user by username.\"\"\"\n        for user in self.users.values():\n            if user.username == username:\n                return user\n        return None\n''',\n            \"api_client.py\": '''\nimport asyncio\nfrom typing import Optional, Dict, Any\n\nasync def fetch_data(url: str, headers: Optional[Dict] = None) -> Dict[Any, Any]:\n    \"\"\"Fetch data from API endpoint.\"\"\"\n    # Simulate API call\n    await asyncio.sleep(0.1)\n    return {\"status\": \"success\", \"data\": {}}\n\ndef handle_api_error(error: Exception) -> Dict[str, Any]:\n    \"\"\"Handle API errors gracefully.\"\"\"\n    return {\n        \"error\": str(error),\n        \"status\": \"failed\",\n        \"retry\": True\n    }\n\nclass APIClient:\n    \"\"\"HTTP API client for external services.\"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n        self.session = None\n\n    async def get(self, endpoint: str) -> Dict[Any, Any]:\n        \"\"\"GET request to API endpoint.\"\"\"\n        url = f\"{self.base_url}{endpoint}\"\n        return await fetch_data(url)\n''',\n            \"database.py\": '''\nimport sqlite3\nfrom typing import List, Dict, Any\n\nclass DatabaseConnection:\n    \"\"\"Database connection manager.\"\"\"\n\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.connection = None\n\n    def connect(self):\n        \"\"\"Establish database connection.\"\"\"\n        self.connection = sqlite3.connect(self.db_path)\n        return self.connection\n\n    def execute_query(self, query: str, params: tuple = ()) -> List[Dict[str, Any]]:\n        \"\"\"Execute SQL query and return results.\"\"\"\n        if not self.connection:\n            self.connect()\n\n        cursor = self.connection.cursor()\n        cursor.execute(query, params)\n\n        columns = [description[0] for description in cursor.description]\n        results = []\n        for row in cursor.fetchall():\n            results.append(dict(zip(columns, row)))\n\n        return results\n''',\n        }\n\n        for filename, content in test_files.items():\n            file_path = self.project_dir / filename\n            file_path.write_text(content)\n\n    def initialize_components(self):\n        \"\"\"Initialize all components for testing.\"\"\"\n        try:\n            # Initialize embedder\n            self.embedder = CodeEmbedder()\n\n            # Initialize chunker\n            self.chunker = MultiLanguageChunker(str(self.project_dir))\n\n            # Initialize hybrid searcher\n            self.hybrid_searcher = HybridSearcher(\n                storage_dir=str(self.storage_dir), bm25_weight=0.4, dense_weight=0.6\n            )\n\n            # Initialize incremental indexer\n            # This should work with HybridSearcher as indexer\n            self.incremental_indexer = IncrementalIndexer(\n                indexer=self.hybrid_searcher,\n                embedder=self.embedder,\n                chunker=self.chunker,\n            )\n\n        except Exception as e:\n            pytest.skip(f\"Could not initialize components: {e}\")\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_searcher_has_add_embeddings_method(self):\n        \"\"\"Test that HybridSearcher has add_embeddings method.\"\"\"\n        self.initialize_components()\n\n        # This should not fail\n        assert hasattr(\n            self.hybrid_searcher, \"add_embeddings\"\n        ), \"HybridSearcher missing add_embeddings method required by incremental indexer\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_incremental_indexing_with_hybrid_search(self):\n        \"\"\"Test that incremental indexing works with hybrid search.\"\"\"\n        self.initialize_components()\n\n        # Attempt to index the project\n        try:\n            result = self.incremental_indexer.incremental_index(\n                str(self.project_dir), project_name=\"test_project\", force_full=True\n            )\n\n            assert result.success, f\"Indexing failed: {result.error}\"\n            assert result.chunks_added > 0, \"No chunks were added to the index\"\n\n        except AttributeError as e:\n            if \"add_embeddings\" in str(e):\n                pytest.fail(\"HybridSearcher missing add_embeddings method\")\n            else:\n                raise\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_indices_are_populated(self):\n        \"\"\"Test that both BM25 and dense indices are populated after indexing.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n\n        assert result.success, \"Indexing must succeed for this test\"\n\n        # Check that HybridSearcher is ready (both indices populated)\n        assert (\n            self.hybrid_searcher.is_ready\n        ), \"HybridSearcher should be ready after indexing (both BM25 and dense indices populated)\"\n\n        # Check BM25 index specifically\n        assert (\n            not self.hybrid_searcher.bm25_index.is_empty\n        ), \"BM25 index should not be empty after indexing\"\n\n        # Check dense index specifically\n        assert (\n            self.hybrid_searcher.dense_index.index is not None\n        ), \"Dense index should exist after indexing\"\n        assert (\n            self.hybrid_searcher.dense_index.index.ntotal > 0\n        ), \"Dense index should contain vectors after indexing\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_search_returns_results(self):\n        \"\"\"Test that hybrid search returns results from both indices.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        # Test search queries that should favor different indices\n        queries_to_test = [\n            # Should favor BM25 (exact text matches)\n            \"def add_numbers\",\n            \"UserManager\",\n            \"sqlite3\",\n            # Should favor dense/semantic (conceptual matches)\n            \"database connection\",\n            \"user authentication\",\n            \"API client\",\n            \"error handling\",\n            \"calculate numbers\",\n        ]\n\n        for query in queries_to_test:\n            results = self.hybrid_searcher.search(query, k=5)\n            assert len(results) > 0, f\"No results found for query: '{query}'\"\n\n            # Check that results have the expected format\n            for result in results:\n                assert hasattr(result, \"doc_id\"), \"Result missing doc_id\"\n                assert hasattr(result, \"score\"), \"Result missing score\"\n                assert hasattr(result, \"metadata\"), \"Result missing metadata\"\n                assert (\n                    result.score > 0\n                ), f\"Result score should be positive: {result.score}\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_bm25_vs_dense_results_differ(self):\n        \"\"\"Test that BM25-only and dense-only searches return different results.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        # Test a query that should show different results for BM25 vs dense\n        query = \"user authentication\"\n\n        # Get BM25-only results\n        bm25_results = self.hybrid_searcher._search_bm25(query, k=5, min_score=0.0)\n\n        # Get dense-only results\n        dense_results = self.hybrid_searcher._search_dense(query, k=5, filters=None)\n\n        # Both should return results\n        assert len(bm25_results) > 0, \"BM25 search should return results\"\n        assert len(dense_results) > 0, \"Dense search should return results\"\n\n        # Extract doc IDs for comparison\n        bm25_doc_ids = [doc_id for doc_id, _, _ in bm25_results]\n        dense_doc_ids = [doc_id for doc_id, _, _ in dense_results]\n\n        # The results should be different (different ranking/selection)\n        # This tests that both indices are contributing to the search\n        assert (\n            bm25_doc_ids != dense_doc_ids\n        ), \"BM25 and dense search should return different results for semantic queries\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_reranking_combines_results(self):\n        \"\"\"Test that hybrid reranking properly combines BM25 and dense results.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        query = \"calculate sum\"\n\n        # Get results from both searches\n        bm25_results = self.hybrid_searcher._search_bm25(query, k=10, min_score=0.0)\n        dense_results = self.hybrid_searcher._search_dense(query, k=10, filters=None)\n\n        # Get hybrid results\n        hybrid_results = self.hybrid_searcher.search(query, k=5, use_parallel=False)\n\n        # Hybrid results should exist\n        assert len(hybrid_results) > 0, \"Hybrid search should return results\"\n\n        # Check that hybrid results contain documents from both searches\n        hybrid_doc_ids = {result.doc_id for result in hybrid_results}\n        bm25_doc_ids = {doc_id for doc_id, _, _ in bm25_results}\n        dense_doc_ids = {doc_id for doc_id, _, _ in dense_results}\n\n        # At least some hybrid results should come from BM25 or dense\n        assert (\n            len(hybrid_doc_ids & bm25_doc_ids) > 0\n            or len(hybrid_doc_ids & dense_doc_ids) > 0\n        ), \"Hybrid results should include documents from BM25 or dense searches\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_parallel_vs_sequential_search(self):\n        \"\"\"Test that parallel and sequential search modes work and return similar results.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        query = \"database connection\"\n\n        # Get results with both modes\n        parallel_results = self.hybrid_searcher.search(query, k=5, use_parallel=True)\n        sequential_results = self.hybrid_searcher.search(query, k=5, use_parallel=False)\n\n        # Both should return results\n        assert len(parallel_results) > 0, \"Parallel search should return results\"\n        assert len(sequential_results) > 0, \"Sequential search should return results\"\n\n        # Results should be similar (same reranking algorithm)\n        parallel_doc_ids = [r.doc_id for r in parallel_results]\n        sequential_doc_ids = [r.doc_id for r in sequential_results]\n\n        # Allow for some differences due to threading, but expect significant overlap\n        overlap = len(set(parallel_doc_ids) & set(sequential_doc_ids))\n        assert (\n            overlap >= len(parallel_results) // 2\n        ), \"Parallel and sequential search should have significant overlap in results\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_index_persistence(self):\n        \"\"\"Test that hybrid indices persist across searcher instances.\"\"\"\n        self.initialize_components()\n\n        # Index with first searcher instance\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Initial indexing must succeed\"\n\n        # Save indices\n        self.hybrid_searcher.save_indices()\n\n        # Create new searcher instance\n        new_searcher = HybridSearcher(\n            storage_dir=str(self.storage_dir), bm25_weight=0.4, dense_weight=0.6\n        )\n\n        # Load indices\n        load_success = new_searcher.load_indices()\n        assert load_success, \"Loading indices should succeed\"\n\n        # Verify the new searcher is ready\n        assert (\n            new_searcher.is_ready\n        ), \"New searcher should be ready after loading indices\"\n\n        # Test that search works with loaded indices\n        query = \"calculate sum\"\n        results = new_searcher.search(query, k=3)\n        assert len(results) > 0, \"Search should work with loaded indices\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_incremental_updates(self):\n        \"\"\"Test that incremental updates work with hybrid search.\"\"\"\n        self.initialize_components()\n\n        # Initial indexing\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Initial indexing must succeed\"\n\n        # Add a new file\n        new_file = self.project_dir / \"new_module.py\"\n        new_file.write_text(\n            '''\ndef process_data(data_list):\n    \"\"\"Process a list of data items.\"\"\"\n    processed = []\n    for item in data_list:\n        if validate_item(item):\n            processed.append(transform_item(item))\n    return processed\n\ndef validate_item(item):\n    \"\"\"Validate a single data item.\"\"\"\n    return item is not None and len(str(item)) > 0\n'''\n        )\n\n        # Incremental update\n        update_result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=False\n        )\n\n        assert update_result.success, \"Incremental update must succeed\"\n        assert update_result.files_added > 0, \"New file should be detected\"\n        assert update_result.chunks_added > 0, \"New chunks should be added\"\n\n        # Verify that new content is searchable\n        results = self.hybrid_searcher.search(\"process data\", k=3)\n        assert len(results) > 0, \"New content should be searchable\"\n\n        # Check that at least one result is from the new file\n        new_file_results = [r for r in results if \"new_module.py\" in r.doc_id]\n        assert len(new_file_results) > 0, \"Should find results from new file\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_search_mode_configuration(self):\n        \"\"\"Test search mode configuration and switching.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        # Test weight adjustment\n        original_bm25_weight = self.hybrid_searcher.bm25_weight\n        original_dense_weight = self.hybrid_searcher.dense_weight\n\n        # Change weights\n        self.hybrid_searcher.bm25_weight = 0.8\n        self.hybrid_searcher.dense_weight = 0.2\n\n        # Search should still work\n        results = self.hybrid_searcher.search(\"user authentication\", k=3)\n        assert len(results) > 0, \"Search should work with different weights\"\n\n        # Restore weights\n        self.hybrid_searcher.bm25_weight = original_bm25_weight\n        self.hybrid_searcher.dense_weight = original_dense_weight\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_error_handling(self):\n        \"\"\"Test error handling in hybrid search system.\"\"\"\n        self.initialize_components()\n\n        # Test search on empty index\n        results = self.hybrid_searcher.search(\"test query\", k=5)\n        assert results == [], \"Search on empty index should return empty results\"\n\n        # Test with invalid query\n        results = self.hybrid_searcher.search(\"\", k=5)\n        assert isinstance(results, list), \"Empty query should return list\"\n\n        # Test with zero k\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        results = self.hybrid_searcher.search(\"test\", k=0)\n        assert len(results) == 0, \"k=0 should return no results\"\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_statistics_and_monitoring(self):\n        \"\"\"Test statistics collection and monitoring features.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        # Get initial stats\n        stats = self.hybrid_searcher.stats\n        assert \"bm25_stats\" in stats, \"Stats should include BM25 information\"\n        assert \"dense_stats\" in stats, \"Stats should include dense information\"\n        assert \"gpu_memory\" in stats, \"Stats should include GPU memory information\"\n\n        # Perform some searches to generate performance stats\n        queries = [\"calculate\", \"user\", \"database\", \"API\"]\n        for query in queries:\n            self.hybrid_searcher.search(query, k=3)\n\n        # Get search mode stats\n        search_stats = self.hybrid_searcher.get_search_mode_stats()\n        assert search_stats[\"total_searches\"] == len(\n            queries\n        ), \"Should track search count\"\n        assert \"average_times\" in search_stats, \"Should include timing information\"\n\n    def teardown_method(self):\n        \"\"\"Clean up test environment.\"\"\"\n        try:\n            if hasattr(self, \"hybrid_searcher\") and self.hybrid_searcher:\n                self.hybrid_searcher.shutdown()\n        except Exception:\n            pass\n\n        try:\n            shutil.rmtree(self.temp_dir, ignore_errors=True)\n        except Exception:\n            pass",
              "content_preview": "class TestHybridSearchIntegration:\n    \"\"\"Integration tests for hybrid search system.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment with real components.\"\"\"\n        self.temp_dir ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012955465587044534,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:301-426:method:search",
            "score": 2.574,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 301,
              "end_line": 426,
              "name": "search",
              "parent_name": "HybridSearcher",
              "docstring": "Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)\n        \"\"\"\n        # Check if indices are ready based on search mode\n        if search_mode == \"bm25\":\n            if self.bm25_index.is_empty:\n                self._logger.warning(\"BM25 search requested but BM25 index is empty\")\n                return []\n        elif search_mode == \"semantic\":\n            if not self.dense_index.index or self.dense_index.index.ntotal == 0:\n                self._logger.warning(\n                    \"Semantic search requested but dense index is empty\"\n                )\n                return []\n        else:  # hybrid\n            if not self.is_ready:\n                self._logger.warning(\"Hybrid search not ready - indices may be empty\")\n                return []\n\n        self._logger.debug(f\"{search_mode.title()} search for: '{query}' (k={k})\")\n\n        start_time = time.time()\n\n        # Handle different search modes\n        if search_mode == \"bm25\":\n            # BM25-only search\n            bm25_results = self._search_bm25(query, k, min_bm25_score)\n            # Convert BM25 results to SearchResult format\n            final_results = self._convert_bm25_to_search_results(bm25_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        elif search_mode == \"semantic\":\n            # Dense-only search\n            dense_results = self._search_dense(query, k, filters)\n            # Convert dense results to SearchResult format\n            final_results = self._convert_dense_to_search_results(dense_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        else:  # hybrid mode\n            search_k = k * 2  # Get more results for better reranking\n\n            if use_parallel and not self._is_shutdown:\n                # Parallel execution\n                bm25_results, dense_results = self._parallel_search(\n                    query, search_k, min_bm25_score, filters\n                )\n            else:\n                # Sequential execution\n                bm25_results, dense_results = self._sequential_search(\n                    query, search_k, min_bm25_score, filters\n                )\n\n            # Rerank results\n            rerank_start = time.time()\n            self._logger.debug(\n                f\"[RERANK] Using weights: BM25={self.bm25_weight}, Dense={self.dense_weight}, \"\n                f\"BM25_results={len(bm25_results)}, Dense_results={len(dense_results)}\"\n            )\n            final_results = self.reranker.rerank_simple(\n                bm25_results=bm25_results,\n                dense_results=dense_results,\n                max_results=k,\n                bm25_weight=self.bm25_weight,\n                dense_weight=self.dense_weight,\n            )\n            rerank_time = time.time() - rerank_start\n            self._logger.debug(\n                f\"[RERANK] Produced {len(final_results)} results in {rerank_time:.3f}s\"\n            )\n\n        # Update statistics\n        total_time = time.time() - start_time\n        self._search_stats[\"total_searches\"] += 1\n        self._search_stats[\"rerank_time\"] += rerank_time\n\n        if use_parallel:\n            parallel_time = max(\n                self._search_stats.get(\"last_bm25_time\", 0),\n                self._search_stats.get(\"last_dense_time\", 0),\n            )\n            sequential_time = self._search_stats.get(\n                \"last_bm25_time\", 0\n            ) + self._search_stats.get(\"last_dense_time\", 0)\n            if sequential_time > 0:\n                efficiency = 1.0 - (parallel_time / sequential_time)\n                self._search_stats[\"parallel_efficiency\"] = efficiency\n\n        # Mode-specific logging\n        if search_mode == \"bm25\":\n            self._logger.debug(\n                f\"BM25 search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        elif search_mode == \"semantic\":\n            self._logger.debug(\n                f\"Semantic search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        else:  # hybrid\n            self._logger.debug(\n                f\"Hybrid search complete: {len(final_results)} results, \"\n                f\"BM25: {len(bm25_results)}, Dense: {len(dense_results)}, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n\n        return final_results",
              "content_preview": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012317073170731708,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          },
          {
            "doc_id": "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
            "score": 2.557,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 1284,
              "end_line": 1363,
              "name": "configure_search_mode",
              "parent_name": null,
              "docstring": "Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes\n    \"\"\"\n    try:\n        # Validate search mode\n        valid_modes = [\"hybrid\", \"semantic\", \"bm25\", \"auto\"]\n        if search_mode not in valid_modes:\n            return json.dumps(\n                {\n                    \"error\": f\"Invalid search_mode '{search_mode}'. Must be one of: {valid_modes}\"\n                }\n            )\n\n        # Validate weights\n        if not (0.0 <= bm25_weight <= 1.0) or not (0.0 <= dense_weight <= 1.0):\n            return json.dumps({\"error\": \"Weights must be between 0.0 and 1.0\"})\n\n        # Normalize weights if they don't sum to 1.0\n        total_weight = bm25_weight + dense_weight\n        if total_weight > 0:\n            bm25_weight = bm25_weight / total_weight\n            dense_weight = dense_weight / total_weight\n\n        # Get current config and update\n        config_manager = get_config_manager()\n        config = config_manager.load_config()\n\n        # Update configuration\n        config.default_search_mode = search_mode\n        config.enable_hybrid_search = search_mode == \"hybrid\"\n        config.bm25_weight = bm25_weight\n        config.dense_weight = dense_weight\n        config.use_parallel_search = enable_parallel\n\n        # Save configuration\n        config_manager.save_config(config)\n\n        # Reset searcher to pick up new configuration\n        global _searcher\n        _searcher = None\n\n        response = {\n            \"success\": True,\n            \"message\": \"Search configuration updated successfully\",\n            \"new_config\": {\n                \"search_mode\": search_mode,\n                \"enable_hybrid_search\": config.enable_hybrid_search,\n                \"bm25_weight\": round(bm25_weight, 3),\n                \"dense_weight\": round(dense_weight, 3),\n                \"use_parallel_search\": enable_parallel,\n            },\n            \"note\": \"Changes will take effect on next search. Searcher will be reinitialized.\",\n        }\n\n        logger.info(\n            f\"Search configuration updated: mode={search_mode}, \"\n            f\"weights=({bm25_weight:.3f}, {dense_weight:.3f}), parallel={enable_parallel}\"\n        )\n\n        return json.dumps(response, indent=2)\n\n    except Exception as e:\n        logger.error(f\"Error configuring search mode: {e}\")\n        return json.dumps({\"error\": str(e)})",
              "content_preview": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure s...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012081128747795412,
              "appears_in_lists": 2,
              "final_rank": 6
            }
          }
        ],
        "all_doc_ids": [
          "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
          "search\\hybrid_searcher.py:301-426:method:search",
          "search\\searcher.py:123-165:method:_semantic_search",
          "tests\\unit\\test_search_config.py:11-45:class:TestSearchConfig",
          "search\\hybrid_searcher.py:727-769:method:_search_dense",
          "tests\\unit\\test_search_config.py:48-160:class:TestSearchConfigManager",
          "tests\\unit\\test_reranker.py:35-350:class:TestRRFReranker",
          "search\\searcher.py:33-478:class:IntelligentSearcher",
          "tests\\integration\\test_hybrid_search_integration.py:28-556:class:TestHybridSearchIntegration",
          "tests\\unit\\test_reranker.py:38-53:method:setup_method"
        ],
        "unique_discoveries": [
          "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
          "search\\hybrid_searcher.py:301-426:method:search",
          "tests\\unit\\test_search_config.py:11-45:class:TestSearchConfig",
          "tests\\unit\\test_search_config.py:48-160:class:TestSearchConfigManager",
          "tests\\unit\\test_reranker.py:35-350:class:TestRRFReranker",
          "tests\\unit\\test_reranker.py:38-53:method:setup_method"
        ]
      },
      "comparison": {
        "time_overhead_ms": -2253.11,
        "time_overhead_pct": -96.8,
        "top5_overlap_count": 3,
        "top5_overlap_pct": 60.0,
        "unique_discovery_count": 6,
        "value_rating": "HIGH"
      }
    },
    {
      "query": "hybrid search combining BM25 and semantic",
      "single_hop": {
        "time_ms": 17.92,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "search\\hybrid_searcher.py:301-426:method:search",
            "score": 13.669,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 301,
              "end_line": 426,
              "name": "search",
              "parent_name": "HybridSearcher",
              "docstring": "Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)\n        \"\"\"\n        # Check if indices are ready based on search mode\n        if search_mode == \"bm25\":\n            if self.bm25_index.is_empty:\n                self._logger.warning(\"BM25 search requested but BM25 index is empty\")\n                return []\n        elif search_mode == \"semantic\":\n            if not self.dense_index.index or self.dense_index.index.ntotal == 0:\n                self._logger.warning(\n                    \"Semantic search requested but dense index is empty\"\n                )\n                return []\n        else:  # hybrid\n            if not self.is_ready:\n                self._logger.warning(\"Hybrid search not ready - indices may be empty\")\n                return []\n\n        self._logger.debug(f\"{search_mode.title()} search for: '{query}' (k={k})\")\n\n        start_time = time.time()\n\n        # Handle different search modes\n        if search_mode == \"bm25\":\n            # BM25-only search\n            bm25_results = self._search_bm25(query, k, min_bm25_score)\n            # Convert BM25 results to SearchResult format\n            final_results = self._convert_bm25_to_search_results(bm25_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        elif search_mode == \"semantic\":\n            # Dense-only search\n            dense_results = self._search_dense(query, k, filters)\n            # Convert dense results to SearchResult format\n            final_results = self._convert_dense_to_search_results(dense_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        else:  # hybrid mode\n            search_k = k * 2  # Get more results for better reranking\n\n            if use_parallel and not self._is_shutdown:\n                # Parallel execution\n                bm25_results, dense_results = self._parallel_search(\n                    query, search_k, min_bm25_score, filters\n                )\n            else:\n                # Sequential execution\n                bm25_results, dense_results = self._sequential_search(\n                    query, search_k, min_bm25_score, filters\n                )\n\n            # Rerank results\n            rerank_start = time.time()\n            self._logger.debug(\n                f\"[RERANK] Using weights: BM25={self.bm25_weight}, Dense={self.dense_weight}, \"\n                f\"BM25_results={len(bm25_results)}, Dense_results={len(dense_results)}\"\n            )\n            final_results = self.reranker.rerank_simple(\n                bm25_results=bm25_results,\n                dense_results=dense_results,\n                max_results=k,\n                bm25_weight=self.bm25_weight,\n                dense_weight=self.dense_weight,\n            )\n            rerank_time = time.time() - rerank_start\n            self._logger.debug(\n                f\"[RERANK] Produced {len(final_results)} results in {rerank_time:.3f}s\"\n            )\n\n        # Update statistics\n        total_time = time.time() - start_time\n        self._search_stats[\"total_searches\"] += 1\n        self._search_stats[\"rerank_time\"] += rerank_time\n\n        if use_parallel:\n            parallel_time = max(\n                self._search_stats.get(\"last_bm25_time\", 0),\n                self._search_stats.get(\"last_dense_time\", 0),\n            )\n            sequential_time = self._search_stats.get(\n                \"last_bm25_time\", 0\n            ) + self._search_stats.get(\"last_dense_time\", 0)\n            if sequential_time > 0:\n                efficiency = 1.0 - (parallel_time / sequential_time)\n                self._search_stats[\"parallel_efficiency\"] = efficiency\n\n        # Mode-specific logging\n        if search_mode == \"bm25\":\n            self._logger.debug(\n                f\"BM25 search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        elif search_mode == \"semantic\":\n            self._logger.debug(\n                f\"Semantic search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        else:  # hybrid\n            self._logger.debug(\n                f\"Hybrid search complete: {len(final_results)} results, \"\n                f\"BM25: {len(bm25_results)}, Dense: {len(dense_results)}, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n\n        return final_results",
              "content_preview": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional...",
              "project_name": "claude-context-local",
              "rrf_score": 0.0162876784769963,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:57-1252:class:HybridSearcher",
            "score": 10.489,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "class",
              "start_line": 57,
              "end_line": 1252,
              "name": "HybridSearcher",
              "parent_name": null,
              "docstring": "Orchestrates BM25 + dense search with GPU awareness and parallel execution.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class HybridSearcher:\n    \"\"\"Orchestrates BM25 + dense search with GPU awareness and parallel execution.\"\"\"\n\n    def __init__(\n        self,\n        storage_dir: str,\n        embedder=None,\n        bm25_weight: float = 0.4,\n        dense_weight: float = 0.6,\n        rrf_k: int = 60,\n        max_workers: int = 2,\n    ):\n        \"\"\"\n        Initialize hybrid searcher.\n\n        Args:\n            storage_dir: Directory for storing indices\n            embedder: CodeEmbedder instance for semantic search (optional)\n            bm25_weight: Weight for BM25 results (0.0 to 1.0)\n            dense_weight: Weight for dense vector results (0.0 to 1.0)\n            rrf_k: RRF parameter for reranking\n            max_workers: Maximum thread pool workers for parallel execution\n        \"\"\"\n        self.storage_dir = Path(storage_dir)\n        self.storage_dir.mkdir(parents=True, exist_ok=True)\n\n        # Store embedder for semantic search\n        self.embedder = embedder\n\n        # Weights\n        self.bm25_weight = bm25_weight\n        self.dense_weight = dense_weight\n\n        # Components - use existing storage structure\n        self._logger = logging.getLogger(__name__)\n\n        # BM25 index gets its own subdirectory\n        self._logger.info(f\"[INIT] Creating BM25Index at: {self.storage_dir / 'bm25'}\")\n        try:\n            self.bm25_index = BM25Index(str(self.storage_dir / \"bm25\"))\n            self._logger.info(\"[INIT] BM25Index created successfully\")\n        except Exception as e:\n            self._logger.error(f\"[INIT] Failed to create BM25Index: {e}\")\n            raise\n\n        # Try to load existing BM25 index\n        self._logger.info(f\"[INIT] BM25 storage path: {self.storage_dir / 'bm25'}\")\n        bm25_loaded = self.bm25_index.load()\n        if bm25_loaded:\n            self._logger.info(\n                f\"[INIT] Loaded existing BM25 index with {self.bm25_index.size} documents\"\n            )\n        else:\n            self._logger.info(\"[INIT] No existing BM25 index found, starting fresh\")\n            # Log what files we're looking for\n            bm25_dir = self.storage_dir / \"bm25\"\n            self._logger.debug(f\"[INIT] BM25 directory exists: {bm25_dir.exists()}\")\n            if bm25_dir.exists():\n                files = list(bm25_dir.iterdir())\n                self._logger.debug(\n                    f\"[INIT] BM25 files found: {[f.name for f in files]}\"\n                )\n\n        # Dense index uses the main storage directory where existing indices are stored\n        self._logger.info(f\"[INIT] Initializing dense index at: {self.storage_dir}\")\n        self.dense_index = CodeIndexManager(str(self.storage_dir))\n        # Dense index loads automatically in its __init__\n        dense_count = self.dense_index.index.ntotal if self.dense_index.index else 0\n        if dense_count > 0:\n            self._logger.info(\n                f\"[INIT] Loaded existing dense index with {dense_count} vectors\"\n            )\n        else:\n            self._logger.info(\"[INIT] No existing dense index found, starting fresh\")\n\n        # Log final initialization status\n        total_bm25 = self.bm25_index.size\n        self._logger.info(\n            f\"[INIT] HybridSearcher initialized - BM25: {total_bm25} docs, Dense: {dense_count} vectors\"\n        )\n        self._logger.info(\n            f\"[INIT] Ready status: BM25={not self.bm25_index.is_empty}, Dense={dense_count > 0}, Overall={self.is_ready}\"\n        )\n\n        self.reranker = RRFReranker(k=rrf_k)\n        self.gpu_monitor = GPUMemoryMonitor()\n\n        # Threading\n        self.max_workers = max_workers\n        self._thread_pool = ThreadPoolExecutor(max_workers=max_workers)\n        self._shutdown_lock = threading.Lock()\n        self._is_shutdown = False\n\n        # Performance tracking\n        self._search_stats = {\n            \"total_searches\": 0,\n            \"bm25_time\": 0.0,\n            \"dense_time\": 0.0,\n            \"rerank_time\": 0.0,\n            \"parallel_efficiency\": 0.0,\n        }\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.shutdown()\n\n    def shutdown(self):\n        \"\"\"Shutdown the thread pool.\"\"\"\n        with self._shutdown_lock:\n            if not self._is_shutdown:\n                self._thread_pool.shutdown(wait=True)\n                self._is_shutdown = True\n                self._logger.info(\"HybridSearcher shut down\")\n\n    @property\n    def is_ready(self) -> bool:\n        \"\"\"Check if both indices are ready.\"\"\"\n        bm25_ready = not self.bm25_index.is_empty\n        dense_ready = (\n            self.dense_index.index is not None and self.dense_index.index.ntotal > 0\n        )\n\n        self._logger.debug(\n            f\"[IS_READY] BM25 ready: {bm25_ready} (size: {self.bm25_index.size})\"\n        )\n        self._logger.debug(\n            f\"[IS_READY] Dense ready: {dense_ready} (vectors: {self.dense_index.index.ntotal if self.dense_index.index else 0})\"\n        )\n\n        is_ready = bm25_ready and dense_ready\n        self._logger.debug(f\"[IS_READY] Overall ready: {is_ready}\")\n\n        return is_ready\n\n    @property\n    def stats(self) -> Dict[str, Any]:\n        \"\"\"Get search performance statistics.\"\"\"\n        stats = self._search_stats.copy()\n\n        # Add index stats\n        stats.update(\n            {\n                \"bm25_stats\": self.bm25_index.get_stats(),\n                \"dense_stats\": {\n                    \"total_vectors\": (\n                        self.dense_index.index.ntotal if self.dense_index.index else 0\n                    ),\n                    \"on_gpu\": getattr(self.dense_index, \"_on_gpu\", False),\n                },\n                \"gpu_memory\": self.gpu_monitor.get_available_memory(),\n            }\n        )\n\n        return stats\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get index statistics in the format expected by MCP server.\"\"\"\n        bm25_count = self.bm25_index.size\n        dense_count = self.dense_index.index.ntotal if self.dense_index.index else 0\n        total_chunks = max(bm25_count, dense_count)  # Use the higher count as total\n\n        return {\n            \"total_chunks\": total_chunks,\n            \"bm25_documents\": bm25_count,\n            \"dense_vectors\": dense_count,\n            \"is_ready\": self.is_ready,\n            \"bm25_ready\": not self.bm25_index.is_empty,\n            \"dense_ready\": dense_count > 0,\n        }\n\n    def get_index_size(self) -> int:\n        \"\"\"Get total index size (compatible with incremental indexer interface).\"\"\"\n        bm25_count = self.bm25_index.size\n        dense_count = self.dense_index.index.ntotal if self.dense_index.index else 0\n        return max(bm25_count, dense_count)  # Return the higher count\n\n    def index_documents(\n        self,\n        documents: List[str],\n        doc_ids: List[str],\n        embeddings: List[List[float]],\n        metadata: Optional[Dict[str, Dict]] = None,\n    ) -> None:\n        \"\"\"Index documents in both BM25 and dense indices.\"\"\"\n        if len(documents) != len(doc_ids) or len(documents) != len(embeddings):\n            raise ValueError(\"All input lists must have the same length\")\n\n        self._logger.info(f\"[INDEX_DOCUMENTS] Called with {len(documents)} documents\")\n\n        # Index in BM25 (CPU)\n        self._logger.info(\"[BM25] Starting BM25 indexing...\")\n        start_time = time.time()\n        bm25_size_before = self.bm25_index.size\n        self._logger.info(f\"[BM25] Before indexing - size: {bm25_size_before}\")\n\n        self.bm25_index.index_documents(documents, doc_ids, metadata)\n\n        bm25_time = time.time() - start_time\n        bm25_size_after = self.bm25_index.size\n        self._logger.info(f\"[BM25] After indexing - size: {bm25_size_after}\")\n\n        self._logger.debug(\n            f\"[BM25] Indexing completed: {bm25_size_before} -> {bm25_size_after} documents ({bm25_time:.2f}s)\"\n        )\n        self._logger.debug(f\"[BM25] Index directory: {self.bm25_index.storage_dir}\")\n        self._logger.debug(\n            f\"[BM25] Index files will be saved as: {[str(p) for p in [self.bm25_index.index_path, self.bm25_index.docs_path, self.bm25_index.metadata_path]]}\"\n        )\n\n        # Verify BM25 indexing worked\n        if bm25_size_after == bm25_size_before:\n            self._logger.error(\"[BM25] ERROR: No documents were indexed!\")\n            self._logger.debug(f\"[BM25] Documents provided: {len(documents)}\")\n            self._logger.debug(\n                f\"[BM25] First document: {documents[0][:200] if documents else 'EMPTY'}\"\n            )\n\n        # Index in dense (potentially GPU)\n        start_time = time.time()\n        # Convert embeddings to EmbeddingResult format\n        import numpy as np\n\n        from embeddings.embedder import EmbeddingResult\n\n        embedding_results = []\n        for _i, (doc_id, embedding) in enumerate(\n            zip(doc_ids, embeddings, strict=False)\n        ):\n            result = EmbeddingResult(\n                embedding=np.array(embedding, dtype=np.float32),\n                chunk_id=doc_id,\n                metadata=metadata.get(doc_id, {}) if metadata else {},\n            )\n            embedding_results.append(result)\n\n        self.dense_index.add_embeddings(embedding_results)\n        dense_time = time.time() - start_time\n\n        self._logger.info(\n            f\"Hybrid indexing complete: BM25 {bm25_time:.2f}s, Dense {dense_time:.2f}s\"\n        )\n\n    def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)\n        \"\"\"\n        # Check if indices are ready based on search mode\n        if search_mode == \"bm25\":\n            if self.bm25_index.is_empty:\n                self._logger.warning(\"BM25 search requested but BM25 index is empty\")\n                return []\n        elif search_mode == \"semantic\":\n            if not self.dense_index.index or self.dense_index.index.ntotal == 0:\n                self._logger.warning(\n                    \"Semantic search requested but dense index is empty\"\n                )\n                return []\n        else:  # hybrid\n            if not self.is_ready:\n                self._logger.warning(\"Hybrid search not ready - indices may be empty\")\n                return []\n\n        self._logger.debug(f\"{search_mode.title()} search for: '{query}' (k={k})\")\n\n        start_time = time.time()\n\n        # Handle different search modes\n        if search_mode == \"bm25\":\n            # BM25-only search\n            bm25_results = self._search_bm25(query, k, min_bm25_score)\n            # Convert BM25 results to SearchResult format\n            final_results = self._convert_bm25_to_search_results(bm25_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        elif search_mode == \"semantic\":\n            # Dense-only search\n            dense_results = self._search_dense(query, k, filters)\n            # Convert dense results to SearchResult format\n            final_results = self._convert_dense_to_search_results(dense_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        else:  # hybrid mode\n            search_k = k * 2  # Get more results for better reranking\n\n            if use_parallel and not self._is_shutdown:\n                # Parallel execution\n                bm25_results, dense_results = self._parallel_search(\n                    query, search_k, min_bm25_score, filters\n                )\n            else:\n                # Sequential execution\n                bm25_results, dense_results = self._sequential_search(\n                    query, search_k, min_bm25_score, filters\n                )\n\n            # Rerank results\n            rerank_start = time.time()\n            self._logger.debug(\n                f\"[RERANK] Using weights: BM25={self.bm25_weight}, Dense={self.dense_weight}, \"\n                f\"BM25_results={len(bm25_results)}, Dense_results={len(dense_results)}\"\n            )\n            final_results = self.reranker.rerank_simple(\n                bm25_results=bm25_results,\n                dense_results=dense_results,\n                max_results=k,\n                bm25_weight=self.bm25_weight,\n                dense_weight=self.dense_weight,\n            )\n            rerank_time = time.time() - rerank_start\n            self._logger.debug(\n                f\"[RERANK] Produced {len(final_results)} results in {rerank_time:.3f}s\"\n            )\n\n        # Update statistics\n        total_time = time.time() - start_time\n        self._search_stats[\"total_searches\"] += 1\n        self._search_stats[\"rerank_time\"] += rerank_time\n\n        if use_parallel:\n            parallel_time = max(\n                self._search_stats.get(\"last_bm25_time\", 0),\n                self._search_stats.get(\"last_dense_time\", 0),\n            )\n            sequential_time = self._search_stats.get(\n                \"last_bm25_time\", 0\n            ) + self._search_stats.get(\"last_dense_time\", 0)\n            if sequential_time > 0:\n                efficiency = 1.0 - (parallel_time / sequential_time)\n                self._search_stats[\"parallel_efficiency\"] = efficiency\n\n        # Mode-specific logging\n        if search_mode == \"bm25\":\n            self._logger.debug(\n                f\"BM25 search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        elif search_mode == \"semantic\":\n            self._logger.debug(\n                f\"Semantic search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        else:  # hybrid\n            self._logger.debug(\n                f\"Hybrid search complete: {len(final_results)} results, \"\n                f\"BM25: {len(bm25_results)}, Dense: {len(dense_results)}, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n\n        return final_results\n\n    def find_similar_to_chunk(self, chunk_id: str, k: int = 5) -> List:\n        \"\"\"\n        Find chunks similar to a given chunk using dense semantic search.\n\n        Args:\n            chunk_id: The ID of the reference chunk\n            k: Number of similar chunks to return\n\n        Returns:\n            List of SearchResult objects with similar chunks\n        \"\"\"\n        from .searcher import SearchResult\n\n        # Use dense index for semantic similarity\n        similar_chunks = self.dense_index.get_similar_chunks(chunk_id, k)\n\n        # Convert to SearchResult format expected by MCP tool\n        results = []\n        for cid, similarity, metadata in similar_chunks:\n            result = SearchResult(\n                chunk_id=cid,\n                similarity_score=similarity,\n                content_preview=metadata.get(\"content_preview\", \"\"),\n                file_path=metadata.get(\"file_path\", \"\"),\n                relative_path=metadata.get(\"relative_path\", \"\"),\n                folder_structure=metadata.get(\"folder_structure\", []),\n                chunk_type=metadata.get(\"chunk_type\", \"unknown\"),\n                name=metadata.get(\"name\"),\n                parent_name=metadata.get(\"parent_name\"),\n                start_line=metadata.get(\"start_line\", 0),\n                end_line=metadata.get(\"end_line\", 0),\n                docstring=metadata.get(\"docstring\"),\n                tags=metadata.get(\"tags\", []),\n                context_info={},\n            )\n            results.append(result)\n\n        return results\n\n    def multi_hop_search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        hops: int = 2,\n        expansion_factor: float = 0.3,\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List:\n        \"\"\"\n        Multi-hop semantic search for discovering interconnected code relationships.\n\n        Performs iterative search by:\n        1. Initial query-based search (Hop 1)\n        2. Finding similar chunks for each result (Hop 2+)\n        3. Re-ranking all discovered chunks by query relevance\n\n        Args:\n            query: Search query\n            k: Number of final results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            hops: Number of search hops (default: 2)\n            expansion_factor: Fraction of k to expand per hop (default: 0.3)\n            use_parallel: Whether to use parallel search\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for search\n\n        Returns:\n            List of SearchResult objects with discovered related code\n        \"\"\"\n        from .searcher import SearchResult\n\n        # Validate parameters\n        if hops < 1:\n            self._logger.warning(f\"Invalid hops={hops}, using 1\")\n            hops = 1\n        if expansion_factor < 0 or expansion_factor > 2.0:\n            self._logger.warning(f\"Invalid expansion_factor={expansion_factor}, using 0.3\")\n            expansion_factor = 0.3\n\n        self._logger.info(\n            f\"[MULTI_HOP] Starting {hops}-hop search for '{query}' \"\n            f\"(k={k}, expansion={expansion_factor}, mode={search_mode})\"\n        )\n\n        # Hop 1: Initial query-based search\n        initial_k = k * 2  # Get more initial results for better expansion\n        initial_results = self.search(\n            query=query,\n            k=initial_k,\n            search_mode=search_mode,\n            use_parallel=use_parallel,\n            min_bm25_score=min_bm25_score,\n            filters=filters,\n        )\n\n        if not initial_results:\n            self._logger.info(\"[MULTI_HOP] No initial results found\")\n            return []\n\n        self._logger.info(\n            f\"[MULTI_HOP] Hop 1: Found {len(initial_results)} initial results\"\n        )\n\n        # Track all discovered chunks (avoid duplicates)\n        # Note: HybridSearcher.search() returns reranker.SearchResult with doc_id\n        all_doc_ids = {r.doc_id for r in initial_results}\n        all_results = {r.doc_id: r for r in initial_results}\n\n        # If only 1 hop requested, return initial results\n        if hops == 1:\n            return initial_results[:k]\n\n        # Hop 2+: Expand from each initial result\n        expansion_k = max(1, int(k * expansion_factor))\n\n        for hop in range(2, hops + 1):\n            hop_discovered = 0\n\n            # Expand from top initial results only (not from previously expanded)\n            source_results = initial_results[:k]  # Use top k as expansion sources\n\n            for result in source_results:\n                try:\n                    # Find similar chunks to this result\n                    # find_similar_to_chunk returns searcher.SearchResult with chunk_id\n                    similar_chunks = self.find_similar_to_chunk(\n                        chunk_id=result.doc_id,  # doc_id is the same as chunk_id\n                        k=expansion_k\n                    )\n\n                    # Add new chunks\n                    for sim_result in similar_chunks:\n                        # Convert searcher.SearchResult chunk_id to doc_id for consistency\n                        doc_id = sim_result.chunk_id\n                        if doc_id not in all_doc_ids:\n                            all_doc_ids.add(doc_id)\n                            # Convert to reranker.SearchResult format\n                            from .reranker import SearchResult as RerankerSearchResult\n                            reranker_result = RerankerSearchResult(\n                                doc_id=doc_id,\n                                score=sim_result.similarity_score,\n                                metadata=sim_result.__dict__,\n                                source=\"multi_hop\"\n                            )\n                            all_results[doc_id] = reranker_result\n                            hop_discovered += 1\n\n                except Exception as e:\n                    self._logger.warning(\n                        f\"[MULTI_HOP] Failed to find similar chunks for {result.doc_id}: {e}\"\n                    )\n                    continue\n\n            self._logger.info(\n                f\"[MULTI_HOP] Hop {hop}: Discovered {hop_discovered} new chunks \"\n                f\"(total: {len(all_results)})\"\n            )\n\n        # Re-rank all discovered results by query relevance\n        self._logger.info(\n            f\"[MULTI_HOP] Re-ranking {len(all_results)} total chunks by query relevance\"\n        )\n\n        final_results = self._rerank_by_query(\n            query=query,\n            results=list(all_results.values()),\n            k=k,\n            search_mode=search_mode\n        )\n\n        self._logger.info(\n            f\"[MULTI_HOP] Returning top {len(final_results)} results after re-ranking\"\n        )\n\n        return final_results\n\n    def _rerank_by_query(\n        self,\n        query: str,\n        results: List,\n        k: int,\n        search_mode: str = \"hybrid\"\n    ) -> List:\n        \"\"\"\n        Re-rank results by computing fresh relevance scores against the original query.\n\n        Args:\n            query: Original search query\n            results: List of SearchResult objects to re-rank\n            k: Number of top results to return\n            search_mode: Search mode for re-ranking strategy\n\n        Returns:\n            Top k results sorted by query relevance\n        \"\"\"\n        if not results:\n            return []\n\n        # For semantic/hybrid modes: re-score using dense similarity\n        if search_mode in (\"semantic\", \"hybrid\") and self.embedder:\n            try:\n                # Get query embedding\n                query_embedding = self.embedder.embed_query(query)\n\n                # Re-score each result by cosine similarity to query\n                import numpy as np\n\n                for result in results:\n                    # Get chunk embedding from dense index\n                    # reranker.SearchResult uses doc_id, not chunk_id\n                    doc_id = result.doc_id\n                    chunk_metadata = self.dense_index.metadata_db.get(doc_id)\n                    if chunk_metadata and \"embedding\" in chunk_metadata:\n                        chunk_emb = np.array(chunk_metadata[\"embedding\"])\n                        # Compute cosine similarity\n                        similarity = np.dot(query_embedding, chunk_emb) / (\n                            np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb)\n                        )\n                        # reranker.SearchResult uses score, not similarity_score\n                        result.score = float(similarity)\n                    # Keep original score if embedding not found\n\n            except Exception as e:\n                self._logger.warning(\n                    f\"[MULTI_HOP] Failed to re-score with embeddings: {e}, \"\n                    \"keeping original scores\"\n                )\n\n        # Sort by score (descending)\n        # reranker.SearchResult uses score, not similarity_score\n        sorted_results = sorted(\n            results,\n            key=lambda r: r.score,\n            reverse=True\n        )\n\n        return sorted_results[:k]\n\n    def _parallel_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"Execute BM25 and dense search in parallel.\"\"\"\n        try:\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                # Submit both searches\n                bm25_future = executor.submit(\n                    self._search_bm25, query, k, min_bm25_score\n                )\n                dense_future = executor.submit(self._search_dense, query, k, filters)\n\n                # Wait for results\n                bm25_results = bm25_future.result()\n                dense_results = dense_future.result()\n\n                return bm25_results, dense_results\n\n        except Exception as e:\n            self._logger.warning(\n                f\"Parallel search failed, falling back to sequential: {e}\"\n            )\n            return self._sequential_search(query, k, min_bm25_score, filters)\n\n    def _sequential_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"Execute BM25 and dense search sequentially.\"\"\"\n        bm25_results = self._search_bm25(query, k, min_bm25_score)\n        dense_results = self._search_dense(query, k, filters)\n        return bm25_results, dense_results\n\n    def _search_bm25(self, query: str, k: int, min_score: float) -> List[Tuple]:\n        \"\"\"Search using BM25 index.\"\"\"\n        start_time = time.time()\n        try:\n            results = self.bm25_index.search(query, k, min_score)\n            search_time = time.time() - start_time\n\n            self._search_stats[\"bm25_time\"] += search_time\n            self._search_stats[\"last_bm25_time\"] = search_time\n\n            self._logger.debug(\n                f\"BM25 search: {len(results)} results in {search_time:.3f}s\"\n            )\n            return results\n\n        except Exception as e:\n            self._logger.error(f\"BM25 search failed: {e}\")\n            return []\n\n    def _search_dense(self, query: str, k: int, filters: Optional[Dict]) -> List[Tuple]:\n        \"\"\"Search using dense vector index.\"\"\"\n        start_time = time.time()\n        try:\n            # Use stored embedder or create one if not provided\n            if self.embedder is None:\n                self._logger.warning(\n                    \"No embedder provided to HybridSearcher, creating new instance\"\n                )\n                from pathlib import Path\n\n                from embeddings.embedder import CodeEmbedder\n\n                # Use same cache directory as main embedder\n                cache_dir = Path.home() / \".claude_code_search\" / \"models\"\n                cache_dir.mkdir(parents=True, exist_ok=True)\n                self.embedder = CodeEmbedder(cache_dir=str(cache_dir))\n                self._logger.info(\n                    \"Created new CodeEmbedder instance for semantic search\"\n                )\n\n            query_embedding = self.embedder.embed_query(query)\n\n            # Search in dense index\n            results = self.dense_index.search(query_embedding, k, filters)\n\n            search_time = time.time() - start_time\n            self._search_stats[\"dense_time\"] += search_time\n            self._search_stats[\"last_dense_time\"] = search_time\n\n            self._logger.debug(\n                f\"Dense search: {len(results)} results in {search_time:.3f}s\"\n            )\n            return results\n\n        except Exception as e:\n            self._logger.error(f\"Dense search failed: {e}\")\n            import traceback\n\n            self._logger.error(\n                f\"Dense search exception details: {traceback.format_exc()}\"\n            )\n            return []\n\n    def _convert_bm25_to_search_results(\n        self, bm25_results: List[Tuple]\n    ) -> List[SearchResult]:\n        \"\"\"Convert BM25 search results to SearchResult format.\"\"\"\n        search_results = []\n        for i, (doc_id, score, metadata) in enumerate(bm25_results):\n            search_result = SearchResult(\n                doc_id=doc_id, score=score, metadata=metadata, source=\"bm25\", rank=i\n            )\n            search_results.append(search_result)\n        return search_results\n\n    def _convert_dense_to_search_results(\n        self, dense_results: List[Tuple]\n    ) -> List[SearchResult]:\n        \"\"\"Convert dense search results to SearchResult format.\"\"\"\n        search_results = []\n        for i, (doc_id, score, metadata) in enumerate(dense_results):\n            search_result = SearchResult(\n                doc_id=doc_id, score=score, metadata=metadata, source=\"semantic\", rank=i\n            )\n            search_results.append(search_result)\n        return search_results\n\n    def get_search_mode_stats(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about search mode performance.\"\"\"\n        total_searches = self._search_stats[\"total_searches\"]\n        if total_searches == 0:\n            return {\"message\": \"No searches performed yet\"}\n\n        avg_bm25_time = self._search_stats[\"bm25_time\"] / total_searches\n        avg_dense_time = self._search_stats[\"dense_time\"] / total_searches\n        avg_rerank_time = self._search_stats[\"rerank_time\"] / total_searches\n\n        return {\n            \"total_searches\": total_searches,\n            \"average_times\": {\n                \"bm25\": avg_bm25_time,\n                \"dense\": avg_dense_time,\n                \"reranking\": avg_rerank_time,\n                \"total\": avg_bm25_time + avg_dense_time + avg_rerank_time,\n            },\n            \"parallel_efficiency\": self._search_stats.get(\"parallel_efficiency\", 0.0),\n            \"gpu_utilization\": self.gpu_monitor.get_available_memory(),\n            \"search_distribution\": {\n                \"bm25_contribution\": self.bm25_weight,\n                \"dense_contribution\": self.dense_weight,\n            },\n        }\n\n    def optimize_weights(\n        self, test_queries: List[str], ground_truth: Optional[List[List[str]]] = None\n    ) -> Dict[str, float]:\n        \"\"\"\n        Optimize BM25/dense weights based on test queries.\n\n        Args:\n            test_queries: List of test queries\n            ground_truth: Optional ground truth results for each query\n\n        Returns:\n            Optimized weights\n        \"\"\"\n        self._logger.info(f\"Optimizing weights with {len(test_queries)} test queries\")\n\n        weight_combinations = [\n            (0.2, 0.8),\n            (0.3, 0.7),\n            (0.4, 0.6),\n            (0.5, 0.5),\n            (0.6, 0.4),\n            (0.7, 0.3),\n            (0.8, 0.2),\n        ]\n\n        best_weights = (self.bm25_weight, self.dense_weight)\n        best_score = 0.0\n\n        for bm25_w, dense_w in weight_combinations:\n            # Temporarily set weights\n            orig_bm25_w, orig_dense_w = self.bm25_weight, self.dense_weight\n            self.bm25_weight, self.dense_weight = bm25_w, dense_w\n\n            total_score = 0.0\n            for _i, query in enumerate(test_queries):\n                results = self.search(query, k=10, use_parallel=False)\n\n                # Score based on result quality metrics\n                if results:\n                    analysis = self.reranker.analyze_fusion_quality(results)\n                    score = (\n                        analysis[\"diversity_score\"] * 0.4\n                        + analysis[\"coverage_balance\"] * 0.3\n                        + analysis[\"high_quality_ratio\"] * 0.3\n                    )\n                    total_score += score\n\n            avg_score = total_score / len(test_queries) if test_queries else 0.0\n\n            if avg_score > best_score:\n                best_score = avg_score\n                best_weights = (bm25_w, dense_w)\n\n            # Restore original weights\n            self.bm25_weight, self.dense_weight = orig_bm25_w, orig_dense_w\n\n        # Set optimal weights\n        self.bm25_weight, self.dense_weight = best_weights\n\n        self._logger.info(\n            f\"Optimized weights: BM25={self.bm25_weight:.2f}, \"\n            f\"Dense={self.dense_weight:.2f} (score: {best_score:.3f})\"\n        )\n\n        return {\n            \"bm25_weight\": self.bm25_weight,\n            \"dense_weight\": self.dense_weight,\n            \"optimization_score\": best_score,\n            \"tested_combinations\": len(weight_combinations),\n        }\n\n    def save_indices(self) -> None:\n        \"\"\"Save both BM25 and dense indices.\"\"\"\n        try:\n            self._logger.info(\"[SAVE] Starting save operation\")\n\n            # Log comprehensive state before save\n            bm25_dir = self.storage_dir / \"bm25\"\n            dense_size = self.dense_index.index.ntotal if self.dense_index.index else 0\n\n            self._logger.info(\"[SAVE] === PRE-SAVE STATE ===\")\n            self._logger.info(f\"[SAVE] BM25 directory exists: {bm25_dir.exists()}\")\n            self._logger.info(\n                f\"[SAVE] BM25 index size: {self.bm25_index.size} documents\"\n            )\n            self._logger.info(\n                f\"[SAVE] BM25 has index: {self.bm25_index._bm25 is not None}\"\n            )\n            self._logger.info(\n                f\"[SAVE] BM25 tokenized docs: {len(self.bm25_index._tokenized_docs)}\"\n            )\n            self._logger.info(f\"[SAVE] Dense index size: {dense_size} vectors\")\n            self._logger.info(\n                f\"[SAVE] Dense has index: {self.dense_index.index is not None}\"\n            )\n            self._logger.info(f\"[SAVE] Overall ready state: {self.is_ready}\")\n            self._logger.info(\"[SAVE] === END PRE-SAVE STATE ===\")\n\n            # Log BM25 state before save (keep original logging for compatibility)\n            self._logger.info(f\"[SAVE] BM25 size before save: {self.bm25_index.size}\")\n\n            # Save BM25 index\n            if hasattr(self.bm25_index, \"save\"):\n                self._logger.info(\"[SAVE] Calling BM25 index save...\")\n                self.bm25_index.save()\n                self._logger.info(\"[SAVE] BM25 index save completed\")\n            else:\n                self._logger.warning(\"[SAVE] BM25 index does not support saving\")\n\n            # Save dense index\n            if hasattr(self.dense_index, \"save_index\"):\n                self._logger.info(\"[SAVE] Calling dense index save_index...\")\n                self.dense_index.save_index()\n                self._logger.info(\"[SAVE] Dense index save completed\")\n            elif hasattr(self.dense_index, \"save\"):\n                self._logger.info(\"[SAVE] Calling dense index save...\")\n                self.dense_index.save()\n                self._logger.info(\"[SAVE] Dense index save completed\")\n            else:\n                self._logger.warning(\"[SAVE] Dense index does not support saving\")\n\n            # Verify files after save\n            self._verify_bm25_files()\n\n            # Log comprehensive state after save\n            bm25_dir = self.storage_dir / \"bm25\"\n            dense_size_after = (\n                self.dense_index.index.ntotal if self.dense_index.index else 0\n            )\n\n            self._logger.info(\"[SAVE] === POST-SAVE STATE ===\")\n            self._logger.info(f\"[SAVE] BM25 directory exists: {bm25_dir.exists()}\")\n            if bm25_dir.exists():\n                files = list(bm25_dir.iterdir())\n                self._logger.info(f\"[SAVE] BM25 files: {[f.name for f in files]}\")\n            self._logger.info(\n                f\"[SAVE] BM25 index size: {self.bm25_index.size} documents\"\n            )\n            self._logger.info(\n                f\"[SAVE] BM25 has index: {self.bm25_index._bm25 is not None}\"\n            )\n            self._logger.info(f\"[SAVE] Dense index size: {dense_size_after} vectors\")\n            self._logger.info(\n                f\"[SAVE] Dense has index: {self.dense_index.index is not None}\"\n            )\n            self._logger.info(f\"[SAVE] Overall ready state: {self.is_ready}\")\n            self._logger.info(\"[SAVE] === END POST-SAVE STATE ===\")\n\n            self._logger.info(\"[SAVE] Hybrid indices saved successfully\")\n        except Exception as e:\n            self._logger.error(f\"[SAVE] Failed to save indices: {e}\")\n            raise\n\n    def load_indices(self) -> bool:\n        \"\"\"Load both BM25 and dense indices.\"\"\"\n        try:\n            bm25_loaded = self.bm25_index.load()\n            dense_loaded = self.dense_index.load()\n\n            success = bm25_loaded and dense_loaded\n            if success:\n                self._logger.info(\"Hybrid indices loaded successfully\")\n            else:\n                self._logger.warning(\n                    f\"Index loading partial: BM25={bm25_loaded}, Dense={dense_loaded}\"\n                )\n\n            return success\n\n        except Exception as e:\n            self._logger.error(f\"Failed to load indices: {e}\")\n            return False\n\n    def add_embeddings(self, embedding_results) -> None:\n        \"\"\"\n        Add embeddings to both BM25 and dense indices.\n        Compatible with incremental indexer interface.\n\n        Args:\n            embedding_results: List of EmbeddingResult objects\n        \"\"\"\n        if not embedding_results:\n            self._logger.debug(\"[ADD_EMBEDDINGS] No embedding results provided\")\n            return\n\n        self._logger.info(\n            f\"[ADD_EMBEDDINGS] Called with {len(embedding_results)} results\"\n        )\n        self._logger.debug(f\"[ADD_EMBEDDINGS] Storage directory: {self.storage_dir}\")\n        self._logger.debug(\n            f\"[ADD_EMBEDDINGS] BM25 index path: {self.storage_dir / 'bm25'}\"\n        )\n        self._logger.debug(f\"[ADD_EMBEDDINGS] Dense index path: {self.storage_dir}\")\n\n        # Extract data for both indices\n        documents = []\n        doc_ids = []\n        embeddings = []\n        metadata = {}\n\n        for result in embedding_results:\n            doc_id = result.chunk_id\n            doc_ids.append(doc_id)\n\n            # Extract text content for BM25 (from metadata or content)\n            content = result.metadata.get(\"content\", \"\")\n            if not content:\n                # Fallback: try other content fields\n                content = (\n                    result.metadata.get(\"content_preview\", \"\")\n                    or result.metadata.get(\"raw_content\", \"\")\n                    or \"\"\n                )\n            documents.append(content)\n\n            # Embeddings for dense index\n            if hasattr(result.embedding, \"tolist\"):\n                embeddings.append(result.embedding.tolist())\n            else:\n                embeddings.append(list(result.embedding))\n\n            # Metadata for both indices\n            metadata[doc_id] = result.metadata\n\n        # Log data extraction\n        self._logger.debug(f\"[ADD_EMBEDDINGS] Extracted {len(documents)} documents\")\n        self._logger.debug(\n            f\"[ADD_EMBEDDINGS] First doc sample: {documents[0][:100] if documents else 'EMPTY'}...\"\n        )\n\n        # Index in both systems using existing method\n        try:\n            # Log before calling index_documents\n            self._logger.info(\n                f\"[ADD_EMBEDDINGS] Calling index_documents with {len(documents)} docs\"\n            )\n\n            self.index_documents(documents, doc_ids, embeddings, metadata)\n\n            self._logger.info(\n                f\"[ADD_EMBEDDINGS] Successfully added {len(embedding_results)} embeddings to hybrid index\"\n            )\n            self._logger.debug(\n                f\"[ADD_EMBEDDINGS] BM25 index size after adding: {self.bm25_index.size}\"\n            )\n            self._logger.debug(\n                f\"[ADD_EMBEDDINGS] Dense index size after adding: {self.dense_index.index.ntotal if self.dense_index.index else 0}\"\n            )\n\n        except Exception as e:\n            self._logger.error(\n                f\"[ADD_EMBEDDINGS] Failed to add embeddings to hybrid index: {e}\"\n            )\n            raise\n\n    def clear_index(self) -> None:\n        \"\"\"\n        Clear both BM25 and dense indices.\n        Compatible with incremental indexer interface.\n        \"\"\"\n        self._logger.info(\"Clearing hybrid indices\")\n\n        try:\n            # Clear BM25 index by recreating it\n            self.bm25_index = BM25Index(str(self.storage_dir / \"bm25\"))\n\n            # Clear dense index by recreating it\n            self.dense_index = CodeIndexManager(str(self.storage_dir))\n\n            self._logger.info(\"Successfully cleared hybrid indices\")\n        except Exception as e:\n            self._logger.error(f\"Failed to clear hybrid indices: {e}\")\n            raise\n\n    def remove_file_chunks(self, file_path: str, project_name: str) -> int:\n        \"\"\"\n        Remove chunks for a specific file from both indices.\n        Compatible with incremental indexer interface.\n\n        Args:\n            file_path: Relative path of the file\n            project_name: Name of the project\n\n        Returns:\n            Number of chunks removed\n        \"\"\"\n        self._logger.debug(f\"Removing chunks for file: {file_path}\")\n\n        try:\n            removed_count = 0\n\n            # Remove from dense index\n            if hasattr(self.dense_index, \"remove_file_chunks\"):\n                removed_dense = self.dense_index.remove_file_chunks(\n                    file_path, project_name\n                )\n                removed_count += removed_dense\n                self._logger.debug(f\"Removed {removed_dense} chunks from dense index\")\n\n            # Remove from BM25 index\n            if hasattr(self.bm25_index, \"remove_file_chunks\"):\n                removed_bm25 = self.bm25_index.remove_file_chunks(\n                    file_path, project_name\n                )\n                removed_count += removed_bm25\n                self._logger.debug(f\"Removed {removed_bm25} chunks from BM25 index\")\n            else:\n                self._logger.warning(\"BM25 index does not support file chunk removal\")\n\n            self._logger.info(\n                f\"Removed {removed_count} total chunks for file: {file_path}\"\n            )\n            return removed_count\n\n        except Exception as e:\n            self._logger.error(f\"Failed to remove chunks for file {file_path}: {e}\")\n            return 0\n\n    def remove_multiple_files(self, file_paths: set, project_name: str) -> int:\n        \"\"\"\n        Remove chunks for multiple files from both indices in a single pass.\n        Much faster than calling remove_file_chunks repeatedly.\n\n        IMPORTANT: This method properly removes chunks from both FAISS and BM25 indices,\n        preventing index corruption.\n\n        Args:\n            file_paths: Set of file paths to remove\n            project_name: Name of the project\n\n        Returns:\n            Total number of chunks removed\n        \"\"\"\n        self._logger.info(\n            f\"Batch removing chunks for {len(file_paths)} files from hybrid indices\"\n        )\n\n        removed_count = 0\n        dense_failed = False\n        bm25_failed = False\n\n        # Remove from dense index\n        if hasattr(self.dense_index, \"remove_multiple_files\"):\n            try:\n                removed_dense = self.dense_index.remove_multiple_files(\n                    file_paths, project_name\n                )\n                removed_count += removed_dense\n                self._logger.info(\n                    f\"Batch removed {removed_dense} chunks from dense (FAISS) index\"\n                )\n            except Exception as e:\n                self._logger.error(f\"Failed to batch remove from dense index: {e}\")\n                import traceback\n                self._logger.error(traceback.format_exc())\n                dense_failed = True\n\n        # Remove from BM25 index\n        if hasattr(self.bm25_index, \"remove_multiple_files\"):\n            try:\n                removed_bm25 = self.bm25_index.remove_multiple_files(\n                    file_paths, project_name\n                )\n                removed_count += removed_bm25\n                self._logger.info(\n                    f\"Batch removed {removed_bm25} chunks from BM25 index\"\n                )\n            except Exception as e:\n                self._logger.error(f\"Failed to batch remove from BM25 index: {e}\")\n                import traceback\n                self._logger.error(traceback.format_exc())\n                bm25_failed = True\n        else:\n            self._logger.warning(\n                \"BM25 index does not support batch file chunk removal\"\n            )\n\n        # If both failed, raise exception to trigger error recovery\n        if dense_failed and bm25_failed:\n            raise RuntimeError(\n                \"Batch removal failed for both dense and BM25 indices. \"\n                \"Indices may be in corrupted state.\"\n            )\n\n        self._logger.info(\n            f\"Batch removed {removed_count} total chunks for {len(file_paths)} files\"\n        )\n        return removed_count\n\n    def save_index(self) -> None:\n        \"\"\"\n        Save both BM25 and dense indices to disk.\n        Compatible with incremental indexer interface.\n        \"\"\"\n        self._logger.info(\"Saving hybrid indices\")\n\n        try:\n            # Save both indices\n            self.save_indices()\n            self._logger.info(\"Successfully saved hybrid indices\")\n        except Exception as e:\n            self._logger.error(f\"Failed to save hybrid indices: {e}\")\n            raise\n\n    def _verify_bm25_files(self):\n        \"\"\"Verify BM25 files exist and are non-empty.\"\"\"\n        bm25_dir = Path(self.bm25_index.storage_dir)\n        expected_files = [\"bm25.index\", \"bm25_docs.json\", \"bm25_metadata.json\"]\n\n        self._logger.info(f\"[VERIFY] Checking BM25 files in: {bm25_dir}\")\n\n        for filename in expected_files:\n            filepath = bm25_dir / filename\n            if filepath.exists():\n                size = filepath.stat().st_size\n                if size == 0:\n                    self._logger.error(f\"[VERIFY] {filename} exists but is EMPTY\")\n                else:\n                    self._logger.info(f\"[VERIFY] {filename}: {size} bytes\")\n            else:\n                self._logger.error(f\"[VERIFY] {filename} does NOT exist\")\n\n        # Log overall BM25 directory status\n        if bm25_dir.exists():\n            files = list(bm25_dir.iterdir())\n            self._logger.info(\n                f\"[VERIFY] BM25 files after save: {[f.name for f in files]}\"\n            )\n            for f in files:\n                self._logger.info(f\"[VERIFY] {f.name}: {f.stat().st_size} bytes\")\n        else:\n            self._logger.error(\"[VERIFY] BM25 directory does not exist after save!\")",
              "content_preview": "class HybridSearcher:\n    \"\"\"Orchestrates BM25 + dense search with GPU awareness and parallel execution.\"\"\"\n\n    def __init__(\n        self,\n        storage_dir: str,\n        embedder=None,\n        bm...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015156871409633229,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "evaluation\\semantic_evaluator.py:20-66:method:__init__",
            "score": 10.192,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\evaluation\\semantic_evaluator.py",
              "relative_path": "evaluation\\semantic_evaluator.py",
              "folder_structure": [
                "evaluation"
              ],
              "chunk_type": "method",
              "start_line": 20,
              "end_line": 66,
              "name": "__init__",
              "parent_name": "SemanticSearchEvaluator",
              "docstring": "Initialize semantic search evaluator.\n\n        Args:\n            output_dir: Output directory for results\n            storage_dir: Directory for storing search indices\n            max_instances: Maximum number of instances to evaluate\n            k: Number of top results to consider\n            use_gpu: Whether to use GPU acceleration\n            bm25_weight: Weight for BM25 results in hybrid search\n            dense_weight: Weight for dense vector results in hybrid search\n            rrf_k: RRF parameter for reranking (lower = more emphasis on top ranks)",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def __init__(\n        self,\n        output_dir: str,\n        storage_dir: Optional[str] = None,\n        max_instances: Optional[int] = None,\n        k: int = 10,\n        use_gpu: bool = True,\n        bm25_weight: float = 0.4,\n        dense_weight: float = 0.6,\n        rrf_k: int = 60,\n    ):\n        \"\"\"\n        Initialize semantic search evaluator.\n\n        Args:\n            output_dir: Output directory for results\n            storage_dir: Directory for storing search indices\n            max_instances: Maximum number of instances to evaluate\n            k: Number of top results to consider\n            use_gpu: Whether to use GPU acceleration\n            bm25_weight: Weight for BM25 results in hybrid search\n            dense_weight: Weight for dense vector results in hybrid search\n            rrf_k: RRF parameter for reranking (lower = more emphasis on top ranks)\n        \"\"\"\n        super().__init__(output_dir, max_instances, k)\n\n        self.storage_dir = storage_dir or str(Path(output_dir) / \"search_indices\")\n        self.use_gpu = use_gpu\n        self.bm25_weight = bm25_weight\n        self.dense_weight = dense_weight\n        self.rrf_k = rrf_k\n\n        # Initialize logger first\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n        # Initialize components\n        self.hybrid_searcher: Optional[HybridSearcher] = None\n        self.chunker = MultiLanguageChunker()\n        device = \"auto\" if use_gpu else \"cpu\"\n\n        # Read model from configuration (respects CLAUDE_EMBEDDING_MODEL environment variable)\n        config = get_search_config()\n        model_name = config.embedding_model_name\n        self.logger.info(\n            f\"Creating embedder with model: {model_name} ({config.model_dimension} dimensions)\"\n        )\n        self.embedder = CodeEmbedder(model_name=model_name, device=device)",
              "content_preview": "def __init__(\n        self,\n        output_dir: str,\n        storage_dir: Optional[str] = None,\n        max_instances: Optional[int] = None,\n        k: int = 10,\n        use_gpu: bool = True,\n        ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014780405405405405,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
            "score": 13.37,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 1284,
              "end_line": 1363,
              "name": "configure_search_mode",
              "parent_name": null,
              "docstring": "Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes\n    \"\"\"\n    try:\n        # Validate search mode\n        valid_modes = [\"hybrid\", \"semantic\", \"bm25\", \"auto\"]\n        if search_mode not in valid_modes:\n            return json.dumps(\n                {\n                    \"error\": f\"Invalid search_mode '{search_mode}'. Must be one of: {valid_modes}\"\n                }\n            )\n\n        # Validate weights\n        if not (0.0 <= bm25_weight <= 1.0) or not (0.0 <= dense_weight <= 1.0):\n            return json.dumps({\"error\": \"Weights must be between 0.0 and 1.0\"})\n\n        # Normalize weights if they don't sum to 1.0\n        total_weight = bm25_weight + dense_weight\n        if total_weight > 0:\n            bm25_weight = bm25_weight / total_weight\n            dense_weight = dense_weight / total_weight\n\n        # Get current config and update\n        config_manager = get_config_manager()\n        config = config_manager.load_config()\n\n        # Update configuration\n        config.default_search_mode = search_mode\n        config.enable_hybrid_search = search_mode == \"hybrid\"\n        config.bm25_weight = bm25_weight\n        config.dense_weight = dense_weight\n        config.use_parallel_search = enable_parallel\n\n        # Save configuration\n        config_manager.save_config(config)\n\n        # Reset searcher to pick up new configuration\n        global _searcher\n        _searcher = None\n\n        response = {\n            \"success\": True,\n            \"message\": \"Search configuration updated successfully\",\n            \"new_config\": {\n                \"search_mode\": search_mode,\n                \"enable_hybrid_search\": config.enable_hybrid_search,\n                \"bm25_weight\": round(bm25_weight, 3),\n                \"dense_weight\": round(dense_weight, 3),\n                \"use_parallel_search\": enable_parallel,\n            },\n            \"note\": \"Changes will take effect on next search. Searcher will be reinitialized.\",\n        }\n\n        logger.info(\n            f\"Search configuration updated: mode={search_mode}, \"\n            f\"weights=({bm25_weight:.3f}, {dense_weight:.3f}), parallel={enable_parallel}\"\n        )\n\n        return json.dumps(response, indent=2)\n\n    except Exception as e:\n        logger.error(f\"Error configuring search mode: {e}\")\n        return json.dumps({\"error\": str(e)})",
              "content_preview": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure s...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014725274725274726,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:60-157:method:__init__",
            "score": 9.954,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 60,
              "end_line": 157,
              "name": "__init__",
              "parent_name": "HybridSearcher",
              "docstring": "Initialize hybrid searcher.\n\n        Args:\n            storage_dir: Directory for storing indices\n            embedder: CodeEmbedder instance for semantic search (optional)\n            bm25_weight: Weight for BM25 results (0.0 to 1.0)\n            dense_weight: Weight for dense vector results (0.0 to 1.0)\n            rrf_k: RRF parameter for reranking\n            max_workers: Maximum thread pool workers for parallel execution",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def __init__(\n        self,\n        storage_dir: str,\n        embedder=None,\n        bm25_weight: float = 0.4,\n        dense_weight: float = 0.6,\n        rrf_k: int = 60,\n        max_workers: int = 2,\n    ):\n        \"\"\"\n        Initialize hybrid searcher.\n\n        Args:\n            storage_dir: Directory for storing indices\n            embedder: CodeEmbedder instance for semantic search (optional)\n            bm25_weight: Weight for BM25 results (0.0 to 1.0)\n            dense_weight: Weight for dense vector results (0.0 to 1.0)\n            rrf_k: RRF parameter for reranking\n            max_workers: Maximum thread pool workers for parallel execution\n        \"\"\"\n        self.storage_dir = Path(storage_dir)\n        self.storage_dir.mkdir(parents=True, exist_ok=True)\n\n        # Store embedder for semantic search\n        self.embedder = embedder\n\n        # Weights\n        self.bm25_weight = bm25_weight\n        self.dense_weight = dense_weight\n\n        # Components - use existing storage structure\n        self._logger = logging.getLogger(__name__)\n\n        # BM25 index gets its own subdirectory\n        self._logger.info(f\"[INIT] Creating BM25Index at: {self.storage_dir / 'bm25'}\")\n        try:\n            self.bm25_index = BM25Index(str(self.storage_dir / \"bm25\"))\n            self._logger.info(\"[INIT] BM25Index created successfully\")\n        except Exception as e:\n            self._logger.error(f\"[INIT] Failed to create BM25Index: {e}\")\n            raise\n\n        # Try to load existing BM25 index\n        self._logger.info(f\"[INIT] BM25 storage path: {self.storage_dir / 'bm25'}\")\n        bm25_loaded = self.bm25_index.load()\n        if bm25_loaded:\n            self._logger.info(\n                f\"[INIT] Loaded existing BM25 index with {self.bm25_index.size} documents\"\n            )\n        else:\n            self._logger.info(\"[INIT] No existing BM25 index found, starting fresh\")\n            # Log what files we're looking for\n            bm25_dir = self.storage_dir / \"bm25\"\n            self._logger.debug(f\"[INIT] BM25 directory exists: {bm25_dir.exists()}\")\n            if bm25_dir.exists():\n                files = list(bm25_dir.iterdir())\n                self._logger.debug(\n                    f\"[INIT] BM25 files found: {[f.name for f in files]}\"\n                )\n\n        # Dense index uses the main storage directory where existing indices are stored\n        self._logger.info(f\"[INIT] Initializing dense index at: {self.storage_dir}\")\n        self.dense_index = CodeIndexManager(str(self.storage_dir))\n        # Dense index loads automatically in its __init__\n        dense_count = self.dense_index.index.ntotal if self.dense_index.index else 0\n        if dense_count > 0:\n            self._logger.info(\n                f\"[INIT] Loaded existing dense index with {dense_count} vectors\"\n            )\n        else:\n            self._logger.info(\"[INIT] No existing dense index found, starting fresh\")\n\n        # Log final initialization status\n        total_bm25 = self.bm25_index.size\n        self._logger.info(\n            f\"[INIT] HybridSearcher initialized - BM25: {total_bm25} docs, Dense: {dense_count} vectors\"\n        )\n        self._logger.info(\n            f\"[INIT] Ready status: BM25={not self.bm25_index.is_empty}, Dense={dense_count > 0}, Overall={self.is_ready}\"\n        )\n\n        self.reranker = RRFReranker(k=rrf_k)\n        self.gpu_monitor = GPUMemoryMonitor()\n\n        # Threading\n        self.max_workers = max_workers\n        self._thread_pool = ThreadPoolExecutor(max_workers=max_workers)\n        self._shutdown_lock = threading.Lock()\n        self._is_shutdown = False\n\n        # Performance tracking\n        self._search_stats = {\n            \"total_searches\": 0,\n            \"bm25_time\": 0.0,\n            \"dense_time\": 0.0,\n            \"rerank_time\": 0.0,\n            \"parallel_efficiency\": 0.0,\n        }",
              "content_preview": "def __init__(\n        self,\n        storage_dir: str,\n        embedder=None,\n        bm25_weight: float = 0.4,\n        dense_weight: float = 0.6,\n        rrf_k: int = 60,\n        max_workers: int = 2,...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014718614718614718,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
          "evaluation\\semantic_evaluator.py:378-428:method:search",
          "evaluation\\semantic_evaluator.py:17-306:class:SemanticSearchEvaluator",
          "tests\\integration\\test_hybrid_search_integration.py:323-355:decorated_definition:test_hybrid_reranking_combines_results",
          "evaluation\\semantic_evaluator.py:20-66:method:__init__",
          "search\\hybrid_searcher.py:301-426:method:search",
          "search\\hybrid_searcher.py:57-1252:class:HybridSearcher",
          "search\\config.py:30-98:decorated_definition:SearchConfig",
          "tests\\integration\\test_installation.py:417-428:method:test_hybrid_search_verification",
          "search\\hybrid_searcher.py:60-157:method:__init__"
        ]
      },
      "multi_hop": {
        "time_ms": 49.73,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "search\\hybrid_searcher.py:301-426:method:search",
            "score": 13.669,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 301,
              "end_line": 426,
              "name": "search",
              "parent_name": "HybridSearcher",
              "docstring": "Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)\n        \"\"\"\n        # Check if indices are ready based on search mode\n        if search_mode == \"bm25\":\n            if self.bm25_index.is_empty:\n                self._logger.warning(\"BM25 search requested but BM25 index is empty\")\n                return []\n        elif search_mode == \"semantic\":\n            if not self.dense_index.index or self.dense_index.index.ntotal == 0:\n                self._logger.warning(\n                    \"Semantic search requested but dense index is empty\"\n                )\n                return []\n        else:  # hybrid\n            if not self.is_ready:\n                self._logger.warning(\"Hybrid search not ready - indices may be empty\")\n                return []\n\n        self._logger.debug(f\"{search_mode.title()} search for: '{query}' (k={k})\")\n\n        start_time = time.time()\n\n        # Handle different search modes\n        if search_mode == \"bm25\":\n            # BM25-only search\n            bm25_results = self._search_bm25(query, k, min_bm25_score)\n            # Convert BM25 results to SearchResult format\n            final_results = self._convert_bm25_to_search_results(bm25_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        elif search_mode == \"semantic\":\n            # Dense-only search\n            dense_results = self._search_dense(query, k, filters)\n            # Convert dense results to SearchResult format\n            final_results = self._convert_dense_to_search_results(dense_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        else:  # hybrid mode\n            search_k = k * 2  # Get more results for better reranking\n\n            if use_parallel and not self._is_shutdown:\n                # Parallel execution\n                bm25_results, dense_results = self._parallel_search(\n                    query, search_k, min_bm25_score, filters\n                )\n            else:\n                # Sequential execution\n                bm25_results, dense_results = self._sequential_search(\n                    query, search_k, min_bm25_score, filters\n                )\n\n            # Rerank results\n            rerank_start = time.time()\n            self._logger.debug(\n                f\"[RERANK] Using weights: BM25={self.bm25_weight}, Dense={self.dense_weight}, \"\n                f\"BM25_results={len(bm25_results)}, Dense_results={len(dense_results)}\"\n            )\n            final_results = self.reranker.rerank_simple(\n                bm25_results=bm25_results,\n                dense_results=dense_results,\n                max_results=k,\n                bm25_weight=self.bm25_weight,\n                dense_weight=self.dense_weight,\n            )\n            rerank_time = time.time() - rerank_start\n            self._logger.debug(\n                f\"[RERANK] Produced {len(final_results)} results in {rerank_time:.3f}s\"\n            )\n\n        # Update statistics\n        total_time = time.time() - start_time\n        self._search_stats[\"total_searches\"] += 1\n        self._search_stats[\"rerank_time\"] += rerank_time\n\n        if use_parallel:\n            parallel_time = max(\n                self._search_stats.get(\"last_bm25_time\", 0),\n                self._search_stats.get(\"last_dense_time\", 0),\n            )\n            sequential_time = self._search_stats.get(\n                \"last_bm25_time\", 0\n            ) + self._search_stats.get(\"last_dense_time\", 0)\n            if sequential_time > 0:\n                efficiency = 1.0 - (parallel_time / sequential_time)\n                self._search_stats[\"parallel_efficiency\"] = efficiency\n\n        # Mode-specific logging\n        if search_mode == \"bm25\":\n            self._logger.debug(\n                f\"BM25 search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        elif search_mode == \"semantic\":\n            self._logger.debug(\n                f\"Semantic search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        else:  # hybrid\n            self._logger.debug(\n                f\"Hybrid search complete: {len(final_results)} results, \"\n                f\"BM25: {len(bm25_results)}, Dense: {len(dense_results)}, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n\n        return final_results",
              "content_preview": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional...",
              "project_name": "claude-context-local",
              "rrf_score": 0.0162876784769963,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\unit\\test_search_config.py:120-139:method:test_auto_mode_detection",
            "score": 13.391,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_search_config.py",
              "relative_path": "tests\\unit\\test_search_config.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 120,
              "end_line": 139,
              "name": "test_auto_mode_detection",
              "parent_name": "TestSearchConfigManager",
              "docstring": "Test automatic search mode detection.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_auto_mode_detection(self):\n        \"\"\"Test automatic search mode detection.\"\"\"\n        # Create config with auto mode\n        test_config = {\"default_search_mode\": \"auto\"}\n        with open(self.config_file, \"w\") as f:\n            json.dump(test_config, f)\n\n        manager = SearchConfigManager(self.config_file)\n\n        # Text-heavy query should suggest BM25\n        mode = manager.get_search_mode_for_query(\"find error message text\")\n        assert mode == \"bm25\"\n\n        # Code structure query should suggest semantic\n        mode = manager.get_search_mode_for_query(\"find class definition\")\n        assert mode == \"semantic\"\n\n        # Generic query should default to hybrid\n        mode = manager.get_search_mode_for_query(\"database connection\")\n        assert mode == \"hybrid\"",
              "content_preview": "def test_auto_mode_detection(self):\n        \"\"\"Test automatic search mode detection.\"\"\"\n        # Create config with auto mode\n        test_config = {\"default_search_mode\": \"auto\"}\n        with open(s...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01225,
              "appears_in_lists": 2,
              "final_rank": 13
            }
          },
          {
            "doc_id": "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
            "score": 13.37,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 1284,
              "end_line": 1363,
              "name": "configure_search_mode",
              "parent_name": null,
              "docstring": "Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes\n    \"\"\"\n    try:\n        # Validate search mode\n        valid_modes = [\"hybrid\", \"semantic\", \"bm25\", \"auto\"]\n        if search_mode not in valid_modes:\n            return json.dumps(\n                {\n                    \"error\": f\"Invalid search_mode '{search_mode}'. Must be one of: {valid_modes}\"\n                }\n            )\n\n        # Validate weights\n        if not (0.0 <= bm25_weight <= 1.0) or not (0.0 <= dense_weight <= 1.0):\n            return json.dumps({\"error\": \"Weights must be between 0.0 and 1.0\"})\n\n        # Normalize weights if they don't sum to 1.0\n        total_weight = bm25_weight + dense_weight\n        if total_weight > 0:\n            bm25_weight = bm25_weight / total_weight\n            dense_weight = dense_weight / total_weight\n\n        # Get current config and update\n        config_manager = get_config_manager()\n        config = config_manager.load_config()\n\n        # Update configuration\n        config.default_search_mode = search_mode\n        config.enable_hybrid_search = search_mode == \"hybrid\"\n        config.bm25_weight = bm25_weight\n        config.dense_weight = dense_weight\n        config.use_parallel_search = enable_parallel\n\n        # Save configuration\n        config_manager.save_config(config)\n\n        # Reset searcher to pick up new configuration\n        global _searcher\n        _searcher = None\n\n        response = {\n            \"success\": True,\n            \"message\": \"Search configuration updated successfully\",\n            \"new_config\": {\n                \"search_mode\": search_mode,\n                \"enable_hybrid_search\": config.enable_hybrid_search,\n                \"bm25_weight\": round(bm25_weight, 3),\n                \"dense_weight\": round(dense_weight, 3),\n                \"use_parallel_search\": enable_parallel,\n            },\n            \"note\": \"Changes will take effect on next search. Searcher will be reinitialized.\",\n        }\n\n        logger.info(\n            f\"Search configuration updated: mode={search_mode}, \"\n            f\"weights=({bm25_weight:.3f}, {dense_weight:.3f}), parallel={enable_parallel}\"\n        )\n\n        return json.dumps(response, indent=2)\n\n    except Exception as e:\n        logger.error(f\"Error configuring search mode: {e}\")\n        return json.dumps({\"error\": str(e)})",
              "content_preview": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure s...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014725274725274726,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "search\\config.py:222-254:method:get_search_mode_for_query",
            "score": 12.807,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\config.py",
              "relative_path": "search\\config.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 222,
              "end_line": 254,
              "name": "get_search_mode_for_query",
              "parent_name": "SearchConfigManager",
              "docstring": "Determine best search mode for a query.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def get_search_mode_for_query(\n        self, query: str, explicit_mode: Optional[str] = None\n    ) -> str:\n        \"\"\"Determine best search mode for a query.\"\"\"\n        config = self.load_config()\n\n        # Use explicit mode if provided\n        if explicit_mode and explicit_mode != \"auto\":\n            return explicit_mode\n\n        # Use default mode if not auto\n        if config.default_search_mode != \"auto\":\n            return config.default_search_mode\n\n        # Auto-detect based on query characteristics\n        query_lower = query.lower()\n\n        # Text-heavy queries -> BM25\n        if any(\n            keyword in query_lower\n            for keyword in [\"text\", \"string\", \"message\", \"error\", \"log\"]\n        ):\n            return \"bm25\"\n\n        # Code structure queries -> semantic\n        if any(\n            keyword in query_lower\n            for keyword in [\"class\", \"function\", \"method\", \"interface\"]\n        ):\n            return \"semantic\"\n\n        # Default to hybrid for balanced approach\n        return \"hybrid\"",
              "content_preview": "def get_search_mode_for_query(\n        self, query: str, explicit_mode: Optional[str] = None\n    ) -> str:\n        \"\"\"Determine best search mode for a query.\"\"\"\n        config = self.load_config()\n\n  ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012004801920768308,
              "appears_in_lists": 2,
              "final_rank": 14
            }
          },
          {
            "doc_id": "search\\config.py:30-98:decorated_definition:SearchConfig",
            "score": 10.848,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\config.py",
              "relative_path": "search\\config.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 30,
              "end_line": 98,
              "name": "SearchConfig",
              "parent_name": null,
              "docstring": "Configuration for search behavior.",
              "decorators": [
                "@dataclass"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@dataclass\nclass SearchConfig:\n    \"\"\"Configuration for search behavior.\"\"\"\n\n    # Embedding Model Configuration\n    embedding_model_name: str = \"google/embeddinggemma-300m\"\n    model_dimension: int = 768\n    embedding_batch_size: int = 128  # Dynamic based on model, see MODEL_REGISTRY\n\n    # Search Mode Configuration\n    default_search_mode: str = \"hybrid\"  # hybrid, semantic, bm25, auto\n    enable_hybrid_search: bool = True\n\n    # Hybrid Search Weights\n    bm25_weight: float = 0.4\n    dense_weight: float = 0.6\n\n    # Performance Settings\n    use_parallel_search: bool = True\n    max_parallel_workers: int = 2\n\n    # BM25 Configuration\n    bm25_k_parameter: int = 100\n    bm25_use_stopwords: bool = True\n    min_bm25_score: float = 0.1\n\n    # Reranking Configuration\n    rrf_k_parameter: int = 100\n    enable_result_reranking: bool = True\n\n    # GPU Configuration\n    prefer_gpu: bool = True\n    gpu_memory_threshold: float = 0.8\n\n    # Auto-reindexing\n    enable_auto_reindex: bool = True\n    max_index_age_minutes: float = 5.0\n\n    # Multi-hop Search Configuration\n    enable_multi_hop: bool = True\n    multi_hop_count: int = 2\n    multi_hop_expansion: float = 0.3\n\n    # Search Result Limits\n    default_k: int = 5\n    max_k: int = 50\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"SearchConfig\":\n        \"\"\"Create from dictionary.\"\"\"\n        # Auto-update dimension and batch size if model is in registry\n        if \"embedding_model_name\" in data:\n            model_config = get_model_config(data[\"embedding_model_name\"])\n            if model_config:\n                data[\"model_dimension\"] = model_config[\"dimension\"]\n                # Only auto-set batch size if not explicitly provided\n                if \"embedding_batch_size\" not in data:\n                    data[\"embedding_batch_size\"] = model_config.get(\n                        \"recommended_batch_size\", 128\n                    )\n\n        # Filter only known fields to avoid TypeError\n        valid_fields = {f.name for f in cls.__dataclass_fields__.values()}\n        filtered_data = {k: v for k, v in data.items() if k in valid_fields}\n        return cls(**filtered_data)",
              "content_preview": "@dataclass\nclass SearchConfig:\n    \"\"\"Configuration for search behavior.\"\"\"\n\n    # Embedding Model Configuration\n    embedding_model_name: str = \"google/embeddinggemma-300m\"\n    model_dimension: int =...",
              "project_name": "claude-context-local",
              "rrf_score": 0.013528539659006671,
              "appears_in_lists": 2,
              "final_rank": 9
            }
          }
        ],
        "all_doc_ids": [
          "search\\config.py:222-254:method:get_search_mode_for_query",
          "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
          "tests\\integration\\test_hybrid_search_integration.py:323-355:decorated_definition:test_hybrid_reranking_combines_results",
          "evaluation\\semantic_evaluator.py:20-66:method:__init__",
          "search\\hybrid_searcher.py:301-426:method:search",
          "evaluation\\run_evaluation.py:188-287:function:run_method_comparison",
          "search\\hybrid_searcher.py:57-1252:class:HybridSearcher",
          "tests\\unit\\test_search_config.py:120-139:method:test_auto_mode_detection",
          "search\\config.py:30-98:decorated_definition:SearchConfig",
          "search\\hybrid_searcher.py:60-157:method:__init__"
        ],
        "unique_discoveries": [
          "search\\config.py:222-254:method:get_search_mode_for_query",
          "evaluation\\run_evaluation.py:188-287:function:run_method_comparison",
          "tests\\unit\\test_search_config.py:120-139:method:test_auto_mode_detection"
        ]
      },
      "comparison": {
        "time_overhead_ms": 31.81,
        "time_overhead_pct": 177.6,
        "top5_overlap_count": 2,
        "top5_overlap_pct": 40.0,
        "unique_discovery_count": 3,
        "value_rating": "MEDIUM"
      }
    },
    {
      "query": "configuration management system",
      "single_hop": {
        "time_ms": 21.91,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\test_data\\python_project\\src\\utils\\helpers.py:47-86:class:ConfigManager",
            "score": 5.378,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\test_data\\python_project\\src\\utils\\helpers.py",
              "relative_path": "tests\\test_data\\python_project\\src\\utils\\helpers.py",
              "folder_structure": [
                "tests",
                "test_data",
                "python_project",
                "src",
                "utils"
              ],
              "chunk_type": "class",
              "start_line": 47,
              "end_line": 86,
              "name": "ConfigManager",
              "parent_name": null,
              "docstring": "Manages application configuration.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class ConfigManager:\n    \"\"\"Manages application configuration.\"\"\"\n\n    def __init__(self, config_file: Optional[str] = None):\n        self.config_file = config_file\n        self.config = {}\n        self.logger = logging.getLogger(__name__)\n\n        if config_file and os.path.exists(config_file):\n            self.load_config()\n\n    def load_config(self) -> None:\n        \"\"\"Load configuration from file.\"\"\"\n        try:\n            with open(self.config_file, \"r\") as f:\n                self.config = json.load(f)\n            self.logger.info(f\"Configuration loaded from {self.config_file}\")\n        except (IOError, json.JSONDecodeError) as e:\n            self.logger.error(f\"Failed to load configuration: {e}\")\n            self.config = {}\n\n    def get(self, key: str, default: Any = None) -> Any:\n        \"\"\"Get configuration value.\"\"\"\n        return self.config.get(key, default)\n\n    def set(self, key: str, value: Any) -> None:\n        \"\"\"Set configuration value.\"\"\"\n        self.config[key] = value\n\n    def save_config(self) -> None:\n        \"\"\"Save configuration to file.\"\"\"\n        if not self.config_file:\n            raise ValueError(\"No config file specified\")\n\n        try:\n            with open(self.config_file, \"w\") as f:\n                json.dump(self.config, f, indent=2)\n            self.logger.info(f\"Configuration saved to {self.config_file}\")\n        except IOError as e:\n            self.logger.error(f\"Failed to save configuration: {e}\")",
              "content_preview": "class ConfigManager:\n    \"\"\"Manages application configuration.\"\"\"\n\n    def __init__(self, config_file: Optional[str] = None):\n        self.config_file = config_file\n        self.config = {}\n        se...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015806214827501837,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "scripts\\manual_configure.py:22-239:class:ClaudeConfigManager",
            "score": 5.004,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\manual_configure.py",
              "relative_path": "scripts\\manual_configure.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "class",
              "start_line": 22,
              "end_line": 239,
              "name": "ClaudeConfigManager",
              "parent_name": null,
              "docstring": "Manages Claude Code MCP server configuration.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class ClaudeConfigManager:\n    \"\"\"Manages Claude Code MCP server configuration.\"\"\"\n\n    def __init__(self, project_dir: Optional[Path] = None):\n        \"\"\"Initialize configuration manager.\"\"\"\n        self.project_dir = project_dir or Path.cwd()\n        self.config_path: Optional[Path] = None\n\n    def get_config_path(self, global_scope: bool = True) -> Path:\n        \"\"\"Get the path to the Claude configuration file.\"\"\"\n        if global_scope:\n            # Global config in user's home directory\n            home = Path.home()\n            return home / \".claude.json\"\n        else:\n            # Project-specific config\n            return self.project_dir / \".claude.json\"\n\n    def load_config(self, config_path: Path) -> Dict[str, Any]:\n        \"\"\"Load existing Claude configuration.\"\"\"\n        if not config_path.exists():\n            print(f\"[INFO] Config file not found: {config_path}\")\n            print(\"[INFO] Creating new configuration file...\")\n            return {}\n\n        try:\n            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                config = json.load(f)\n            print(f\"[OK] Loaded existing configuration from {config_path}\")\n            return config\n        except json.JSONDecodeError as e:\n            print(f\"[ERROR] Failed to parse config file: {e}\")\n            print(\"[WARNING] Creating backup and starting fresh...\")\n            backup_path = config_path.with_suffix(\".json.backup\")\n            if config_path.exists():\n                config_path.rename(backup_path)\n                print(f\"[OK] Backup created: {backup_path}\")\n            return {}\n        except Exception as e:\n            print(f\"[ERROR] Failed to load config: {e}\")\n            return {}\n\n    def save_config(self, config: Dict[str, Any], config_path: Path) -> bool:\n        \"\"\"Save Claude configuration to file.\"\"\"\n        try:\n            # Ensure parent directory exists\n            config_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Write with proper formatting\n            with open(config_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(config, f, indent=2, ensure_ascii=False)\n\n            print(f\"[OK] Configuration saved to {config_path}\")\n            return True\n        except Exception as e:\n            print(f\"[ERROR] Failed to save config: {e}\")\n            return False\n\n    def create_mcp_server_config(\n        self, command: str, args: list = None, env: Dict[str, str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Create MCP server configuration structure.\"\"\"\n        config = {\"type\": \"stdio\", \"command\": command}\n\n        # Always include args field, even if empty (required by Claude Code)\n        if args is not None:\n            config[\"args\"] = args\n        else:\n            config[\"args\"] = []\n\n        # Always include env field if provided (recommended by Claude Code)\n        if env:\n            config[\"env\"] = env\n\n        return config\n\n    def add_mcp_server(\n        self,\n        name: str,\n        command: str,\n        args: list = None,\n        env: Dict[str, str] = None,\n        global_scope: bool = True,\n        force: bool = False,\n        verbose: bool = False,\n    ) -> bool:\n        \"\"\"Add or update MCP server configuration.\"\"\"\n        config_path = self.get_config_path(global_scope)\n        self.config_path = config_path\n\n        print(\"\\n=== Claude Code MCP Manual Configuration ===\")\n        print(f\"Server Name: {name}\")\n        print(f\"Command: {command}\")\n        if args:\n            print(f\"Arguments: {' '.join(args)}\")\n        if env:\n            print(\"Environment Variables:\")\n            for key, value in env.items():\n                # Show truncated path for readability\n                display_value = value if len(value) < 60 else f\"{value[:57]}...\"\n                print(f\"  {key}: {display_value}\")\n        print(f\"Config File: {config_path}\")\n        print(\n            f\"Scope: {'Global (all projects)' if global_scope else 'Project-specific'}\"\n        )\n        print()\n\n        # Load existing config\n        config = self.load_config(config_path)\n\n        # Ensure mcpServers exists\n        if \"mcpServers\" not in config:\n            config[\"mcpServers\"] = {}\n\n        # Check if server already exists\n        if name in config[\"mcpServers\"] and not force:\n            print(f\"[WARNING] MCP server '{name}' already exists\")\n            print(\"Current configuration:\")\n            current = config[\"mcpServers\"][name]\n            print(f\"  Command: {current.get('command', 'N/A')}\")\n            if current.get(\"args\"):\n                print(f\"  Args: {' '.join(current['args'])}\")\n            if current.get(\"env\"):\n                print(f\"  Environment variables: {len(current['env'])} set\")\n            print()\n\n            response = input(\"Update configuration? (y/N): \").strip().lower()\n            if response != \"y\":\n                print(\"[INFO] Configuration unchanged\")\n                return False\n\n        # Create server config\n        server_config = self.create_mcp_server_config(command, args, env)\n\n        # Show configuration details if verbose mode\n        if verbose:\n            print(\"[VERBOSE] Server configuration to be saved:\")\n            print(json.dumps(server_config, indent=2))\n            print()\n\n        # Add/update server\n        config[\"mcpServers\"][name] = server_config\n\n        # Save config\n        if self.save_config(config, config_path):\n            print(\"\\n[SUCCESS] MCP server configuration added!\")\n            print()\n            print(\"Next steps:\")\n            print(\"  1. Restart Claude Code\")\n            print(\"  2. Verify with: /mcp\")\n            print('  3. Test with: /search_code \"test query\"')\n            print()\n            return True\n        else:\n            return False\n\n    def validate_config(self, config_path: Optional[Path] = None) -> bool:\n        \"\"\"Validate the configuration file.\"\"\"\n        if config_path is None:\n            config_path = self.config_path\n\n        if not config_path or not config_path.exists():\n            print(f\"[ERROR] Config file not found: {config_path}\")\n            return False\n\n        config = self.load_config(config_path)\n\n        if \"mcpServers\" not in config:\n            print(\"[WARNING] No mcpServers section found\")\n            return False\n\n        if \"code-search\" not in config[\"mcpServers\"]:\n            print(\"[WARNING] code-search server not configured\")\n            return False\n\n        server = config[\"mcpServers\"][\"code-search\"]\n\n        print(\"\\n=== Configuration Validation ===\")\n        validation_passed = True\n\n        # Check required fields\n        if not server.get(\"command\"):\n            print(\"[ERROR] Missing 'command' field\")\n            validation_passed = False\n        else:\n            print(f\"[OK] Command: {server['command']}\")\n\n        # Check for args field (should always be present, even if empty)\n        if \"args\" not in server:\n            print(\n                \"[WARNING] Missing 'args' field (should be present, even if empty array)\"\n            )\n        else:\n            if server[\"args\"]:\n                print(f\"[OK] Args: {' '.join(server['args'])}\")\n            else:\n                print(\"[OK] Args: [] (empty)\")\n\n        if not server.get(\"env\"):\n            print(\"[WARNING] Missing 'env' field - environment variables not set\")\n        else:\n            env = server[\"env\"]\n            if \"PYTHONPATH\" in env:\n                print(f\"[OK] PYTHONPATH: {env['PYTHONPATH']}\")\n            else:\n                print(\"[WARNING] PYTHONPATH not set\")\n\n            if \"PYTHONUNBUFFERED\" in env:\n                print(f\"[OK] PYTHONUNBUFFERED: {env['PYTHONUNBUFFERED']}\")\n            else:\n                print(\"[WARNING] PYTHONUNBUFFERED not set\")\n\n        if validation_passed:\n            print(\"\\n[OK] Configuration is valid!\")\n        else:\n            print(\"\\n[ERROR] Configuration has errors\")\n\n        return validation_passed",
              "content_preview": "class ClaudeConfigManager:\n    \"\"\"Manages Claude Code MCP server configuration.\"\"\"\n\n    def __init__(self, project_dir: Optional[Path] = None):\n        \"\"\"Initialize configuration manager.\"\"\"\n        ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014864572047670641,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\unit\\test_model_selection.py:85-128:class:TestSearchConfigManager",
            "score": 0.564,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_model_selection.py",
              "relative_path": "tests\\unit\\test_model_selection.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "class",
              "start_line": 85,
              "end_line": 128,
              "name": "TestSearchConfigManager",
              "parent_name": null,
              "docstring": "Test SearchConfigManager with model persistence.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestSearchConfigManager:\n    \"\"\"Test SearchConfigManager with model persistence.\"\"\"\n\n    def test_config_manager_default_model(self):\n        \"\"\"Test that config manager loads default model.\"\"\"\n        import tempfile\n        from pathlib import Path\n\n        # Use temp config file to avoid loading user's real config\n        with tempfile.TemporaryDirectory() as tmpdir:\n            config_file = Path(tmpdir) / \"test_config.json\"\n            mgr = SearchConfigManager(config_file=str(config_file))\n            config = mgr.load_config()\n            # Should default to Gemma when no config exists\n            assert \"gemma\" in config.embedding_model_name.lower()\n\n    def test_config_manager_save_and_load(self):\n        \"\"\"Test saving and loading model configuration.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            config_file = Path(tmpdir) / \"test_config.json\"\n            mgr = SearchConfigManager(config_file=str(config_file))\n\n            # Load default\n            config = mgr.load_config()\n\n            # Change to BGE-M3\n            config.embedding_model_name = \"BAAI/bge-m3\"\n            mgr.save_config(config)\n\n            # Verify file was created\n            assert config_file.exists()\n\n            # Load in new manager instance\n            mgr2 = SearchConfigManager(config_file=str(config_file))\n            config2 = mgr2.load_config()\n            assert config2.embedding_model_name == \"BAAI/bge-m3\"\n\n    def test_environment_variable_override(self, monkeypatch):\n        \"\"\"Test that CLAUDE_EMBEDDING_MODEL env var works.\"\"\"\n        monkeypatch.setenv(\"CLAUDE_EMBEDDING_MODEL\", \"BAAI/bge-m3\")\n\n        mgr = SearchConfigManager()\n        config = mgr.load_config()\n        assert config.embedding_model_name == \"BAAI/bge-m3\"",
              "content_preview": "class TestSearchConfigManager:\n    \"\"\"Test SearchConfigManager with model persistence.\"\"\"\n\n    def test_config_manager_default_model(self):\n        \"\"\"Test that config manager loads default model.\"\"\"\n...",
              "project_name": "claude-context-local",
              "rrf_score": 0.009677419354838708,
              "appears_in_lists": 1,
              "final_rank": 3
            }
          },
          {
            "doc_id": "tests\\unit\\test_model_selection.py:101-120:method:test_config_manager_save_and_load",
            "score": 0.564,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_model_selection.py",
              "relative_path": "tests\\unit\\test_model_selection.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 101,
              "end_line": 120,
              "name": "test_config_manager_save_and_load",
              "parent_name": "TestSearchConfigManager",
              "docstring": "Test saving and loading model configuration.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_config_manager_save_and_load(self):\n        \"\"\"Test saving and loading model configuration.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            config_file = Path(tmpdir) / \"test_config.json\"\n            mgr = SearchConfigManager(config_file=str(config_file))\n\n            # Load default\n            config = mgr.load_config()\n\n            # Change to BGE-M3\n            config.embedding_model_name = \"BAAI/bge-m3\"\n            mgr.save_config(config)\n\n            # Verify file was created\n            assert config_file.exists()\n\n            # Load in new manager instance\n            mgr2 = SearchConfigManager(config_file=str(config_file))\n            config2 = mgr2.load_config()\n            assert config2.embedding_model_name == \"BAAI/bge-m3\"",
              "content_preview": "def test_config_manager_save_and_load(self):\n        \"\"\"Test saving and loading model configuration.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            config_file = Path(tmpdir) / \"...",
              "project_name": "claude-context-local",
              "rrf_score": 0.009523809523809523,
              "appears_in_lists": 1,
              "final_rank": 4
            }
          },
          {
            "doc_id": "tests\\unit\\test_model_selection.py:88-99:method:test_config_manager_default_model",
            "score": 0.558,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_model_selection.py",
              "relative_path": "tests\\unit\\test_model_selection.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 88,
              "end_line": 99,
              "name": "test_config_manager_default_model",
              "parent_name": "TestSearchConfigManager",
              "docstring": "Test that config manager loads default model.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_config_manager_default_model(self):\n        \"\"\"Test that config manager loads default model.\"\"\"\n        import tempfile\n        from pathlib import Path\n\n        # Use temp config file to avoid loading user's real config\n        with tempfile.TemporaryDirectory() as tmpdir:\n            config_file = Path(tmpdir) / \"test_config.json\"\n            mgr = SearchConfigManager(config_file=str(config_file))\n            config = mgr.load_config()\n            # Should default to Gemma when no config exists\n            assert \"gemma\" in config.embedding_model_name.lower()",
              "content_preview": "def test_config_manager_default_model(self):\n        \"\"\"Test that config manager loads default model.\"\"\"\n        import tempfile\n        from pathlib import Path\n\n        # Use temp config file to avo...",
              "project_name": "claude-context-local",
              "rrf_score": 0.009375,
              "appears_in_lists": 1,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "search\\config.py:261-266:function:get_config_manager",
          "search\\config.py:101-254:class:SearchConfigManager",
          "scripts\\manual_configure.py:22-239:class:ClaudeConfigManager",
          "tests\\unit\\test_model_selection.py:85-128:class:TestSearchConfigManager",
          "tests\\unit\\test_model_selection.py:88-99:method:test_config_manager_default_model",
          "search\\config.py:158-193:method:_load_from_environment",
          "tests\\test_data\\python_project\\src\\utils\\helpers.py:47-86:class:ConfigManager",
          "scripts\\manual_configure.py:25-28:method:__init__",
          "tests\\unit\\test_model_selection.py:101-120:method:test_config_manager_save_and_load",
          "tests\\integration\\test_hybrid_search_integration.py:562-565:method:setup_method"
        ]
      },
      "multi_hop": {
        "time_ms": 46.7,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "scripts\\__init__.py:1-2:module",
            "score": 8.427,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\__init__.py",
              "relative_path": "scripts\\__init__.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "module",
              "start_line": 1,
              "end_line": 2,
              "name": null,
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "\"\"\"Scripts for code indexing and management.\"\"\"\n",
              "content_preview": "\"\"\"Scripts for code indexing and management.\"\"\"\n",
              "project_name": "claude-context-local",
              "rrf_score": 0.013786292711830931,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "tests\\test_data\\python_project\\src\\utils\\helpers.py:47-86:class:ConfigManager",
            "score": 5.378,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\test_data\\python_project\\src\\utils\\helpers.py",
              "relative_path": "tests\\test_data\\python_project\\src\\utils\\helpers.py",
              "folder_structure": [
                "tests",
                "test_data",
                "python_project",
                "src",
                "utils"
              ],
              "chunk_type": "class",
              "start_line": 47,
              "end_line": 86,
              "name": "ConfigManager",
              "parent_name": null,
              "docstring": "Manages application configuration.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class ConfigManager:\n    \"\"\"Manages application configuration.\"\"\"\n\n    def __init__(self, config_file: Optional[str] = None):\n        self.config_file = config_file\n        self.config = {}\n        self.logger = logging.getLogger(__name__)\n\n        if config_file and os.path.exists(config_file):\n            self.load_config()\n\n    def load_config(self) -> None:\n        \"\"\"Load configuration from file.\"\"\"\n        try:\n            with open(self.config_file, \"r\") as f:\n                self.config = json.load(f)\n            self.logger.info(f\"Configuration loaded from {self.config_file}\")\n        except (IOError, json.JSONDecodeError) as e:\n            self.logger.error(f\"Failed to load configuration: {e}\")\n            self.config = {}\n\n    def get(self, key: str, default: Any = None) -> Any:\n        \"\"\"Get configuration value.\"\"\"\n        return self.config.get(key, default)\n\n    def set(self, key: str, value: Any) -> None:\n        \"\"\"Set configuration value.\"\"\"\n        self.config[key] = value\n\n    def save_config(self) -> None:\n        \"\"\"Save configuration to file.\"\"\"\n        if not self.config_file:\n            raise ValueError(\"No config file specified\")\n\n        try:\n            with open(self.config_file, \"w\") as f:\n                json.dump(self.config, f, indent=2)\n            self.logger.info(f\"Configuration saved to {self.config_file}\")\n        except IOError as e:\n            self.logger.error(f\"Failed to save configuration: {e}\")",
              "content_preview": "class ConfigManager:\n    \"\"\"Manages application configuration.\"\"\"\n\n    def __init__(self, config_file: Optional[str] = None):\n        self.config_file = config_file\n        self.config = {}\n        se...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015806214827501837,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "scripts\\manual_configure.py:22-239:class:ClaudeConfigManager",
            "score": 5.004,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\manual_configure.py",
              "relative_path": "scripts\\manual_configure.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "class",
              "start_line": 22,
              "end_line": 239,
              "name": "ClaudeConfigManager",
              "parent_name": null,
              "docstring": "Manages Claude Code MCP server configuration.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class ClaudeConfigManager:\n    \"\"\"Manages Claude Code MCP server configuration.\"\"\"\n\n    def __init__(self, project_dir: Optional[Path] = None):\n        \"\"\"Initialize configuration manager.\"\"\"\n        self.project_dir = project_dir or Path.cwd()\n        self.config_path: Optional[Path] = None\n\n    def get_config_path(self, global_scope: bool = True) -> Path:\n        \"\"\"Get the path to the Claude configuration file.\"\"\"\n        if global_scope:\n            # Global config in user's home directory\n            home = Path.home()\n            return home / \".claude.json\"\n        else:\n            # Project-specific config\n            return self.project_dir / \".claude.json\"\n\n    def load_config(self, config_path: Path) -> Dict[str, Any]:\n        \"\"\"Load existing Claude configuration.\"\"\"\n        if not config_path.exists():\n            print(f\"[INFO] Config file not found: {config_path}\")\n            print(\"[INFO] Creating new configuration file...\")\n            return {}\n\n        try:\n            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                config = json.load(f)\n            print(f\"[OK] Loaded existing configuration from {config_path}\")\n            return config\n        except json.JSONDecodeError as e:\n            print(f\"[ERROR] Failed to parse config file: {e}\")\n            print(\"[WARNING] Creating backup and starting fresh...\")\n            backup_path = config_path.with_suffix(\".json.backup\")\n            if config_path.exists():\n                config_path.rename(backup_path)\n                print(f\"[OK] Backup created: {backup_path}\")\n            return {}\n        except Exception as e:\n            print(f\"[ERROR] Failed to load config: {e}\")\n            return {}\n\n    def save_config(self, config: Dict[str, Any], config_path: Path) -> bool:\n        \"\"\"Save Claude configuration to file.\"\"\"\n        try:\n            # Ensure parent directory exists\n            config_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Write with proper formatting\n            with open(config_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(config, f, indent=2, ensure_ascii=False)\n\n            print(f\"[OK] Configuration saved to {config_path}\")\n            return True\n        except Exception as e:\n            print(f\"[ERROR] Failed to save config: {e}\")\n            return False\n\n    def create_mcp_server_config(\n        self, command: str, args: list = None, env: Dict[str, str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Create MCP server configuration structure.\"\"\"\n        config = {\"type\": \"stdio\", \"command\": command}\n\n        # Always include args field, even if empty (required by Claude Code)\n        if args is not None:\n            config[\"args\"] = args\n        else:\n            config[\"args\"] = []\n\n        # Always include env field if provided (recommended by Claude Code)\n        if env:\n            config[\"env\"] = env\n\n        return config\n\n    def add_mcp_server(\n        self,\n        name: str,\n        command: str,\n        args: list = None,\n        env: Dict[str, str] = None,\n        global_scope: bool = True,\n        force: bool = False,\n        verbose: bool = False,\n    ) -> bool:\n        \"\"\"Add or update MCP server configuration.\"\"\"\n        config_path = self.get_config_path(global_scope)\n        self.config_path = config_path\n\n        print(\"\\n=== Claude Code MCP Manual Configuration ===\")\n        print(f\"Server Name: {name}\")\n        print(f\"Command: {command}\")\n        if args:\n            print(f\"Arguments: {' '.join(args)}\")\n        if env:\n            print(\"Environment Variables:\")\n            for key, value in env.items():\n                # Show truncated path for readability\n                display_value = value if len(value) < 60 else f\"{value[:57]}...\"\n                print(f\"  {key}: {display_value}\")\n        print(f\"Config File: {config_path}\")\n        print(\n            f\"Scope: {'Global (all projects)' if global_scope else 'Project-specific'}\"\n        )\n        print()\n\n        # Load existing config\n        config = self.load_config(config_path)\n\n        # Ensure mcpServers exists\n        if \"mcpServers\" not in config:\n            config[\"mcpServers\"] = {}\n\n        # Check if server already exists\n        if name in config[\"mcpServers\"] and not force:\n            print(f\"[WARNING] MCP server '{name}' already exists\")\n            print(\"Current configuration:\")\n            current = config[\"mcpServers\"][name]\n            print(f\"  Command: {current.get('command', 'N/A')}\")\n            if current.get(\"args\"):\n                print(f\"  Args: {' '.join(current['args'])}\")\n            if current.get(\"env\"):\n                print(f\"  Environment variables: {len(current['env'])} set\")\n            print()\n\n            response = input(\"Update configuration? (y/N): \").strip().lower()\n            if response != \"y\":\n                print(\"[INFO] Configuration unchanged\")\n                return False\n\n        # Create server config\n        server_config = self.create_mcp_server_config(command, args, env)\n\n        # Show configuration details if verbose mode\n        if verbose:\n            print(\"[VERBOSE] Server configuration to be saved:\")\n            print(json.dumps(server_config, indent=2))\n            print()\n\n        # Add/update server\n        config[\"mcpServers\"][name] = server_config\n\n        # Save config\n        if self.save_config(config, config_path):\n            print(\"\\n[SUCCESS] MCP server configuration added!\")\n            print()\n            print(\"Next steps:\")\n            print(\"  1. Restart Claude Code\")\n            print(\"  2. Verify with: /mcp\")\n            print('  3. Test with: /search_code \"test query\"')\n            print()\n            return True\n        else:\n            return False\n\n    def validate_config(self, config_path: Optional[Path] = None) -> bool:\n        \"\"\"Validate the configuration file.\"\"\"\n        if config_path is None:\n            config_path = self.config_path\n\n        if not config_path or not config_path.exists():\n            print(f\"[ERROR] Config file not found: {config_path}\")\n            return False\n\n        config = self.load_config(config_path)\n\n        if \"mcpServers\" not in config:\n            print(\"[WARNING] No mcpServers section found\")\n            return False\n\n        if \"code-search\" not in config[\"mcpServers\"]:\n            print(\"[WARNING] code-search server not configured\")\n            return False\n\n        server = config[\"mcpServers\"][\"code-search\"]\n\n        print(\"\\n=== Configuration Validation ===\")\n        validation_passed = True\n\n        # Check required fields\n        if not server.get(\"command\"):\n            print(\"[ERROR] Missing 'command' field\")\n            validation_passed = False\n        else:\n            print(f\"[OK] Command: {server['command']}\")\n\n        # Check for args field (should always be present, even if empty)\n        if \"args\" not in server:\n            print(\n                \"[WARNING] Missing 'args' field (should be present, even if empty array)\"\n            )\n        else:\n            if server[\"args\"]:\n                print(f\"[OK] Args: {' '.join(server['args'])}\")\n            else:\n                print(\"[OK] Args: [] (empty)\")\n\n        if not server.get(\"env\"):\n            print(\"[WARNING] Missing 'env' field - environment variables not set\")\n        else:\n            env = server[\"env\"]\n            if \"PYTHONPATH\" in env:\n                print(f\"[OK] PYTHONPATH: {env['PYTHONPATH']}\")\n            else:\n                print(\"[WARNING] PYTHONPATH not set\")\n\n            if \"PYTHONUNBUFFERED\" in env:\n                print(f\"[OK] PYTHONUNBUFFERED: {env['PYTHONUNBUFFERED']}\")\n            else:\n                print(\"[WARNING] PYTHONUNBUFFERED not set\")\n\n        if validation_passed:\n            print(\"\\n[OK] Configuration is valid!\")\n        else:\n            print(\"\\n[ERROR] Configuration has errors\")\n\n        return validation_passed",
              "content_preview": "class ClaudeConfigManager:\n    \"\"\"Manages Claude Code MCP server configuration.\"\"\"\n\n    def __init__(self, project_dir: Optional[Path] = None):\n        \"\"\"Initialize configuration manager.\"\"\"\n        ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014864572047670641,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "scripts\\manual_configure.py:98-176:method:add_mcp_server",
            "score": 4.963,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\manual_configure.py",
              "relative_path": "scripts\\manual_configure.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "method",
              "start_line": 98,
              "end_line": 176,
              "name": "add_mcp_server",
              "parent_name": "ClaudeConfigManager",
              "docstring": "Add or update MCP server configuration.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def add_mcp_server(\n        self,\n        name: str,\n        command: str,\n        args: list = None,\n        env: Dict[str, str] = None,\n        global_scope: bool = True,\n        force: bool = False,\n        verbose: bool = False,\n    ) -> bool:\n        \"\"\"Add or update MCP server configuration.\"\"\"\n        config_path = self.get_config_path(global_scope)\n        self.config_path = config_path\n\n        print(\"\\n=== Claude Code MCP Manual Configuration ===\")\n        print(f\"Server Name: {name}\")\n        print(f\"Command: {command}\")\n        if args:\n            print(f\"Arguments: {' '.join(args)}\")\n        if env:\n            print(\"Environment Variables:\")\n            for key, value in env.items():\n                # Show truncated path for readability\n                display_value = value if len(value) < 60 else f\"{value[:57]}...\"\n                print(f\"  {key}: {display_value}\")\n        print(f\"Config File: {config_path}\")\n        print(\n            f\"Scope: {'Global (all projects)' if global_scope else 'Project-specific'}\"\n        )\n        print()\n\n        # Load existing config\n        config = self.load_config(config_path)\n\n        # Ensure mcpServers exists\n        if \"mcpServers\" not in config:\n            config[\"mcpServers\"] = {}\n\n        # Check if server already exists\n        if name in config[\"mcpServers\"] and not force:\n            print(f\"[WARNING] MCP server '{name}' already exists\")\n            print(\"Current configuration:\")\n            current = config[\"mcpServers\"][name]\n            print(f\"  Command: {current.get('command', 'N/A')}\")\n            if current.get(\"args\"):\n                print(f\"  Args: {' '.join(current['args'])}\")\n            if current.get(\"env\"):\n                print(f\"  Environment variables: {len(current['env'])} set\")\n            print()\n\n            response = input(\"Update configuration? (y/N): \").strip().lower()\n            if response != \"y\":\n                print(\"[INFO] Configuration unchanged\")\n                return False\n\n        # Create server config\n        server_config = self.create_mcp_server_config(command, args, env)\n\n        # Show configuration details if verbose mode\n        if verbose:\n            print(\"[VERBOSE] Server configuration to be saved:\")\n            print(json.dumps(server_config, indent=2))\n            print()\n\n        # Add/update server\n        config[\"mcpServers\"][name] = server_config\n\n        # Save config\n        if self.save_config(config, config_path):\n            print(\"\\n[SUCCESS] MCP server configuration added!\")\n            print()\n            print(\"Next steps:\")\n            print(\"  1. Restart Claude Code\")\n            print(\"  2. Verify with: /mcp\")\n            print('  3. Test with: /search_code \"test query\"')\n            print()\n            return True\n        else:\n            return False",
              "content_preview": "def add_mcp_server(\n        self,\n        name: str,\n        command: str,\n        args: list = None,\n        env: Dict[str, str] = None,\n        global_scope: bool = True,\n        force: bool = False...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012452107279693488,
              "appears_in_lists": 2,
              "final_rank": 7
            }
          },
          {
            "doc_id": "scripts\\manual_configure.py:40-62:method:load_config",
            "score": 4.692,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\manual_configure.py",
              "relative_path": "scripts\\manual_configure.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "method",
              "start_line": 40,
              "end_line": 62,
              "name": "load_config",
              "parent_name": "ClaudeConfigManager",
              "docstring": "Load existing Claude configuration.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def load_config(self, config_path: Path) -> Dict[str, Any]:\n        \"\"\"Load existing Claude configuration.\"\"\"\n        if not config_path.exists():\n            print(f\"[INFO] Config file not found: {config_path}\")\n            print(\"[INFO] Creating new configuration file...\")\n            return {}\n\n        try:\n            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                config = json.load(f)\n            print(f\"[OK] Loaded existing configuration from {config_path}\")\n            return config\n        except json.JSONDecodeError as e:\n            print(f\"[ERROR] Failed to parse config file: {e}\")\n            print(\"[WARNING] Creating backup and starting fresh...\")\n            backup_path = config_path.with_suffix(\".json.backup\")\n            if config_path.exists():\n                config_path.rename(backup_path)\n                print(f\"[OK] Backup created: {backup_path}\")\n            return {}\n        except Exception as e:\n            print(f\"[ERROR] Failed to load config: {e}\")\n            return {}",
              "content_preview": "def load_config(self, config_path: Path) -> Dict[str, Any]:\n        \"\"\"Load existing Claude configuration.\"\"\"\n        if not config_path.exists():\n            print(f\"[INFO] Config file not found: {co...",
              "project_name": "claude-context-local",
              "rrf_score": 0.011585030269675289,
              "appears_in_lists": 2,
              "final_rank": 9
            }
          }
        ],
        "all_doc_ids": [
          "tests\\unit\\test_model_selection.py:17-45:class:TestModelRegistry",
          "scripts\\manual_configure.py:22-239:class:ClaudeConfigManager",
          "tests\\test_data\\python_project\\src\\utils\\helpers.py:72-74:method:set",
          "scripts\\manual_configure.py:40-62:method:load_config",
          "tests\\integration\\test_hybrid_search_integration.py:619-621:method:teardown_method",
          "scripts\\manual_configure.py:98-176:method:add_mcp_server",
          "search\\config.py:269-271:function:get_search_config",
          "scripts\\manual_configure.py:242-332:function:main",
          "tests\\test_data\\python_project\\src\\utils\\helpers.py:47-86:class:ConfigManager",
          "scripts\\__init__.py:1-2:module"
        ],
        "unique_discoveries": [
          "tests\\unit\\test_model_selection.py:17-45:class:TestModelRegistry",
          "tests\\test_data\\python_project\\src\\utils\\helpers.py:72-74:method:set",
          "scripts\\manual_configure.py:40-62:method:load_config",
          "tests\\integration\\test_hybrid_search_integration.py:619-621:method:teardown_method",
          "scripts\\manual_configure.py:98-176:method:add_mcp_server",
          "search\\config.py:269-271:function:get_search_config",
          "scripts\\manual_configure.py:242-332:function:main",
          "scripts\\__init__.py:1-2:module"
        ]
      },
      "comparison": {
        "time_overhead_ms": 24.79,
        "time_overhead_pct": 113.1,
        "top5_overlap_count": 2,
        "top5_overlap_pct": 40.0,
        "unique_discovery_count": 8,
        "value_rating": "HIGH"
      }
    },
    {
      "query": "embedding model loading and initialization",
      "single_hop": {
        "time_ms": 20.2,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "mcp_server\\server.py:147-168:function:get_embedder",
            "score": 11.454,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "function",
              "start_line": 147,
              "end_line": 168,
              "name": "get_embedder",
              "parent_name": null,
              "docstring": "Lazy initialization of embedder with configurable model.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def get_embedder() -> CodeEmbedder:\n    \"\"\"Lazy initialization of embedder with configurable model.\"\"\"\n    global _embedder\n    if _embedder is None:\n        cache_dir = get_storage_dir() / \"models\"\n        cache_dir.mkdir(exist_ok=True)\n\n        # Get model from config (defaults to Gemma for backward compatibility)\n        try:\n            from search.config import get_search_config\n\n            config = get_search_config()\n            model_name = config.embedding_model_name\n            logger.info(f\"Using embedding model: {model_name}\")\n        except Exception as e:\n            logger.warning(f\"Failed to load model from config: {e}\")\n            model_name = \"google/embeddinggemma-300m\"  # Fallback to default\n            logger.info(f\"Falling back to default model: {model_name}\")\n\n        _embedder = CodeEmbedder(model_name=model_name, cache_dir=str(cache_dir))\n        logger.info(\"Embedder initialized successfully\")\n    return _embedder",
              "content_preview": "def get_embedder() -> CodeEmbedder:\n    \"\"\"Lazy initialization of embedder with configurable model.\"\"\"\n    global _embedder\n    if _embedder is None:\n        cache_dir = get_storage_dir() / \"models\"\n ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016185271922976842,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "scripts\\verify_installation.py:501-548:method:test_embedding_model",
            "score": 14.85,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\verify_installation.py",
              "relative_path": "scripts\\verify_installation.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "method",
              "start_line": 501,
              "end_line": 548,
              "name": "test_embedding_model",
              "parent_name": "InstallationVerifier",
              "docstring": "Test EmbeddingGemma model loading (optional).",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_embedding_model(self) -> bool:\n        \"\"\"Test EmbeddingGemma model loading (optional).\"\"\"\n        print()\n        print(\"=== EmbeddingGemma Model Test ===\")\n        print()\n        print(\"[INFO] This test will download ~1.2GB model if not cached...\")\n\n        embedding_code = \"\"\"\ntry:\n    from sentence_transformers import SentenceTransformer\n    import torch\n\n    print('Loading EmbeddingGemma model...')\n    model = SentenceTransformer('google/embeddinggemma-300m')\n    print(f'Model loaded on device: {model.device}')\n\n    test_text = 'def hello_world(): return \"Hello World\"'\n    embedding = model.encode([test_text])\n    print(f'Generated embedding shape: {embedding.shape}')\n    print('EmbeddingGemma test successful')\n\nexcept Exception as e:\n    print(f'EmbeddingGemma test failed: {e}')\n    print('Note: This is acceptable if model is not downloaded yet')\n    raise\n\"\"\"\n\n        success, output = self._run_python_test(\n            embedding_code, \"EmbeddingGemma Model Loading\"\n        )\n\n        if success:\n            self._print_test_result(\n                \"EmbeddingGemma Model Loading\",\n                True,\n                details=\"EmbeddingGemma model working\",\n            )\n            for line in output.strip().split(\"\\n\"):\n                if line.strip():\n                    print(f\"       {line.strip()}\")\n        else:\n            self._print_warning(\n                \"EmbeddingGemma Model Loading\",\n                \"EmbeddingGemma model test failed\",\n                \"Model will be downloaded on first use\",\n            )\n\n        return True  # Don't fail overall test for model download issues",
              "content_preview": "def test_embedding_model(self) -> bool:\n        \"\"\"Test EmbeddingGemma model loading (optional).\"\"\"\n        print()\n        print(\"=== EmbeddingGemma Model Test ===\")\n        print()\n        print(\"[I...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01564828614008942,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "scripts\\verify_hf_auth.py:46-72:function:test_model_loading",
            "score": 14.187,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\verify_hf_auth.py",
              "relative_path": "scripts\\verify_hf_auth.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "function",
              "start_line": 46,
              "end_line": 72,
              "name": "test_model_loading",
              "parent_name": null,
              "docstring": "Test loading the EmbeddingGemma model.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_model_loading():\n    \"\"\"Test loading the EmbeddingGemma model.\"\"\"\n    print(\"\\n=== Model Loading Test ===\\n\")\n\n    try:\n        from sentence_transformers import SentenceTransformer\n\n        print(\"\ud83d\udd04 [INFO] Initializing SentenceTransformer...\")\n        print(\"\u23f3 [INFO] This may download the model (~1.3GB) if not cached...\")\n\n        model = SentenceTransformer(\"google/embeddinggemma-300m\")\n        print(\"\u2705 [OK] Model loaded successfully!\")\n\n        # Test encoding\n        print(\"\ud83e\uddea [INFO] Testing model encoding...\")\n        test_text = \"def test_function(): return True\"\n        embedding = model.encode(test_text)\n        print(f\"\u2705 [OK] Encoding successful! Embedding dimension: {len(embedding)}\")\n\n        return True\n    except Exception as e:\n        print(f\"\u274c [ERROR] Model loading failed: {e}\")\n        print(\"\ud83d\udca1 [HELP] This could indicate:\")\n        print(\"   1. Model license not accepted\")\n        print(\"   2. Authentication issues\")\n        print(\"   3. Network connectivity problems\")\n        return False",
              "content_preview": "def test_model_loading():\n    \"\"\"Test loading the EmbeddingGemma model.\"\"\"\n    print(\"\\n=== Model Loading Test ===\\n\")\n\n    try:\n        from sentence_transformers import SentenceTransformer\n\n        ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015147265077138851,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "embeddings\\embedder.py:38-522:class:CodeEmbedder",
            "score": 9.339,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\embeddings\\embedder.py",
              "relative_path": "embeddings\\embedder.py",
              "folder_structure": [
                "embeddings"
              ],
              "chunk_type": "class",
              "start_line": 38,
              "end_line": 522,
              "name": "CodeEmbedder",
              "parent_name": null,
              "docstring": "Multi-model embedder for generating code embeddings.\n\n    Supports multiple embedding models with automatic configuration detection.\n    Default model is google/embeddinggemma-300m for backward compatibility.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class CodeEmbedder:\n    \"\"\"Multi-model embedder for generating code embeddings.\n\n    Supports multiple embedding models with automatic configuration detection.\n    Default model is google/embeddinggemma-300m for backward compatibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"google/embeddinggemma-300m\",\n        cache_dir: Optional[str] = None,\n        device: str = \"auto\",\n    ):\n        self.model_name = model_name\n        self.cache_dir = cache_dir or os.path.expanduser(\"~/.cache/huggingface/hub\")\n        self.device = device\n        self._model = None\n        self._logger = logging.getLogger(__name__)\n        self._model_config = None\n\n        # Setup logging\n        logging.basicConfig(level=logging.INFO)\n\n    @classmethod\n    def get_supported_models(cls) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get dictionary of supported models and their configurations.\"\"\"\n        from search.config import get_model_registry\n\n        return get_model_registry()\n\n    def _get_model_config(self) -> Dict[str, Any]:\n        \"\"\"Get configuration for the current model.\n\n        Returns model-specific config including dimension, prompt_name, etc.\n        Falls back to sensible defaults for unknown models.\n        \"\"\"\n        if self._model_config is not None:\n            return self._model_config\n\n        from search.config import get_model_config\n\n        # Try to get from registry\n        config = get_model_config(self.model_name)\n        if config:\n            self._model_config = config\n            return config\n\n        # Auto-detect based on model name for unknown models\n        model_lower = self.model_name.lower()\n\n        if \"gemma\" in model_lower:\n            self._model_config = {\n                \"dimension\": 768,\n                \"prompt_name\": \"Retrieval-document\",\n                \"description\": \"EmbeddingGemma model\",\n            }\n        elif \"bge-m3\" in model_lower or \"bge_m3\" in model_lower:\n            self._model_config = {\n                \"dimension\": 1024,\n                \"prompt_name\": None,  # BGE-M3 doesn't use prompts\n                \"description\": \"BGE-M3 model\",\n            }\n        else:\n            # Default fallback\n            self._logger.warning(\n                f\"Unknown model {self.model_name}, using default config\"\n            )\n            self._model_config = {\n                \"dimension\": 768,\n                \"prompt_name\": None,\n                \"description\": \"Unknown model\",\n            }\n\n        return self._model_config\n\n    @property\n    def model(self):\n        \"\"\"Lazy loading of the model.\"\"\"\n        if self._model is None:\n            self._load_model()\n        return self._model\n\n    def _load_model(self):\n        \"\"\"Load the EmbeddingGemma model.\"\"\"\n        if SentenceTransformer is None:\n            raise ImportError(\n                \"sentence-transformers not found. Install with: \"\n                \"pip install sentence-transformers>=5.0.0\"\n            )\n\n        self._logger.info(f\"Loading model: {self.model_name}\")\n\n        # If the model appears to be cached locally, enable offline mode to avoid network HEAD/GET checks\n        local_model_dir = None\n        try:\n            if self._is_model_cached():\n                os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n                os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n                self._logger.info(\n                    \"Model cache detected. Enabling offline mode for faster startup.\"\n                )\n                # Prefer loading directly from local cache path to avoid any remote HEAD/GET\n                local_model_dir = self._find_local_model_dir()\n                if local_model_dir:\n                    self._logger.info(\n                        f\"Loading model from local cache path: {local_model_dir}\"\n                    )\n        except Exception as _e:\n            # Best-effort; continue without failing if detection has issues\n            self._logger.debug(f\"Offline mode detection skipped: {_e}\")\n\n        try:\n            model_source = str(local_model_dir) if local_model_dir else self.model_name\n            resolved_device = self._resolve_device(self.device)\n            self._model = SentenceTransformer(\n                model_source, cache_folder=self.cache_dir, device=resolved_device\n            )\n            # Persist resolved device for later info\n            self.device = resolved_device\n            self._logger.info(\n                f\"Model loaded successfully on device: {self._model.device}\"\n            )\n\n        except Exception as e:\n            self._logger.error(f\"Failed to load model: {e}\")\n            raise\n\n    def create_embedding_content(self, chunk: CodeChunk, max_chars: int = 6000) -> str:\n        \"\"\"Create clean content for embedding generation with size limits.\"\"\"\n        # Prepare clean content without fabricated headers\n        content_parts = []\n\n        # Add docstring if available (important context for code understanding)\n        docstring_budget = 300\n        if chunk.docstring:\n            # Keep docstring but limit length to stay within token budget\n            docstring = (\n                chunk.docstring[:docstring_budget] + \"...\"\n                if len(chunk.docstring) > docstring_budget\n                else chunk.docstring\n            )\n            content_parts.append(f'\"\"\"{docstring}\"\"\"')\n\n        # Calculate remaining budget for code content\n        docstring_len = len(content_parts[0]) if content_parts else 0\n        remaining_budget = max_chars - docstring_len - 10  # small buffer\n\n        # Add the actual code content, truncating if necessary\n        if len(chunk.content) <= remaining_budget:\n            content_parts.append(chunk.content)\n        else:\n            # Smart truncation: try to keep function signature and important parts\n            lines = chunk.content.split(\"\\n\")\n            if len(lines) > 3:\n                # Keep first few lines (signature) and last few lines (return/conclusion)\n                head_lines = []\n                tail_lines = []\n                current_length = docstring_len\n\n                # Add head lines (function signature, early logic)\n                for _i, line in enumerate(lines[: min(len(lines) // 2, 20)]):\n                    if current_length + len(line) + 1 > remaining_budget * 0.7:\n                        break\n                    head_lines.append(line)\n                    current_length += len(line) + 1\n\n                # Add tail lines (return statements, conclusions) if space remains\n                remaining_space = (\n                    remaining_budget - current_length - 20\n                )  # buffer for \"...\"\n                for line in reversed(lines[-min(len(lines) // 3, 10) :]):\n                    if len(\"\\n\".join(tail_lines)) + len(line) + 1 > remaining_space:\n                        break\n                    tail_lines.insert(0, line)\n\n                if tail_lines:\n                    truncated_content = (\n                        \"\\n\".join(head_lines)\n                        + \"\\n    # ... (truncated) ...\\n\"\n                        + \"\\n\".join(tail_lines)\n                    )\n                else:\n                    truncated_content = (\n                        \"\\n\".join(head_lines) + \"\\n    # ... (truncated) ...\"\n                    )\n                content_parts.append(truncated_content)\n            else:\n                # For short chunks, just truncate at character limit\n                content_parts.append(\n                    chunk.content[:remaining_budget] + \"...\"\n                    if len(chunk.content) > remaining_budget\n                    else chunk.content\n                )\n\n        return \"\\n\".join(content_parts)\n\n    def embed_chunk(self, chunk: CodeChunk) -> EmbeddingResult:\n        \"\"\"Generate embedding for a single code chunk.\"\"\"\n        content = self.create_embedding_content(chunk)\n\n        # Get model-specific configuration\n        model_config = self._get_model_config()\n        prompt_name = model_config.get(\"prompt_name\")\n\n        # Use encode with model-specific prompt_name (if supported)\n        # EmbeddingGemma uses \"Retrieval-document\", BGE-M3 doesn't use prompts\n        if prompt_name:\n            embedding = self.model.encode(\n                [content], prompt_name=prompt_name, show_progress_bar=False\n            )[0]\n        else:\n            embedding = self.model.encode([content], show_progress_bar=False)[0]\n\n        # Create unique chunk ID\n        chunk_id = f\"{chunk.relative_path}:{chunk.start_line}-{chunk.end_line}:{chunk.chunk_type}\"\n        if chunk.name:\n            chunk_id += f\":{chunk.name}\"\n\n        # Prepare metadata\n        metadata = {\n            \"file_path\": chunk.file_path,\n            \"relative_path\": chunk.relative_path,\n            \"folder_structure\": chunk.folder_structure,\n            \"chunk_type\": chunk.chunk_type,\n            \"start_line\": chunk.start_line,\n            \"end_line\": chunk.end_line,\n            \"name\": chunk.name,\n            \"parent_name\": chunk.parent_name,\n            \"docstring\": chunk.docstring,\n            \"decorators\": chunk.decorators,\n            \"imports\": chunk.imports,\n            \"complexity_score\": chunk.complexity_score,\n            \"tags\": chunk.tags,\n            \"content\": chunk.content,  # Full content for accurate token counting\n            \"content_preview\": (\n                chunk.content[:200] + \"...\"\n                if len(chunk.content) > 200\n                else chunk.content\n            ),\n        }\n\n        return EmbeddingResult(\n            embedding=embedding, chunk_id=chunk_id, metadata=metadata\n        )\n\n    def embed_chunks(\n        self, chunks: List[CodeChunk], batch_size: Optional[int] = None\n    ) -> List[EmbeddingResult]:\n        \"\"\"Generate embeddings for multiple chunks with batching.\"\"\"\n        results = []\n\n        # Load batch size from config if not explicitly provided\n        if batch_size is None:\n            from search.config import get_search_config\n\n            config = get_search_config()\n            batch_size = config.embedding_batch_size\n            self._logger.info(\n                f\"Using batch size {batch_size} from config for {len(chunks)} chunks\"\n            )\n        else:\n            self._logger.info(\n                f\"Using explicit batch size {batch_size} for {len(chunks)} chunks\"\n            )\n\n        # Get model-specific configuration\n        model_config = self._get_model_config()\n        prompt_name = model_config.get(\"prompt_name\")\n\n        # Process in batches for efficiency\n        for i in range(0, len(chunks), batch_size):\n            batch = chunks[i : i + batch_size]\n            batch_contents = [self.create_embedding_content(chunk) for chunk in batch]\n\n            # Generate embeddings for batch using model-specific prompt_name\n            if prompt_name:\n                batch_embeddings = self.model.encode(\n                    batch_contents,\n                    prompt_name=prompt_name,\n                    show_progress_bar=False,\n                )\n            else:\n                batch_embeddings = self.model.encode(\n                    batch_contents,\n                    show_progress_bar=False,\n                )\n\n            # Create results\n            for _j, (chunk, embedding) in enumerate(\n                zip(batch, batch_embeddings, strict=False)\n            ):\n                chunk_id = f\"{chunk.relative_path}:{chunk.start_line}-{chunk.end_line}:{chunk.chunk_type}\"\n                if chunk.name:\n                    chunk_id += f\":{chunk.name}\"\n\n                metadata = {\n                    \"file_path\": chunk.file_path,\n                    \"relative_path\": chunk.relative_path,\n                    \"folder_structure\": chunk.folder_structure,\n                    \"chunk_type\": chunk.chunk_type,\n                    \"start_line\": chunk.start_line,\n                    \"end_line\": chunk.end_line,\n                    \"name\": chunk.name,\n                    \"parent_name\": chunk.parent_name,\n                    \"docstring\": chunk.docstring,\n                    \"decorators\": chunk.decorators,\n                    \"imports\": chunk.imports,\n                    \"complexity_score\": chunk.complexity_score,\n                    \"tags\": chunk.tags,\n                    \"content\": chunk.content,  # Full content for accurate token counting\n                    \"content_preview\": (\n                        chunk.content[:200] + \"...\"\n                        if len(chunk.content) > 200\n                        else chunk.content\n                    ),\n                }\n\n                results.append(\n                    EmbeddingResult(\n                        embedding=embedding, chunk_id=chunk_id, metadata=metadata\n                    )\n                )\n\n            if i + batch_size < len(chunks):\n                self._logger.info(f\"Processed {i + batch_size}/{len(chunks)} chunks\")\n\n        self._logger.info(\"Embedding generation completed\")\n        return results\n\n    def embed_query(self, query: str) -> np.ndarray:\n        \"\"\"Generate embedding for a search query.\"\"\"\n        # Get model-specific configuration\n        model_config = self._get_model_config()\n        prompt_name = model_config.get(\"prompt_name\")\n\n        # Use encode with model-specific prompt_name for queries\n        # EmbeddingGemma uses \"InstructionRetrieval\" for queries\n        # BGE-M3 doesn't use prompts\n        if prompt_name and \"gemma\" in self.model_name.lower():\n            # For Gemma, use InstructionRetrieval for queries\n            embedding = self.model.encode(\n                [query], prompt_name=\"InstructionRetrieval\", show_progress_bar=False\n            )[0]\n        elif prompt_name:\n            # For other models with prompts, use the document prompt\n            embedding = self.model.encode(\n                [query], prompt_name=prompt_name, show_progress_bar=False\n            )[0]\n        else:\n            # For models without prompts (like BGE-M3)\n            embedding = self.model.encode([query], show_progress_bar=False)[0]\n\n        return embedding\n\n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the loaded model.\"\"\"\n        if self._model is None:\n            return {\"status\": \"not_loaded\"}\n\n        return {\n            \"model_name\": self.model_name,\n            \"embedding_dimension\": self._model.get_sentence_embedding_dimension(),\n            \"max_seq_length\": getattr(self._model, \"max_seq_length\", \"unknown\"),\n            \"device\": str(self._model.device),\n            \"status\": \"loaded\",\n        }\n\n    def cleanup(self):\n        \"\"\"Clean up model from memory to free GPU/CPU resources.\"\"\"\n        if self._model is not None:\n            try:\n                # Move model to CPU and delete\n                if hasattr(self._model, \"to\"):\n                    self._model.to(\"cpu\")\n                if torch is not None and torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                del self._model\n                self._model = None\n                self._logger.info(\"Model cleaned up and memory freed\")\n            except Exception as e:\n                self._logger.warning(f\"Error during model cleanup: {e}\")\n\n    def __enter__(self):\n        \"\"\"Context manager entry - ensure model is loaded.\"\"\"\n        # Trigger model loading\n        _ = self.model\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit - cleanup resources.\"\"\"\n        self.cleanup()\n        return False  # Don't suppress exceptions\n\n    def __del__(self):\n        \"\"\"Ensure cleanup when object is destroyed.\"\"\"\n        try:\n            self.cleanup()\n        except Exception:\n            pass\n\n    def _is_model_cached(self) -> bool:\n        \"\"\"Best-effort check if the target model seems cached in cache_dir.\n\n        We look for a directory under cache_dir that contains the model key\n        (final path segment of model name) and a SentenceTransformers config file.\n        \"\"\"\n        if not self.cache_dir:\n            return False\n        try:\n            model_key = self.model_name.split(\"/\")[-1].lower()\n            cache_root = Path(self.cache_dir)\n            if not cache_root.exists():\n                return False\n            for path in cache_root.rglob(\"config_sentence_transformers.json\"):\n                parent_str = str(path.parent).lower()\n                if model_key in parent_str:\n                    return True\n            # Fallback: look for folders that include the model key\n            for d in cache_root.glob(\"**/*\"):\n                if d.is_dir() and model_key in d.name.lower():\n                    if (d / \"config_sentence_transformers.json\").exists() or (\n                        d / \"README.md\"\n                    ).exists():\n                        return True\n        except Exception:\n            return False\n        return False\n\n    def _find_local_model_dir(self) -> Optional[Path]:\n        \"\"\"Locate the cached model directory if available.\"\"\"\n        if not self.cache_dir:\n            return None\n        try:\n            model_key = self.model_name.split(\"/\")[-1].lower()\n            cache_root = Path(self.cache_dir)\n            if not cache_root.exists():\n                return None\n            for path in cache_root.rglob(\"config_sentence_transformers.json\"):\n                parent = path.parent\n                if model_key in str(parent).lower():\n                    return parent\n            # Fallback: return the most likely directory matching the model key\n            candidates = [\n                d\n                for d in cache_root.glob(\"**/*\")\n                if d.is_dir() and model_key in d.name.lower()\n            ]\n            return candidates[0] if candidates else None\n        except Exception:\n            return None\n\n    def _resolve_device(self, requested: Optional[str]) -> str:\n        \"\"\"Resolve target device string.\n        - \"auto\": prefer cuda, then mps, else cpu\n        - explicit values are validated and coerced to available devices\n        \"\"\"\n        req = (requested or \"auto\").lower()\n        # If torch is not available, default to CPU\n        if torch is None:\n            return \"cpu\"\n        if req in (\"auto\", \"none\", \"\"):\n            if torch.cuda.is_available():\n                return \"cuda\"\n            # MPS for Apple Silicon\n            try:\n                if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n                    return \"mps\"\n            except Exception:\n                pass\n            return \"cpu\"\n        # Validate explicit devices\n        if req.startswith(\"cuda\"):\n            return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if req == \"mps\":\n            try:\n                return (\n                    \"mps\"\n                    if hasattr(torch.backends, \"mps\")\n                    and torch.backends.mps.is_available()\n                    else \"cpu\"\n                )\n            except Exception:\n                return \"cpu\"\n        # Default fallback\n        return \"cpu\"",
              "content_preview": "class CodeEmbedder:\n    \"\"\"Multi-model embedder for generating code embeddings.\n\n    Supports multiple embedding models with automatic configuration detection.\n    Default model is google/embeddinggem...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014786324786324787,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "embeddings\\embedder.py:120-163:method:_load_model",
            "score": 9.287,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\embeddings\\embedder.py",
              "relative_path": "embeddings\\embedder.py",
              "folder_structure": [
                "embeddings"
              ],
              "chunk_type": "method",
              "start_line": 120,
              "end_line": 163,
              "name": "_load_model",
              "parent_name": "CodeEmbedder",
              "docstring": "Load the EmbeddingGemma model.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def _load_model(self):\n        \"\"\"Load the EmbeddingGemma model.\"\"\"\n        if SentenceTransformer is None:\n            raise ImportError(\n                \"sentence-transformers not found. Install with: \"\n                \"pip install sentence-transformers>=5.0.0\"\n            )\n\n        self._logger.info(f\"Loading model: {self.model_name}\")\n\n        # If the model appears to be cached locally, enable offline mode to avoid network HEAD/GET checks\n        local_model_dir = None\n        try:\n            if self._is_model_cached():\n                os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n                os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n                self._logger.info(\n                    \"Model cache detected. Enabling offline mode for faster startup.\"\n                )\n                # Prefer loading directly from local cache path to avoid any remote HEAD/GET\n                local_model_dir = self._find_local_model_dir()\n                if local_model_dir:\n                    self._logger.info(\n                        f\"Loading model from local cache path: {local_model_dir}\"\n                    )\n        except Exception as _e:\n            # Best-effort; continue without failing if detection has issues\n            self._logger.debug(f\"Offline mode detection skipped: {_e}\")\n\n        try:\n            model_source = str(local_model_dir) if local_model_dir else self.model_name\n            resolved_device = self._resolve_device(self.device)\n            self._model = SentenceTransformer(\n                model_source, cache_folder=self.cache_dir, device=resolved_device\n            )\n            # Persist resolved device for later info\n            self.device = resolved_device\n            self._logger.info(\n                f\"Model loaded successfully on device: {self._model.device}\"\n            )\n\n        except Exception as e:\n            self._logger.error(f\"Failed to load model: {e}\")\n            raise",
              "content_preview": "def _load_model(self):\n        \"\"\"Load the EmbeddingGemma model.\"\"\"\n        if SentenceTransformer is None:\n            raise ImportError(\n                \"sentence-transformers not found. Install wit...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014434675935391534,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "mcp_server\\server.py:147-168:function:get_embedder",
          "scripts\\verify_installation.py:501-548:method:test_embedding_model",
          "scripts\\verify_hf_auth.py:46-72:function:test_model_loading",
          "embeddings\\embedder.py:420-424:method:__enter__",
          "tests\\integration\\test_model_switching.py:83-131:decorated_definition:TestBGEM3EmbeddingGeneration",
          "tests\\integration\\test_model_switching.py:36-80:class:TestGemmaEmbeddingGeneration",
          "mcp_server\\server.py:171-194:function:_maybe_start_model_preload",
          "embeddings\\embedder.py:120-163:method:_load_model",
          "embeddings\\__init__.py:1-2:module",
          "embeddings\\embedder.py:38-522:class:CodeEmbedder"
        ]
      },
      "multi_hop": {
        "time_ms": 44.98,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "scripts\\verify_installation.py:501-548:method:test_embedding_model",
            "score": 14.85,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\verify_installation.py",
              "relative_path": "scripts\\verify_installation.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "method",
              "start_line": 501,
              "end_line": 548,
              "name": "test_embedding_model",
              "parent_name": "InstallationVerifier",
              "docstring": "Test EmbeddingGemma model loading (optional).",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_embedding_model(self) -> bool:\n        \"\"\"Test EmbeddingGemma model loading (optional).\"\"\"\n        print()\n        print(\"=== EmbeddingGemma Model Test ===\")\n        print()\n        print(\"[INFO] This test will download ~1.2GB model if not cached...\")\n\n        embedding_code = \"\"\"\ntry:\n    from sentence_transformers import SentenceTransformer\n    import torch\n\n    print('Loading EmbeddingGemma model...')\n    model = SentenceTransformer('google/embeddinggemma-300m')\n    print(f'Model loaded on device: {model.device}')\n\n    test_text = 'def hello_world(): return \"Hello World\"'\n    embedding = model.encode([test_text])\n    print(f'Generated embedding shape: {embedding.shape}')\n    print('EmbeddingGemma test successful')\n\nexcept Exception as e:\n    print(f'EmbeddingGemma test failed: {e}')\n    print('Note: This is acceptable if model is not downloaded yet')\n    raise\n\"\"\"\n\n        success, output = self._run_python_test(\n            embedding_code, \"EmbeddingGemma Model Loading\"\n        )\n\n        if success:\n            self._print_test_result(\n                \"EmbeddingGemma Model Loading\",\n                True,\n                details=\"EmbeddingGemma model working\",\n            )\n            for line in output.strip().split(\"\\n\"):\n                if line.strip():\n                    print(f\"       {line.strip()}\")\n        else:\n            self._print_warning(\n                \"EmbeddingGemma Model Loading\",\n                \"EmbeddingGemma model test failed\",\n                \"Model will be downloaded on first use\",\n            )\n\n        return True  # Don't fail overall test for model download issues",
              "content_preview": "def test_embedding_model(self) -> bool:\n        \"\"\"Test EmbeddingGemma model loading (optional).\"\"\"\n        print()\n        print(\"=== EmbeddingGemma Model Test ===\")\n        print()\n        print(\"[I...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01564828614008942,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "scripts\\verify_hf_auth.py:46-72:function:test_model_loading",
            "score": 14.187,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\verify_hf_auth.py",
              "relative_path": "scripts\\verify_hf_auth.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "function",
              "start_line": 46,
              "end_line": 72,
              "name": "test_model_loading",
              "parent_name": null,
              "docstring": "Test loading the EmbeddingGemma model.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_model_loading():\n    \"\"\"Test loading the EmbeddingGemma model.\"\"\"\n    print(\"\\n=== Model Loading Test ===\\n\")\n\n    try:\n        from sentence_transformers import SentenceTransformer\n\n        print(\"\ud83d\udd04 [INFO] Initializing SentenceTransformer...\")\n        print(\"\u23f3 [INFO] This may download the model (~1.3GB) if not cached...\")\n\n        model = SentenceTransformer(\"google/embeddinggemma-300m\")\n        print(\"\u2705 [OK] Model loaded successfully!\")\n\n        # Test encoding\n        print(\"\ud83e\uddea [INFO] Testing model encoding...\")\n        test_text = \"def test_function(): return True\"\n        embedding = model.encode(test_text)\n        print(f\"\u2705 [OK] Encoding successful! Embedding dimension: {len(embedding)}\")\n\n        return True\n    except Exception as e:\n        print(f\"\u274c [ERROR] Model loading failed: {e}\")\n        print(\"\ud83d\udca1 [HELP] This could indicate:\")\n        print(\"   1. Model license not accepted\")\n        print(\"   2. Authentication issues\")\n        print(\"   3. Network connectivity problems\")\n        return False",
              "content_preview": "def test_model_loading():\n    \"\"\"Test loading the EmbeddingGemma model.\"\"\"\n    print(\"\\n=== Model Loading Test ===\\n\")\n\n    try:\n        from sentence_transformers import SentenceTransformer\n\n        ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015147265077138851,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "mcp_server\\server.py:147-168:function:get_embedder",
            "score": 11.454,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "function",
              "start_line": 147,
              "end_line": 168,
              "name": "get_embedder",
              "parent_name": null,
              "docstring": "Lazy initialization of embedder with configurable model.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def get_embedder() -> CodeEmbedder:\n    \"\"\"Lazy initialization of embedder with configurable model.\"\"\"\n    global _embedder\n    if _embedder is None:\n        cache_dir = get_storage_dir() / \"models\"\n        cache_dir.mkdir(exist_ok=True)\n\n        # Get model from config (defaults to Gemma for backward compatibility)\n        try:\n            from search.config import get_search_config\n\n            config = get_search_config()\n            model_name = config.embedding_model_name\n            logger.info(f\"Using embedding model: {model_name}\")\n        except Exception as e:\n            logger.warning(f\"Failed to load model from config: {e}\")\n            model_name = \"google/embeddinggemma-300m\"  # Fallback to default\n            logger.info(f\"Falling back to default model: {model_name}\")\n\n        _embedder = CodeEmbedder(model_name=model_name, cache_dir=str(cache_dir))\n        logger.info(\"Embedder initialized successfully\")\n    return _embedder",
              "content_preview": "def get_embedder() -> CodeEmbedder:\n    \"\"\"Lazy initialization of embedder with configurable model.\"\"\"\n    global _embedder\n    if _embedder is None:\n        cache_dir = get_storage_dir() / \"models\"\n ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016185271922976842,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_model_switching.py:36-80:class:TestGemmaEmbeddingGeneration",
            "score": 10.93,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_model_switching.py",
              "relative_path": "tests\\integration\\test_model_switching.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "class",
              "start_line": 36,
              "end_line": 80,
              "name": "TestGemmaEmbeddingGeneration",
              "parent_name": null,
              "docstring": "Test embedding generation with Gemma model.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestGemmaEmbeddingGeneration:\n    \"\"\"Test embedding generation with Gemma model.\"\"\"\n\n    def test_gemma_chunk_embedding(self, sample_code_chunk):\n        \"\"\"Test generating embedding for a single chunk with Gemma.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n        result = embedder.embed_chunk(sample_code_chunk)\n\n        assert result.embedding is not None\n        assert result.embedding.shape[0] == 768  # Gemma dimension\n        assert result.chunk_id == \"test.py:1-5:function:test_function\"\n        assert isinstance(result.embedding, np.ndarray)\n\n    def test_gemma_query_embedding(self):\n        \"\"\"Test generating embedding for a query with Gemma.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n        query = \"function that adds two numbers\"\n        embedding = embedder.embed_query(query)\n\n        assert embedding is not None\n        assert embedding.shape[0] == 768\n        assert isinstance(embedding, np.ndarray)\n\n    def test_gemma_batch_embedding(self, sample_code_chunk):\n        \"\"\"Test batch embedding generation with Gemma.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n\n        # Create multiple chunks\n        chunks = [sample_code_chunk for _ in range(5)]\n        results = embedder.embed_chunks(chunks, batch_size=2)\n\n        assert len(results) == 5\n        for result in results:\n            assert result.embedding.shape[0] == 768\n\n    def test_gemma_model_info(self):\n        \"\"\"Test getting model info for Gemma.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n        # Trigger model loading\n        _ = embedder.model\n        info = embedder.get_model_info()\n\n        assert info[\"model_name\"] == \"google/embeddinggemma-300m\"\n        assert info[\"embedding_dimension\"] == 768\n        assert info[\"status\"] == \"loaded\"",
              "content_preview": "class TestGemmaEmbeddingGeneration:\n    \"\"\"Test embedding generation with Gemma model.\"\"\"\n\n    def test_gemma_chunk_embedding(self, sample_code_chunk):\n        \"\"\"Test generating embedding for a singl...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014261954261954264,
              "appears_in_lists": 2,
              "final_rank": 6
            }
          },
          {
            "doc_id": "tests\\integration\\test_model_switching.py:83-131:decorated_definition:TestBGEM3EmbeddingGeneration",
            "score": 10.799,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_model_switching.py",
              "relative_path": "tests\\integration\\test_model_switching.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 83,
              "end_line": 131,
              "name": "TestBGEM3EmbeddingGeneration",
              "parent_name": null,
              "docstring": "Test embedding generation with BGE-M3 model.\n\n    Marked as slow because BGE-M3 is a larger model.",
              "decorators": [
                "@pytest.mark.slow"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@pytest.mark.slow\nclass TestBGEM3EmbeddingGeneration:\n    \"\"\"Test embedding generation with BGE-M3 model.\n\n    Marked as slow because BGE-M3 is a larger model.\n    \"\"\"\n\n    def test_bge_m3_chunk_embedding(self, sample_code_chunk):\n        \"\"\"Test generating embedding for a single chunk with BGE-M3.\"\"\"\n        embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n        result = embedder.embed_chunk(sample_code_chunk)\n\n        assert result.embedding is not None\n        assert result.embedding.shape[0] == 1024  # BGE-M3 dimension\n        assert result.chunk_id == \"test.py:1-5:function:test_function\"\n        assert isinstance(result.embedding, np.ndarray)\n\n    def test_bge_m3_query_embedding(self):\n        \"\"\"Test generating embedding for a query with BGE-M3.\"\"\"\n        embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n        query = \"function that adds two numbers\"\n        embedding = embedder.embed_query(query)\n\n        assert embedding is not None\n        assert embedding.shape[0] == 1024\n        assert isinstance(embedding, np.ndarray)\n\n    def test_bge_m3_batch_embedding(self, sample_code_chunk):\n        \"\"\"Test batch embedding generation with BGE-M3.\"\"\"\n        embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n\n        # Create multiple chunks\n        chunks = [sample_code_chunk for _ in range(5)]\n        results = embedder.embed_chunks(chunks, batch_size=2)\n\n        assert len(results) == 5\n        for result in results:\n            assert result.embedding.shape[0] == 1024\n\n    def test_bge_m3_model_info(self):\n        \"\"\"Test getting model info for BGE-M3.\"\"\"\n        embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n        # Trigger model loading\n        _ = embedder.model\n        info = embedder.get_model_info()\n\n        assert info[\"model_name\"] == \"BAAI/bge-m3\"\n        assert info[\"embedding_dimension\"] == 1024\n        assert info[\"status\"] == \"loaded\"",
              "content_preview": "@pytest.mark.slow\nclass TestBGEM3EmbeddingGeneration:\n    \"\"\"Test embedding generation with BGE-M3 model.\n\n    Marked as slow because BGE-M3 is a larger model.\n    \"\"\"\n\n    def test_bge_m3_chunk_embed...",
              "project_name": "claude-context-local",
              "rrf_score": 0.013752913752913754,
              "appears_in_lists": 2,
              "final_rank": 7
            }
          }
        ],
        "all_doc_ids": [
          "scripts\\verify_installation.py:501-548:method:test_embedding_model",
          "mcp_server\\server.py:147-168:function:get_embedder",
          "scripts\\verify_hf_auth.py:46-72:function:test_model_loading",
          "embeddings\\embedder.py:420-424:method:__enter__",
          "tests\\integration\\test_model_switching.py:83-131:decorated_definition:TestBGEM3EmbeddingGeneration",
          "tests\\integration\\test_model_switching.py:71-80:method:test_gemma_model_info",
          "mcp_server\\server.py:1421-1455:decorated_definition:list_embedding_models",
          "tests\\integration\\test_model_switching.py:36-80:class:TestGemmaEmbeddingGeneration",
          "embeddings\\embedder.py:120-163:method:_load_model",
          "embeddings\\embedder.py:38-522:class:CodeEmbedder"
        ],
        "unique_discoveries": [
          "mcp_server\\server.py:1421-1455:decorated_definition:list_embedding_models",
          "tests\\integration\\test_model_switching.py:71-80:method:test_gemma_model_info"
        ]
      },
      "comparison": {
        "time_overhead_ms": 24.79,
        "time_overhead_pct": 122.7,
        "top5_overlap_count": 3,
        "top5_overlap_pct": 60.0,
        "unique_discovery_count": 2,
        "value_rating": "MEDIUM"
      }
    },
    {
      "query": "chunking code into semantic units",
      "single_hop": {
        "time_ms": 21.92,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "chunking\\multi_language_chunker.py:116-138:method:chunk_file",
            "score": 11.956,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\multi_language_chunker.py",
              "relative_path": "chunking\\multi_language_chunker.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "method",
              "start_line": 116,
              "end_line": 138,
              "name": "chunk_file",
              "parent_name": "MultiLanguageChunker",
              "docstring": "Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            List of CodeChunk objects",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def chunk_file(self, file_path: str) -> List[CodeChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            List of CodeChunk objects\n        \"\"\"\n        if not self.is_supported(file_path):\n            logger.debug(f\"File type not supported: {file_path}\")\n            return []\n\n        Path(file_path).suffix.lower()\n\n        # Use tree-sitter for all  languages\n        try:\n            tree_chunks = self.tree_sitter_chunker.chunk_file(file_path)\n            # Convert TreeSitterChunk to CodeChunk\n            return self._convert_tree_chunks(tree_chunks, file_path)\n        except Exception as e:\n            logger.error(f\"Failed to chunk file {file_path}: {e}\")\n            return []",
              "content_preview": "def chunk_file(self, file_path: str) -> List[CodeChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            List of CodeC...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015932377049180328,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "chunking\\tree_sitter.py:211-276:method:chunk_code",
            "score": 9.304,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\tree_sitter.py",
              "relative_path": "chunking\\tree_sitter.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "method",
              "start_line": 211,
              "end_line": 276,
              "name": "chunk_code",
              "parent_name": "LanguageChunker",
              "docstring": "Chunk source code into semantic units.\n\n        Args:\n            source_code: Source code string\n\n        Returns:\n            List of TreeSitterChunk objects",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def chunk_code(self, source_code: str) -> List[TreeSitterChunk]:\n        \"\"\"Chunk source code into semantic units.\n\n        Args:\n            source_code: Source code string\n\n        Returns:\n            List of TreeSitterChunk objects\n        \"\"\"\n        source_bytes = bytes(source_code, \"utf-8\")\n        tree = self.parser.parse(source_bytes)\n        chunks = []\n\n        def traverse(node, depth=0, parent_info=None):\n            \"\"\"Recursively traverse the tree and extract chunks.\"\"\"\n            if self.should_chunk_node(node):\n                start_line, end_line = self.get_line_numbers(node)\n                content = self.get_node_text(node, source_bytes)\n                metadata = self.extract_metadata(node, source_bytes)\n\n                # Add parent information if available\n                if parent_info:\n                    metadata.update(parent_info)\n\n                chunk = TreeSitterChunk(\n                    content=content,\n                    start_line=start_line,\n                    end_line=end_line,\n                    node_type=node.type,\n                    language=self.language_name,\n                    metadata=metadata,\n                )\n                chunks.append(chunk)\n\n                # For classes, continue traversing to find methods\n                # For other chunked nodes, stop traversal\n                if node.type in [\"class_definition\", \"class_declaration\"]:\n                    # Pass class info to children\n                    class_info = {\n                        \"parent_name\": metadata.get(\"name\"),\n                        \"parent_type\": \"class\",\n                    }\n                    for child in node.children:\n                        traverse(child, depth + 1, class_info)\n                return\n\n            # Traverse children, passing along parent info\n            for child in node.children:\n                traverse(child, depth + 1, parent_info)\n\n        traverse(tree.root_node)\n\n        # If no chunks found, create a single module-level chunk\n        if not chunks and source_code.strip():\n            chunks.append(\n                TreeSitterChunk(\n                    content=source_code,\n                    start_line=1,\n                    end_line=len(source_code.split(\"\\n\")),\n                    node_type=\"module\",\n                    language=self.language_name,\n                    metadata={\"type\": \"module\"},\n                )\n            )\n\n        return chunks",
              "content_preview": "def chunk_code(self, source_code: str) -> List[TreeSitterChunk]:\n        \"\"\"Chunk source code into semantic units.\n\n        Args:\n            source_code: Source code string\n\n        Returns:\n        ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01592741935483871,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "chunking\\__init__.py:1-2:module",
            "score": 8.777,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\__init__.py",
              "relative_path": "chunking\\__init__.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "module",
              "start_line": 1,
              "end_line": 2,
              "name": null,
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "\"\"\"Intelligent code chunking module.\"\"\"\n",
              "content_preview": "\"\"\"Intelligent code chunking module.\"\"\"\n",
              "project_name": "claude-context-local",
              "rrf_score": 0.015896671634376552,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "chunking\\tree_sitter.py:1068-1108:method:chunk_file",
            "score": 7.677,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\tree_sitter.py",
              "relative_path": "chunking\\tree_sitter.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "method",
              "start_line": 1068,
              "end_line": 1108,
              "name": "chunk_file",
              "parent_name": "TreeSitterChunker",
              "docstring": "Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n            content: Optional file content (will read from file if not provided)\n\n        Returns:\n            List of TreeSitterChunk objects",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def chunk_file(\n        self, file_path: str, content: Optional[str] = None\n    ) -> List[TreeSitterChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n            content: Optional file content (will read from file if not provided)\n\n        Returns:\n            List of TreeSitterChunk objects\n        \"\"\"\n        chunker = self.get_chunker(file_path)\n\n        if not chunker:\n            logger.debug(f\"No tree-sitter chunker available for {file_path}\")\n            return []\n\n        if content is None:\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    content = f.read()\n            except UnicodeDecodeError:\n                logger.warning(\n                    f\"UTF-8 decode failed for {file_path}, trying with error handling\"\n                )\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                        content = f.read()\n                except Exception as e:\n                    logger.error(f\"Failed to read file {file_path}: {e}\")\n                    return []\n            except Exception as e:\n                logger.error(f\"Failed to read file {file_path}: {e}\")\n                return []\n\n        try:\n            return chunker.chunk_code(content)\n        except Exception as e:\n            logger.warning(f\"Tree-sitter parsing failed for {file_path}: {e}\")\n            return []",
              "content_preview": "def chunk_file(\n        self, file_path: str, content: Optional[str] = None\n    ) -> List[TreeSitterChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to t...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015238095238095238,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "tests\\integration\\test_mcp_functionality.py:58-94:function:test_chunking_core",
            "score": 9.3,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_mcp_functionality.py",
              "relative_path": "tests\\integration\\test_mcp_functionality.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "function",
              "start_line": 58,
              "end_line": 94,
              "name": "test_chunking_core",
              "parent_name": null,
              "docstring": "Test core chunking functionality.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_chunking_core():\n    \"\"\"Test core chunking functionality.\"\"\"\n    print(\"\\nTesting chunking functionality...\")\n\n    try:\n        # Create a simple test\n        import tempfile\n\n        from chunking.multi_language_chunker import MultiLanguageChunker\n\n        test_code = '''\ndef hello_world():\n    \"\"\"Simple test function.\"\"\"\n    return \"Hello, World!\"\n\nclass TestClass:\n    \"\"\"Simple test class.\"\"\"\n    def __init__(self):\n        self.value = 42\n'''\n\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(test_code)\n            f.flush()\n\n            chunker = MultiLanguageChunker(Path(f.name).parent)\n            chunks = chunker.chunk_file(f.name)\n\n            print(f\"[OK] Chunking test completed: {len(chunks)} chunks generated\")\n\n            # Clean up\n            Path(f.name).unlink()\n\n        return True\n    except Exception as e:\n        print(f\"[ERROR] Chunking test failed: {e}\")\n        return False",
              "content_preview": "def test_chunking_core():\n    \"\"\"Test core chunking functionality.\"\"\"\n    print(\"\\nTesting chunking functionality...\")\n\n    try:\n        # Create a simple test\n        import tempfile\n\n        from ch...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014849498327759197,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "tests\\unit\\test_tree_sitter.py:21-46:method:test_function_chunking",
          "chunking\\python_ast_chunker.py:8-48:decorated_definition:CodeChunk",
          "tests\\integration\\test_mcp_functionality.py:58-94:function:test_chunking_core",
          "tests\\unit\\test_tree_sitter.py:78-99:method:test_decorated_definition",
          "tests\\unit\\test_tree_sitter.py:189-209:method:test_chunk_python_file",
          "chunking\\tree_sitter.py:211-276:method:chunk_code",
          "chunking\\tree_sitter.py:1068-1108:method:chunk_file",
          "chunking\\tree_sitter.py:135-276:class:LanguageChunker",
          "chunking\\__init__.py:1-2:module",
          "chunking\\multi_language_chunker.py:116-138:method:chunk_file"
        ]
      },
      "multi_hop": {
        "time_ms": 49.73,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "chunking\\multi_language_chunker.py:116-138:method:chunk_file",
            "score": 11.956,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\multi_language_chunker.py",
              "relative_path": "chunking\\multi_language_chunker.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "method",
              "start_line": 116,
              "end_line": 138,
              "name": "chunk_file",
              "parent_name": "MultiLanguageChunker",
              "docstring": "Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            List of CodeChunk objects",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def chunk_file(self, file_path: str) -> List[CodeChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            List of CodeChunk objects\n        \"\"\"\n        if not self.is_supported(file_path):\n            logger.debug(f\"File type not supported: {file_path}\")\n            return []\n\n        Path(file_path).suffix.lower()\n\n        # Use tree-sitter for all  languages\n        try:\n            tree_chunks = self.tree_sitter_chunker.chunk_file(file_path)\n            # Convert TreeSitterChunk to CodeChunk\n            return self._convert_tree_chunks(tree_chunks, file_path)\n        except Exception as e:\n            logger.error(f\"Failed to chunk file {file_path}: {e}\")\n            return []",
              "content_preview": "def chunk_file(self, file_path: str) -> List[CodeChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            List of CodeC...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015932377049180328,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "chunking\\tree_sitter.py:211-276:method:chunk_code",
            "score": 9.304,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\tree_sitter.py",
              "relative_path": "chunking\\tree_sitter.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "method",
              "start_line": 211,
              "end_line": 276,
              "name": "chunk_code",
              "parent_name": "LanguageChunker",
              "docstring": "Chunk source code into semantic units.\n\n        Args:\n            source_code: Source code string\n\n        Returns:\n            List of TreeSitterChunk objects",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def chunk_code(self, source_code: str) -> List[TreeSitterChunk]:\n        \"\"\"Chunk source code into semantic units.\n\n        Args:\n            source_code: Source code string\n\n        Returns:\n            List of TreeSitterChunk objects\n        \"\"\"\n        source_bytes = bytes(source_code, \"utf-8\")\n        tree = self.parser.parse(source_bytes)\n        chunks = []\n\n        def traverse(node, depth=0, parent_info=None):\n            \"\"\"Recursively traverse the tree and extract chunks.\"\"\"\n            if self.should_chunk_node(node):\n                start_line, end_line = self.get_line_numbers(node)\n                content = self.get_node_text(node, source_bytes)\n                metadata = self.extract_metadata(node, source_bytes)\n\n                # Add parent information if available\n                if parent_info:\n                    metadata.update(parent_info)\n\n                chunk = TreeSitterChunk(\n                    content=content,\n                    start_line=start_line,\n                    end_line=end_line,\n                    node_type=node.type,\n                    language=self.language_name,\n                    metadata=metadata,\n                )\n                chunks.append(chunk)\n\n                # For classes, continue traversing to find methods\n                # For other chunked nodes, stop traversal\n                if node.type in [\"class_definition\", \"class_declaration\"]:\n                    # Pass class info to children\n                    class_info = {\n                        \"parent_name\": metadata.get(\"name\"),\n                        \"parent_type\": \"class\",\n                    }\n                    for child in node.children:\n                        traverse(child, depth + 1, class_info)\n                return\n\n            # Traverse children, passing along parent info\n            for child in node.children:\n                traverse(child, depth + 1, parent_info)\n\n        traverse(tree.root_node)\n\n        # If no chunks found, create a single module-level chunk\n        if not chunks and source_code.strip():\n            chunks.append(\n                TreeSitterChunk(\n                    content=source_code,\n                    start_line=1,\n                    end_line=len(source_code.split(\"\\n\")),\n                    node_type=\"module\",\n                    language=self.language_name,\n                    metadata={\"type\": \"module\"},\n                )\n            )\n\n        return chunks",
              "content_preview": "def chunk_code(self, source_code: str) -> List[TreeSitterChunk]:\n        \"\"\"Chunk source code into semantic units.\n\n        Args:\n            source_code: Source code string\n\n        Returns:\n        ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01592741935483871,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\integration\\test_mcp_functionality.py:58-94:function:test_chunking_core",
            "score": 9.3,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_mcp_functionality.py",
              "relative_path": "tests\\integration\\test_mcp_functionality.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "function",
              "start_line": 58,
              "end_line": 94,
              "name": "test_chunking_core",
              "parent_name": null,
              "docstring": "Test core chunking functionality.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_chunking_core():\n    \"\"\"Test core chunking functionality.\"\"\"\n    print(\"\\nTesting chunking functionality...\")\n\n    try:\n        # Create a simple test\n        import tempfile\n\n        from chunking.multi_language_chunker import MultiLanguageChunker\n\n        test_code = '''\ndef hello_world():\n    \"\"\"Simple test function.\"\"\"\n    return \"Hello, World!\"\n\nclass TestClass:\n    \"\"\"Simple test class.\"\"\"\n    def __init__(self):\n        self.value = 42\n'''\n\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(test_code)\n            f.flush()\n\n            chunker = MultiLanguageChunker(Path(f.name).parent)\n            chunks = chunker.chunk_file(f.name)\n\n            print(f\"[OK] Chunking test completed: {len(chunks)} chunks generated\")\n\n            # Clean up\n            Path(f.name).unlink()\n\n        return True\n    except Exception as e:\n        print(f\"[ERROR] Chunking test failed: {e}\")\n        return False",
              "content_preview": "def test_chunking_core():\n    \"\"\"Test core chunking functionality.\"\"\"\n    print(\"\\nTesting chunking functionality...\")\n\n    try:\n        # Create a simple test\n        import tempfile\n\n        from ch...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014849498327759197,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          },
          {
            "doc_id": "chunking\\__init__.py:1-2:module",
            "score": 8.777,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\__init__.py",
              "relative_path": "chunking\\__init__.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "module",
              "start_line": 1,
              "end_line": 2,
              "name": null,
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "\"\"\"Intelligent code chunking module.\"\"\"\n",
              "content_preview": "\"\"\"Intelligent code chunking module.\"\"\"\n",
              "project_name": "claude-context-local",
              "rrf_score": 0.015896671634376552,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "chunking\\tree_sitter.py:1068-1108:method:chunk_file",
            "score": 7.677,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\tree_sitter.py",
              "relative_path": "chunking\\tree_sitter.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "method",
              "start_line": 1068,
              "end_line": 1108,
              "name": "chunk_file",
              "parent_name": "TreeSitterChunker",
              "docstring": "Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n            content: Optional file content (will read from file if not provided)\n\n        Returns:\n            List of TreeSitterChunk objects",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def chunk_file(\n        self, file_path: str, content: Optional[str] = None\n    ) -> List[TreeSitterChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n            content: Optional file content (will read from file if not provided)\n\n        Returns:\n            List of TreeSitterChunk objects\n        \"\"\"\n        chunker = self.get_chunker(file_path)\n\n        if not chunker:\n            logger.debug(f\"No tree-sitter chunker available for {file_path}\")\n            return []\n\n        if content is None:\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    content = f.read()\n            except UnicodeDecodeError:\n                logger.warning(\n                    f\"UTF-8 decode failed for {file_path}, trying with error handling\"\n                )\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                        content = f.read()\n                except Exception as e:\n                    logger.error(f\"Failed to read file {file_path}: {e}\")\n                    return []\n            except Exception as e:\n                logger.error(f\"Failed to read file {file_path}: {e}\")\n                return []\n\n        try:\n            return chunker.chunk_code(content)\n        except Exception as e:\n            logger.warning(f\"Tree-sitter parsing failed for {file_path}: {e}\")\n            return []",
              "content_preview": "def chunk_file(\n        self, file_path: str, content: Optional[str] = None\n    ) -> List[TreeSitterChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to t...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015238095238095238,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          }
        ],
        "all_doc_ids": [
          "tests\\integration\\test_mcp_functionality.py:58-94:function:test_chunking_core",
          "tests\\integration\\test_system.py:13-124:function:test_chunking",
          "tests\\unit\\test_tree_sitter.py:78-99:method:test_decorated_definition",
          "tests\\unit\\test_tree_sitter.py:189-209:method:test_chunk_python_file",
          "chunking\\tree_sitter.py:211-276:method:chunk_code",
          "tests\\unit\\test_tree_sitter.py:48-76:method:test_class_chunking",
          "chunking\\tree_sitter.py:1068-1108:method:chunk_file",
          "chunking\\tree_sitter.py:135-276:class:LanguageChunker",
          "chunking\\__init__.py:1-2:module",
          "chunking\\multi_language_chunker.py:116-138:method:chunk_file"
        ],
        "unique_discoveries": [
          "tests\\unit\\test_tree_sitter.py:48-76:method:test_class_chunking",
          "tests\\integration\\test_system.py:13-124:function:test_chunking"
        ]
      },
      "comparison": {
        "time_overhead_ms": 27.81,
        "time_overhead_pct": 126.8,
        "top5_overlap_count": 5,
        "top5_overlap_pct": 100.0,
        "unique_discovery_count": 2,
        "value_rating": "MEDIUM"
      }
    },
    {
      "query": "AST parsing for multiple languages",
      "single_hop": {
        "time_ms": 15.13,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "chunking\\multi_language_chunker.py:93-102:method:__init__",
            "score": 13.406,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\multi_language_chunker.py",
              "relative_path": "chunking\\multi_language_chunker.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "method",
              "start_line": 93,
              "end_line": 102,
              "name": "__init__",
              "parent_name": "MultiLanguageChunker",
              "docstring": "Initialize multi-language chunker.\n\n        Args:\n            root_path: Optional root path for relative path calculation",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def __init__(self, root_path: Optional[str] = None):\n        \"\"\"Initialize multi-language chunker.\n\n        Args:\n            root_path: Optional root path for relative path calculation\n        \"\"\"\n        self.root_path = root_path\n        # Use AST chunker for Python (more mature implementation)\n        # Use tree-sitter for other languages\n        self.tree_sitter_chunker = TreeSitterChunker()",
              "content_preview": "def __init__(self, root_path: Optional[str] = None):\n        \"\"\"Initialize multi-language chunker.\n\n        Args:\n            root_path: Optional root path for relative path calculation\n        \"\"\"\n  ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016234796404019036,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
            "score": 12.502,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 704,
              "end_line": 746,
              "name": "test_multi_language_indexing",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test indexing and searching multi-language project.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_language_indexing(self, multi_lang_project_path, mock_storage_dir):\n        \"\"\"Test indexing and searching multi-language project.\"\"\"\n        # Step 1: Chunk the multi-language project\n        chunker = MultiLanguageChunker(str(multi_lang_project_path))\n        all_chunks = []\n\n        # Get all supported files\n        for ext in [\".py\", \".js\", \".ts\", \".jsx\", \".tsx\", \".svelte\"]:\n            for file_path in multi_lang_project_path.glob(f\"*{ext}\"):\n                chunks = chunker.chunk_file(str(file_path))\n                all_chunks.extend(chunks)\n\n        # Should find chunks from multiple languages\n        assert (\n            len(all_chunks) > 5\n        ), f\"Should chunk multiple files, got {len(all_chunks)}\"\n\n        # Verify we have chunks from different file types\n        file_extensions = {Path(chunk.file_path).suffix for chunk in all_chunks}\n        assert (\n            len(file_extensions) >= 3\n        ), f\"Should support multiple languages, got {file_extensions}\"\n\n        # Step 2: Create embeddings and index\n        embeddings = self._create_embeddings_from_chunks(all_chunks)\n\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(embeddings)\n\n        # Step 3: Test searching across languages\n        query_embedding = np.random.random(768).astype(np.float32)\n        results = index_manager.search(query_embedding, k=10)\n\n        assert len(results) > 0\n\n        # Verify we can find chunks from different languages\n        result_extensions = {\n            Path(metadata[\"file_path\"]).suffix for _, _, metadata in results\n        }\n        assert (\n            len(result_extensions) >= 2\n        ), f\"Should find results from multiple languages, got {result_extensions}\"",
              "content_preview": "def test_multi_language_indexing(self, multi_lang_project_path, mock_storage_dir):\n        \"\"\"Test indexing and searching multi-language project.\"\"\"\n        # Step 1: Chunk the multi-language project\n...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01597542242703533,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\unit\\test_tree_sitter.py:226-234:method:test_get_available_languages",
            "score": 8.937,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_tree_sitter.py",
              "relative_path": "tests\\unit\\test_tree_sitter.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 226,
              "end_line": 234,
              "name": "test_get_available_languages",
              "parent_name": "TestTreeSitterChunker",
              "docstring": "Test getting available languages.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_get_available_languages(self):\n        \"\"\"Test getting available languages.\"\"\"\n        languages = TreeSitterChunker.get_available_languages()\n\n        # Should have at least Python if installed\n        import chunking.tree_sitter as tsf\n\n        if \"python\" in tsf.AVAILABLE_LANGUAGES:\n            assert \"python\" in languages",
              "content_preview": "def test_get_available_languages(self):\n        \"\"\"Test getting available languages.\"\"\"\n        languages = TreeSitterChunker.get_available_languages()\n\n        # Should have at least Python if instal...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01572420634920635,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "tests\\unit\\test_tree_sitter.py:177-187:method:test_language_detection",
            "score": 5.919,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_tree_sitter.py",
              "relative_path": "tests\\unit\\test_tree_sitter.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 177,
              "end_line": 187,
              "name": "test_language_detection",
              "parent_name": "TestTreeSitterChunker",
              "docstring": "Test language detection from file extension.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_language_detection(self):\n        \"\"\"Test language detection from file extension.\"\"\"\n        # Check if Python is supported (requires tree-sitter-python)\n        import chunking.tree_sitter as tsf\n\n        if \"python\" in tsf.AVAILABLE_LANGUAGES:\n            assert self.chunker.is_supported(\"test.py\")\n\n        # These won't be supported without their packages\n        assert not self.chunker.is_supported(\"test.txt\")\n        assert not self.chunker.is_supported(\"test.md\")",
              "content_preview": "def test_language_detection(self):\n        \"\"\"Test language detection from file extension.\"\"\"\n        # Check if Python is supported (requires tree-sitter-python)\n        import chunking.tree_sitter a...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014175104228707564,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "chunking\\tree_sitter.py:1139-1146:decorated_definition:get_available_languages",
            "score": 6.554,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\tree_sitter.py",
              "relative_path": "chunking\\tree_sitter.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 1139,
              "end_line": 1146,
              "name": "get_available_languages",
              "parent_name": "TreeSitterChunker",
              "docstring": "Get list of available languages.\n\n        Returns:\n            List of language names",
              "decorators": [
                "@classmethod"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@classmethod\n    def get_available_languages(cls) -> List[str]:\n        \"\"\"Get list of available languages.\n\n        Returns:\n            List of language names\n        \"\"\"\n        return list(AVAILABLE_LANGUAGES.keys())",
              "content_preview": "@classmethod\n    def get_available_languages(cls) -> List[str]:\n        \"\"\"Get list of available languages.\n\n        Returns:\n            List of language names\n        \"\"\"\n        return list(AVAILAB...",
              "project_name": "claude-context-local",
              "rrf_score": 0.013990461049284579,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "chunking\\multi_language_chunker.py:93-102:method:__init__",
          "tests\\unit\\test_tree_sitter.py:177-187:method:test_language_detection",
          "tests\\unit\\test_tree_sitter.py:226-234:method:test_get_available_languages",
          "chunking\\tree_sitter.py:1139-1146:decorated_definition:get_available_languages",
          "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
          "chunking\\multi_language_chunker.py:13-307:class:MultiLanguageChunker",
          "tests\\integration\\test_system.py:13-124:function:test_chunking",
          "chunking\\tree_sitter.py:138-152:method:__init__",
          "tests\\integration\\test_installation.py:43-54:method:test_python_version_parsing",
          "chunking\\tree_sitter.py:998-1146:class:TreeSitterChunker"
        ]
      },
      "multi_hop": {
        "time_ms": 50.13,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "chunking\\multi_language_chunker.py:93-102:method:__init__",
            "score": 13.406,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\multi_language_chunker.py",
              "relative_path": "chunking\\multi_language_chunker.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "method",
              "start_line": 93,
              "end_line": 102,
              "name": "__init__",
              "parent_name": "MultiLanguageChunker",
              "docstring": "Initialize multi-language chunker.\n\n        Args:\n            root_path: Optional root path for relative path calculation",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def __init__(self, root_path: Optional[str] = None):\n        \"\"\"Initialize multi-language chunker.\n\n        Args:\n            root_path: Optional root path for relative path calculation\n        \"\"\"\n        self.root_path = root_path\n        # Use AST chunker for Python (more mature implementation)\n        # Use tree-sitter for other languages\n        self.tree_sitter_chunker = TreeSitterChunker()",
              "content_preview": "def __init__(self, root_path: Optional[str] = None):\n        \"\"\"Initialize multi-language chunker.\n\n        Args:\n            root_path: Optional root path for relative path calculation\n        \"\"\"\n  ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016234796404019036,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
            "score": 12.502,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 704,
              "end_line": 746,
              "name": "test_multi_language_indexing",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test indexing and searching multi-language project.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_language_indexing(self, multi_lang_project_path, mock_storage_dir):\n        \"\"\"Test indexing and searching multi-language project.\"\"\"\n        # Step 1: Chunk the multi-language project\n        chunker = MultiLanguageChunker(str(multi_lang_project_path))\n        all_chunks = []\n\n        # Get all supported files\n        for ext in [\".py\", \".js\", \".ts\", \".jsx\", \".tsx\", \".svelte\"]:\n            for file_path in multi_lang_project_path.glob(f\"*{ext}\"):\n                chunks = chunker.chunk_file(str(file_path))\n                all_chunks.extend(chunks)\n\n        # Should find chunks from multiple languages\n        assert (\n            len(all_chunks) > 5\n        ), f\"Should chunk multiple files, got {len(all_chunks)}\"\n\n        # Verify we have chunks from different file types\n        file_extensions = {Path(chunk.file_path).suffix for chunk in all_chunks}\n        assert (\n            len(file_extensions) >= 3\n        ), f\"Should support multiple languages, got {file_extensions}\"\n\n        # Step 2: Create embeddings and index\n        embeddings = self._create_embeddings_from_chunks(all_chunks)\n\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(embeddings)\n\n        # Step 3: Test searching across languages\n        query_embedding = np.random.random(768).astype(np.float32)\n        results = index_manager.search(query_embedding, k=10)\n\n        assert len(results) > 0\n\n        # Verify we can find chunks from different languages\n        result_extensions = {\n            Path(metadata[\"file_path\"]).suffix for _, _, metadata in results\n        }\n        assert (\n            len(result_extensions) >= 2\n        ), f\"Should find results from multiple languages, got {result_extensions}\"",
              "content_preview": "def test_multi_language_indexing(self, multi_lang_project_path, mock_storage_dir):\n        \"\"\"Test indexing and searching multi-language project.\"\"\"\n        # Step 1: Chunk the multi-language project\n...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01597542242703533,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\unit\\test_tree_sitter.py:226-234:method:test_get_available_languages",
            "score": 8.937,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_tree_sitter.py",
              "relative_path": "tests\\unit\\test_tree_sitter.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 226,
              "end_line": 234,
              "name": "test_get_available_languages",
              "parent_name": "TestTreeSitterChunker",
              "docstring": "Test getting available languages.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_get_available_languages(self):\n        \"\"\"Test getting available languages.\"\"\"\n        languages = TreeSitterChunker.get_available_languages()\n\n        # Should have at least Python if installed\n        import chunking.tree_sitter as tsf\n\n        if \"python\" in tsf.AVAILABLE_LANGUAGES:\n            assert \"python\" in languages",
              "content_preview": "def test_get_available_languages(self):\n        \"\"\"Test getting available languages.\"\"\"\n        languages = TreeSitterChunker.get_available_languages()\n\n        # Should have at least Python if instal...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01572420634920635,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "tests\\unit\\test_tree_sitter.py:165-234:class:TestTreeSitterChunker",
            "score": 7.901,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_tree_sitter.py",
              "relative_path": "tests\\unit\\test_tree_sitter.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "class",
              "start_line": 165,
              "end_line": 234,
              "name": "TestTreeSitterChunker",
              "parent_name": null,
              "docstring": "Test main TreeSitterChunker class.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestTreeSitterChunker(TestCase):\n    \"\"\"Test main TreeSitterChunker class.\"\"\"\n\n    def setUp(self):\n        self.chunker = TreeSitterChunker()\n        self.temp_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        import shutil\n\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_language_detection(self):\n        \"\"\"Test language detection from file extension.\"\"\"\n        # Check if Python is supported (requires tree-sitter-python)\n        import chunking.tree_sitter as tsf\n\n        if \"python\" in tsf.AVAILABLE_LANGUAGES:\n            assert self.chunker.is_supported(\"test.py\")\n\n        # These won't be supported without their packages\n        assert not self.chunker.is_supported(\"test.txt\")\n        assert not self.chunker.is_supported(\"test.md\")\n\n    def test_chunk_python_file(self):\n        \"\"\"Test chunking a Python file.\"\"\"\n        import chunking.tree_sitter as tsf\n\n        if \"python\" not in tsf.AVAILABLE_LANGUAGES:\n            self.skipTest(\"tree-sitter-python not installed\")\n\n        file_path = Path(self.temp_dir) / \"test.py\"\n        code = \"\"\"\ndef test_function():\n    return \"test\"\n\nclass TestClass:\n    pass\n\"\"\"\n        file_path.write_text(code)\n\n        chunks = self.chunker.chunk_file(str(file_path))\n\n        assert len(chunks) >= 2\n        assert all(c.language == \"python\" for c in chunks)\n\n    def test_unsupported_file(self):\n        \"\"\"Test handling of unsupported file types.\"\"\"\n        chunks = self.chunker.chunk_file(\"test.txt\", \"some text content\")\n        assert len(chunks) == 0\n\n    def test_get_supported_extensions(self):\n        \"\"\"Test getting list of supported extensions.\"\"\"\n        extensions = TreeSitterChunker.get_supported_extensions()\n\n        # At minimum Python should be supported if tree-sitter-python is installed\n        import chunking.tree_sitter as tsf\n\n        if \"python\" in tsf.AVAILABLE_LANGUAGES:\n            assert \".py\" in extensions\n\n    def test_get_available_languages(self):\n        \"\"\"Test getting available languages.\"\"\"\n        languages = TreeSitterChunker.get_available_languages()\n\n        # Should have at least Python if installed\n        import chunking.tree_sitter as tsf\n\n        if \"python\" in tsf.AVAILABLE_LANGUAGES:\n            assert \"python\" in languages",
              "content_preview": "class TestTreeSitterChunker(TestCase):\n    \"\"\"Test main TreeSitterChunker class.\"\"\"\n\n    def setUp(self):\n        self.chunker = TreeSitterChunker()\n        self.temp_dir = tempfile.mkdtemp()\n\n    def...",
              "project_name": "claude-context-local",
              "rrf_score": 0.013119429590017825,
              "appears_in_lists": 2,
              "final_rank": 9
            }
          },
          {
            "doc_id": "chunking\\tree_sitter.py:998-1146:class:TreeSitterChunker",
            "score": 7.139,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\tree_sitter.py",
              "relative_path": "chunking\\tree_sitter.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "class",
              "start_line": 998,
              "end_line": 1146,
              "name": "TreeSitterChunker",
              "parent_name": null,
              "docstring": "Main tree-sitter chunker that delegates to language-specific implementations.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TreeSitterChunker:\n    \"\"\"Main tree-sitter chunker that delegates to language-specific implementations.\"\"\"\n\n    # Map file extensions to chunker classes and language names\n    LANGUAGE_MAP = {\n        \".py\": (\"python\", PythonChunker),\n        \".js\": (\"javascript\", JavaScriptChunker),\n        \".jsx\": (\"jsx\", JSXChunker),\n        \".ts\": (\"typescript\", lambda: TypeScriptChunker(use_tsx=False)),\n        \".tsx\": (\"tsx\", lambda: TypeScriptChunker(use_tsx=True)),\n        \".svelte\": (\"svelte\", SvelteChunker),\n        \".go\": (\"go\", GoChunker),\n        \".rs\": (\"rust\", RustChunker),\n        \".java\": (\"java\", JavaChunker),\n        \".c\": (\"c\", CChunker),\n        \".cpp\": (\"cpp\", CppChunker),\n        \".cc\": (\"cpp\", CppChunker),\n        \".cxx\": (\"cpp\", CppChunker),\n        \".c++\": (\"cpp\", CppChunker),\n        \".cs\": (\"csharp\", CSharpChunker),\n        \".glsl\": (\"glsl\", GLSLChunker),\n        \".frag\": (\"glsl\", GLSLChunker),\n        \".vert\": (\"glsl\", GLSLChunker),\n        \".comp\": (\"glsl\", GLSLChunker),\n        \".geom\": (\"glsl\", GLSLChunker),\n        \".tesc\": (\"glsl\", GLSLChunker),\n        \".tese\": (\"glsl\", GLSLChunker),\n    }\n\n    def __init__(self):\n        \"\"\"Initialize the tree-sitter chunker.\"\"\"\n        self.chunkers = {}\n\n    def get_chunker(self, file_path: str) -> Optional[LanguageChunker]:\n        \"\"\"Get the appropriate chunker for a file.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            LanguageChunker instance or None if unsupported\n        \"\"\"\n        suffix = Path(file_path).suffix.lower()\n\n        if suffix not in self.LANGUAGE_MAP:\n            return None\n\n        language_name, chunker_class = self.LANGUAGE_MAP[suffix]\n\n        # Check if language is available\n        if language_name not in AVAILABLE_LANGUAGES:\n            logger.debug(\n                f\"Language {language_name} not available. Install tree-sitter-{language_name}\"\n            )\n            return None\n\n        # Lazy initialization of chunkers\n        if suffix not in self.chunkers:\n            try:\n                # Handle both class and lambda/factory function\n                if callable(chunker_class):\n                    self.chunkers[suffix] = chunker_class()\n                else:\n                    self.chunkers[suffix] = chunker_class\n            except Exception as e:\n                logger.warning(f\"Failed to initialize chunker for {suffix}: {e}\")\n                return None\n\n        return self.chunkers[suffix]\n\n    def chunk_file(\n        self, file_path: str, content: Optional[str] = None\n    ) -> List[TreeSitterChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n            content: Optional file content (will read from file if not provided)\n\n        Returns:\n            List of TreeSitterChunk objects\n        \"\"\"\n        chunker = self.get_chunker(file_path)\n\n        if not chunker:\n            logger.debug(f\"No tree-sitter chunker available for {file_path}\")\n            return []\n\n        if content is None:\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    content = f.read()\n            except UnicodeDecodeError:\n                logger.warning(\n                    f\"UTF-8 decode failed for {file_path}, trying with error handling\"\n                )\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                        content = f.read()\n                except Exception as e:\n                    logger.error(f\"Failed to read file {file_path}: {e}\")\n                    return []\n            except Exception as e:\n                logger.error(f\"Failed to read file {file_path}: {e}\")\n                return []\n\n        try:\n            return chunker.chunk_code(content)\n        except Exception as e:\n            logger.warning(f\"Tree-sitter parsing failed for {file_path}: {e}\")\n            return []\n\n    def is_supported(self, file_path: str) -> bool:\n        \"\"\"Check if a file type is supported.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            True if file type is supported\n        \"\"\"\n        suffix = Path(file_path).suffix.lower()\n        if suffix not in self.LANGUAGE_MAP:\n            return False\n\n        language_name, _ = self.LANGUAGE_MAP[suffix]\n        return language_name in AVAILABLE_LANGUAGES\n\n    @classmethod\n    def get_supported_extensions(cls) -> List[str]:\n        \"\"\"Get list of supported file extensions.\n\n        Returns:\n            List of file extensions\n        \"\"\"\n        supported = []\n        for ext, (lang_name, _) in cls.LANGUAGE_MAP.items():\n            if lang_name in AVAILABLE_LANGUAGES:\n                supported.append(ext)\n        return supported\n\n    @classmethod\n    def get_available_languages(cls) -> List[str]:\n        \"\"\"Get list of available languages.\n\n        Returns:\n            List of language names\n        \"\"\"\n        return list(AVAILABLE_LANGUAGES.keys())",
              "content_preview": "class TreeSitterChunker:\n    \"\"\"Main tree-sitter chunker that delegates to language-specific implementations.\"\"\"\n\n    # Map file extensions to chunker classes and language names\n    LANGUAGE_MAP = {\n ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.013970149253731343,
              "appears_in_lists": 2,
              "final_rank": 6
            }
          }
        ],
        "all_doc_ids": [
          "chunking\\multi_language_chunker.py:93-102:method:__init__",
          "tests\\unit\\test_tree_sitter.py:177-187:method:test_language_detection",
          "tests\\unit\\test_tree_sitter.py:216-224:method:test_get_supported_extensions",
          "tests\\unit\\test_tree_sitter.py:226-234:method:test_get_available_languages",
          "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
          "tests\\unit\\test_tree_sitter.py:165-234:class:TestTreeSitterChunker",
          "chunking\\tree_sitter.py:1139-1146:decorated_definition:get_available_languages",
          "chunking\\tree_sitter.py:138-152:method:__init__",
          "tests\\integration\\test_installation.py:43-54:method:test_python_version_parsing",
          "chunking\\tree_sitter.py:998-1146:class:TreeSitterChunker"
        ],
        "unique_discoveries": [
          "tests\\unit\\test_tree_sitter.py:165-234:class:TestTreeSitterChunker",
          "tests\\unit\\test_tree_sitter.py:216-224:method:test_get_supported_extensions"
        ]
      },
      "comparison": {
        "time_overhead_ms": 35.01,
        "time_overhead_pct": 231.4,
        "top5_overlap_count": 3,
        "top5_overlap_pct": 60.0,
        "unique_discovery_count": 2,
        "value_rating": "MEDIUM"
      }
    },
    {
      "query": "FAISS vector index management",
      "single_hop": {
        "time_ms": 12.09,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "search\\indexer.py:53-74:function:estimate_index_memory_usage",
            "score": 16.298,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "function",
              "start_line": 53,
              "end_line": 74,
              "name": "estimate_index_memory_usage",
              "parent_name": null,
              "docstring": "Estimate memory usage for FAISS index in bytes.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def estimate_index_memory_usage(\n    num_vectors: int, dimension: int, index_type: str = \"flat\"\n) -> Dict[str, int]:\n    \"\"\"Estimate memory usage for FAISS index in bytes.\"\"\"\n    # Base vector storage (float32 = 4 bytes per element)\n    vector_memory = num_vectors * dimension * 4\n\n    # FAISS overhead depends on index type\n    if index_type.lower() == \"flat\":\n        # Flat index: minimal overhead\n        overhead = vector_memory * 0.1  # ~10% overhead\n    else:\n        # IVF/other indexes: more overhead for centroids, inverted lists\n        overhead = vector_memory * 0.3  # ~30% overhead\n\n    total_memory = int(vector_memory + overhead)\n\n    return {\n        \"vectors\": int(vector_memory),\n        \"overhead\": int(overhead),\n        \"total\": total_memory,\n    }",
              "content_preview": "def estimate_index_memory_usage(\n    num_vectors: int, dimension: int, index_type: str = \"flat\"\n) -> Dict[str, int]:\n    \"\"\"Estimate memory usage for FAISS index in bytes.\"\"\"\n    # Base vector storage...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016234796404019036,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "search\\indexer.py:77-976:class:CodeIndexManager",
            "score": 7.372,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "class",
              "start_line": 77,
              "end_line": 976,
              "name": "CodeIndexManager",
              "parent_name": null,
              "docstring": "Manages FAISS vector index and metadata storage for code chunks.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class CodeIndexManager:\n    \"\"\"Manages FAISS vector index and metadata storage for code chunks.\"\"\"\n\n    def __init__(self, storage_dir: str, embedder=None):\n        self.storage_dir = Path(storage_dir)\n        self.storage_dir.mkdir(parents=True, exist_ok=True)\n\n        # File paths\n        self.index_path = self.storage_dir / \"code.index\"\n        self.metadata_path = self.storage_dir / \"metadata.db\"\n        self.chunk_id_path = self.storage_dir / \"chunk_ids.pkl\"\n        self.stats_path = self.storage_dir / \"stats.json\"\n\n        # Initialize components\n        self._index = None\n        self._metadata_db = None\n        self._chunk_ids = []\n        self._logger = logging.getLogger(__name__)\n        self._on_gpu = False\n        self.embedder = embedder  # Optional embedder for dimension validation\n\n        # Check dependencies\n        self._check_dependencies()\n\n    def _check_dependencies(self):\n        \"\"\"Check if required dependencies are available.\"\"\"\n        if faiss is None:\n            raise ImportError(\n                \"faiss-cpu not found. Install with: pip install faiss-cpu\"\n            )\n\n        if SqliteDict is None:\n            raise ImportError(\n                \"sqlitedict not found. Install with: pip install sqlitedict\"\n            )\n\n    @property\n    def index(self):\n        \"\"\"Lazy loading of FAISS index.\"\"\"\n        if self._index is None:\n            self._load_index()\n        return self._index\n\n    @property\n    def metadata_db(self):\n        \"\"\"Lazy loading of metadata database.\"\"\"\n        if self._metadata_db is None:\n            self._metadata_db = SqliteDict(\n                str(self.metadata_path), autocommit=False, journal_mode=\"WAL\"\n            )\n        return self._metadata_db\n\n    def _load_index(self):\n        \"\"\"Load existing FAISS index or create new one.\"\"\"\n        if self.index_path.exists():\n            self._logger.info(f\"Loading existing index from {self.index_path}\")\n            self._index = faiss.read_index(str(self.index_path))\n\n            # Validate index dimension matches current model (if embedder provided)\n            if self.embedder is not None:\n                try:\n                    stored_dim = self._index.d\n                    current_model_dim = self.embedder.get_model_info()[\n                        \"embedding_dimension\"\n                    ]\n\n                    if stored_dim != current_model_dim:\n                        self._logger.warning(\n                            f\"Index dimension mismatch detected!\\n\"\n                            f\"  Stored index: {stored_dim} dimensions\\n\"\n                            f\"  Current model: {current_model_dim} dimensions\\n\"\n                            f\"  Model: {self.embedder.model_name}\\n\"\n                            f\"This index was created with a different embedding model.\\n\"\n                            f\"Creating new index for current model...\"\n                        )\n                        # Clear the incompatible index\n                        self._index = None\n                        self._chunk_ids = []\n                        return  # Will create new index when embeddings are added\n                except Exception as e:\n                    self._logger.debug(f\"Could not validate index dimension: {e}\")\n\n            # If GPU support is available, optionally move to GPU for runtime speed\n            self._maybe_move_index_to_gpu()\n\n            # Load chunk IDs\n            if self.chunk_id_path.exists():\n                with open(self.chunk_id_path, \"rb\") as f:\n                    self._chunk_ids = pickle.load(f)\n        else:\n            self._logger.info(\"Creating new index\")\n            # Create a new index - we'll initialize it when we get the first embedding\n            self._index = None\n            self._chunk_ids = []\n\n    def create_index(self, embedding_dimension: int, index_type: str = \"flat\"):\n        \"\"\"Create a new FAISS index.\"\"\"\n        if index_type == \"flat\":\n            # Simple flat index for exact search\n            self._index = faiss.IndexFlatIP(\n                embedding_dimension\n            )  # Inner product (cosine similarity)\n        elif index_type == \"ivf\":\n            # IVF index for faster approximate search on large datasets\n            quantizer = faiss.IndexFlatIP(embedding_dimension)\n            n_centroids = min(\n                100, max(10, embedding_dimension // 8)\n            )  # Adaptive number of centroids\n            self._index = faiss.IndexIVFFlat(\n                quantizer, embedding_dimension, n_centroids\n            )\n        else:\n            raise ValueError(f\"Unsupported index type: {index_type}\")\n\n        self._logger.info(\n            f\"Created {index_type} index with dimension {embedding_dimension}\"\n        )\n        self._maybe_move_index_to_gpu()\n\n    def add_embeddings(self, embedding_results: List[EmbeddingResult]) -> None:\n        \"\"\"Add embeddings to the index and metadata to the database.\"\"\"\n        if not embedding_results:\n            return\n\n        # Check memory requirements before proceeding\n        embedding_dim = embedding_results[0].embedding.shape[0]\n        num_new_vectors = len(embedding_results)\n\n        memory_check = self.check_memory_requirements(num_new_vectors, embedding_dim)\n\n        # Abort if insufficient memory to prevent OOM\n        if not memory_check[\"sufficient_memory\"]:\n            raise MemoryError(\n                f\"Insufficient memory to add {num_new_vectors} vectors. \"\n                f\"Need {memory_check['required_memory'] // (1024**2):.1f}MB, \"\n                f\"have {memory_check['available_memory']['gpu_available' if memory_check['prefer_gpu'] else 'system_available'] // (1024**2):.1f}MB. \"\n                f\"Consider indexing in smaller batches or freeing memory.\"\n            )\n\n        # Initialize index if needed\n        if self._index is None:\n            # Default to flat index for better recall - only use IVF for very large datasets\n            index_type = \"ivf\" if num_new_vectors > 10000 else \"flat\"\n            self.create_index(embedding_dim, index_type)\n\n        # Prepare embeddings and metadata\n        embeddings = np.array([result.embedding for result in embedding_results])\n\n        # Normalize embeddings for cosine similarity\n        faiss.normalize_L2(embeddings)\n\n        # Train IVF index if needed\n        if hasattr(self._index, \"is_trained\") and not self._index.is_trained:\n            self._logger.info(\"Training IVF index...\")\n            self._index.train(embeddings)\n\n        # Add to FAISS index\n        start_id = len(self._chunk_ids)\n        self._index.add(embeddings)\n\n        # Store metadata and update chunk IDs\n        for i, result in enumerate(embedding_results):\n            chunk_id = result.chunk_id\n            self._chunk_ids.append(chunk_id)\n\n            # Store in metadata database\n            self.metadata_db[chunk_id] = {\n                \"index_id\": start_id + i,\n                \"metadata\": result.metadata,\n            }\n\n        self._logger.info(f\"Added {len(embedding_results)} embeddings to index\")\n\n        # Commit metadata in a single transaction for performance\n        try:\n            self.metadata_db.commit()\n        except Exception:\n            # If commit is unavailable for some reason, continue without failing\n            pass\n\n        # Update statistics\n        self._update_stats()\n\n    def _gpu_is_available(self) -> bool:\n        \"\"\"Check if GPU FAISS support is available and GPUs are present.\"\"\"\n        try:\n            if not hasattr(faiss, \"StandardGpuResources\"):\n                return False\n            get_num_gpus = getattr(faiss, \"get_num_gpus\", None)\n            if get_num_gpus is None:\n                return False\n            return get_num_gpus() > 0\n        except Exception:\n            return False\n\n    def _maybe_move_index_to_gpu(self) -> None:\n        \"\"\"Move the current index to GPU if supported. No-op if already on GPU or unsupported.\"\"\"\n        if self._index is None or self._on_gpu:\n            return\n        if not self._gpu_is_available():\n            return\n        try:\n            # Move index to all GPUs for faster add/search\n            self._index = faiss.index_cpu_to_all_gpus(self._index)\n            self._on_gpu = True\n            self._logger.info(\"FAISS index moved to GPU(s)\")\n        except Exception as e:\n            self._logger.warning(\n                f\"Failed to move FAISS index to GPU, continuing on CPU: {e}\"\n            )\n\n    def search(\n        self,\n        query_embedding: np.ndarray,\n        k: int = 5,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[Tuple[str, float, Dict[str, Any]]]:\n        \"\"\"Search for similar code chunks.\"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n        logger.info(f\"Index manager search called with k={k}, filters={filters}\")\n\n        # Use property to trigger lazy loading\n        index = self.index\n        if index is None or index.ntotal == 0:\n            logger.warning(\n                f\"Index is empty or None. Index: {index}, ntotal: {index.ntotal if index else 'N/A'}\"\n            )\n            return []\n\n        logger.info(f\"Index has {index.ntotal} total vectors\")\n\n        # Normalize query embedding\n        query_embedding = query_embedding.reshape(1, -1)\n        faiss.normalize_L2(query_embedding)\n\n        # Search in FAISS index\n        search_k = min(k * 3, index.ntotal)  # Get more results for filtering\n        similarities, indices = index.search(query_embedding, search_k)\n\n        results = []\n        for _i, (similarity, index_id) in enumerate(\n            zip(similarities[0], indices[0], strict=False)\n        ):\n            if index_id == -1:  # No more results\n                break\n\n            chunk_id = self._chunk_ids[index_id]\n            metadata_entry = self.metadata_db.get(chunk_id)\n\n            if metadata_entry is None:\n                continue\n\n            metadata = metadata_entry[\"metadata\"]\n\n            # Apply filters\n            if filters and not self._matches_filters(metadata, filters):\n                continue\n\n            results.append((chunk_id, float(similarity), metadata))\n\n            if len(results) >= k:\n                break\n\n        return results\n\n    def _matches_filters(\n        self, metadata: Dict[str, Any], filters: Dict[str, Any]\n    ) -> bool:\n        \"\"\"Check if metadata matches the provided filters.\"\"\"\n        for key, value in filters.items():\n            if key == \"file_pattern\":\n                # Pattern matching for file paths\n                if not any(\n                    pattern in metadata.get(\"relative_path\", \"\") for pattern in value\n                ):\n                    return False\n            elif key == \"chunk_type\":\n                # Exact match for chunk type\n                if metadata.get(\"chunk_type\") != value:\n                    return False\n            elif key == \"tags\":\n                # Tag intersection\n                chunk_tags = set(metadata.get(\"tags\", []))\n                required_tags = set(value if isinstance(value, list) else [value])\n                if not required_tags.intersection(chunk_tags):\n                    return False\n            elif key == \"folder_structure\":\n                # Check if any of the required folders are in the path\n                chunk_folders = set(metadata.get(\"folder_structure\", []))\n                required_folders = set(value if isinstance(value, list) else [value])\n                if not required_folders.intersection(chunk_folders):\n                    return False\n            elif key in metadata:\n                # Direct metadata comparison\n                if metadata[key] != value:\n                    return False\n\n        return True\n\n    def get_chunk_by_id(self, chunk_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Retrieve chunk metadata by ID.\"\"\"\n        metadata_entry = self.metadata_db.get(chunk_id)\n        return metadata_entry[\"metadata\"] if metadata_entry else None\n\n    def get_similar_chunks(\n        self, chunk_id: str, k: int = 5\n    ) -> List[Tuple[str, float, Dict[str, Any]]]:\n        \"\"\"Find chunks similar to a given chunk.\"\"\"\n        metadata_entry = self.metadata_db.get(chunk_id)\n        if not metadata_entry:\n            return []\n\n        index_id = metadata_entry[\"index_id\"]\n        if self._index is None or index_id >= self._index.ntotal:\n            return []\n\n        # Get the embedding for this chunk\n        embedding = self._index.reconstruct(index_id)\n\n        # Search for similar chunks (excluding the original)\n        results = self.search(embedding, k + 1)\n\n        # Filter out the original chunk\n        return [(cid, sim, meta) for cid, sim, meta in results if cid != chunk_id][:k]\n\n    def remove_file_chunks(\n        self, file_path: str, project_name: Optional[str] = None\n    ) -> int:\n        \"\"\"Remove all chunks from a specific file.\n\n        Args:\n            file_path: Path to the file (relative or absolute)\n            project_name: Optional project name filter\n\n        Returns:\n            Number of chunks removed\n        \"\"\"\n        chunks_to_remove = []\n\n        # Find chunks to remove\n        for chunk_id in self._chunk_ids:\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                continue\n\n            metadata = metadata_entry[\"metadata\"]\n\n            # Check if this chunk belongs to the file\n            chunk_file = metadata.get(\"file_path\") or metadata.get(\"relative_path\")\n            if not chunk_file:\n                continue\n\n            # Check if paths match (handle both relative and absolute)\n            if file_path in chunk_file or chunk_file in file_path:\n                # Check project name if provided\n                if project_name and metadata.get(\"project_name\") != project_name:\n                    continue\n                chunks_to_remove.append(chunk_id)\n\n        # Remove chunks from metadata\n        for chunk_id in chunks_to_remove:\n            del self.metadata_db[chunk_id]\n\n        # Note: We don't remove from FAISS index directly as it's complex\n        # Instead, we'll rebuild the index periodically or on demand\n\n        self._logger.info(f\"Removed {len(chunks_to_remove)} chunks from {file_path}\")\n\n        # Commit removals in batch\n        try:\n            self.metadata_db.commit()\n        except Exception:\n            pass\n        return len(chunks_to_remove)\n\n    def remove_multiple_files(\n        self, file_paths: set, project_name: Optional[str] = None\n    ) -> int:\n        \"\"\"Remove chunks from multiple files in a single pass.\n\n        This is much faster than calling remove_file_chunks() repeatedly,\n        as it only scans through all chunks once instead of once per file.\n\n        IMPORTANT: This method properly removes vectors from FAISS index by rebuilding it,\n        which prevents index corruption and access violations.\n\n        Args:\n            file_paths: Set of file paths to remove\n            project_name: Optional project name filter\n\n        Returns:\n            Total number of chunks removed\n        \"\"\"\n        if not file_paths:\n            return 0\n\n        chunks_to_remove_ids = set()\n        chunks_to_remove_positions = []\n\n        # Single pass to identify chunks to remove\n        for position, chunk_id in enumerate(self._chunk_ids):\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                continue\n\n            metadata = metadata_entry[\"metadata\"]\n\n            # Check if this chunk belongs to any of the files\n            chunk_file = metadata.get(\"file_path\") or metadata.get(\"relative_path\")\n            if not chunk_file:\n                continue\n\n            # Check if chunk matches any file in the set\n            for file_path in file_paths:\n                if file_path in chunk_file or chunk_file in file_path:\n                    # Check project name if provided\n                    if project_name and metadata.get(\"project_name\") != project_name:\n                        continue\n                    chunks_to_remove_ids.add(chunk_id)\n                    chunks_to_remove_positions.append(position)\n                    break  # Found match, no need to check other files\n\n        if not chunks_to_remove_ids:\n            self._logger.info(\"No chunks found to remove\")\n            return 0\n\n        self._logger.info(\n            f\"Removing {len(chunks_to_remove_ids)} chunks from {len(file_paths)} files\"\n        )\n\n        try:\n            # Rebuild FAISS index without removed chunks\n            if self._index is not None and self._index.ntotal > 0:\n                # Get positions to keep (all except those being removed)\n                positions_to_remove_set = set(chunks_to_remove_positions)\n                positions_to_keep = [\n                    i for i in range(len(self._chunk_ids))\n                    if i not in positions_to_remove_set\n                ]\n\n                if positions_to_keep:\n                    # Reconstruct embeddings for chunks we want to keep\n                    embeddings_to_keep = []\n                    for pos in positions_to_keep:\n                        try:\n                            embedding = self._index.reconstruct(int(pos))\n                            embeddings_to_keep.append(embedding)\n                        except Exception as e:\n                            self._logger.warning(\n                                f\"Failed to reconstruct embedding at position {pos}: {e}\"\n                            )\n                            continue\n\n                    if embeddings_to_keep:\n                        # Create new index with kept embeddings\n                        embeddings_array = np.array(embeddings_to_keep, dtype=np.float32)\n\n                        # Get embedding dimension\n                        embedding_dim = embeddings_array.shape[1]\n\n                        # Determine index type from current index\n                        index_type = \"flat\"  # Default\n                        if hasattr(self._index, \"metric_type\"):\n                            # Preserve metric type\n                            index_type = \"flat\"\n\n                        # Check if we were on GPU\n                        was_on_gpu = self._on_gpu\n\n                        # Create new CPU index\n                        new_index = faiss.IndexFlatIP(embedding_dim)\n\n                        # Normalize embeddings (we use cosine similarity)\n                        faiss.normalize_L2(embeddings_array)\n\n                        # Add kept embeddings to new index\n                        new_index.add(embeddings_array)\n\n                        # Replace old index\n                        if self._on_gpu:\n                            # Clear GPU memory from old index\n                            del self._index\n                            if torch and torch.cuda.is_available():\n                                torch.cuda.empty_cache()\n\n                        self._index = new_index\n                        self._on_gpu = False\n\n                        # Move to GPU if it was on GPU before\n                        if was_on_gpu:\n                            self._maybe_move_index_to_gpu()\n\n                        self._logger.info(\n                            f\"Rebuilt FAISS index: {self._index.ntotal} vectors \"\n                            f\"(removed {len(chunks_to_remove_ids)})\"\n                        )\n                    else:\n                        # No embeddings to keep, clear the index\n                        self._logger.warning(\n                            \"No valid embeddings to keep, clearing index\"\n                        )\n                        self.clear_index()\n                        return len(chunks_to_remove_ids)\n                else:\n                    # All chunks removed, clear the index\n                    self._logger.info(\"All chunks removed, clearing index\")\n                    self.clear_index()\n                    return len(chunks_to_remove_ids)\n\n            # Update chunk_ids list (remove chunks at removed positions)\n            new_chunk_ids = [\n                chunk_id for i, chunk_id in enumerate(self._chunk_ids)\n                if i not in set(chunks_to_remove_positions)\n            ]\n            self._chunk_ids = new_chunk_ids\n\n            # Remove chunks from metadata and update index_ids\n            for chunk_id in chunks_to_remove_ids:\n                if chunk_id in self.metadata_db:\n                    del self.metadata_db[chunk_id]\n\n            # Update metadata with new index positions\n            for new_pos, chunk_id in enumerate(self._chunk_ids):\n                if chunk_id in self.metadata_db:\n                    metadata_entry = self.metadata_db[chunk_id]\n                    metadata_entry[\"index_id\"] = new_pos\n                    self.metadata_db[chunk_id] = metadata_entry\n\n            # Commit all changes\n            try:\n                self.metadata_db.commit()\n            except Exception as e:\n                self._logger.warning(f\"Failed to commit metadata changes: {e}\")\n\n            self._logger.info(\n                f\"Successfully batch removed {len(chunks_to_remove_ids)} chunks from {len(file_paths)} files\"\n            )\n\n            return len(chunks_to_remove_ids)\n\n        except Exception as e:\n            self._logger.error(f\"Failed to batch remove chunks: {e}\")\n            import traceback\n            self._logger.error(traceback.format_exc())\n            # Don't leave index in corrupted state - if rebuild fails, clear it\n            self._logger.warning(\"Batch removal failed, clearing index to prevent corruption\")\n            self.clear_index()\n            raise\n\n    def save_index(self):\n        \"\"\"Save the FAISS index and chunk IDs to disk.\"\"\"\n        if self._index is not None:\n            try:\n                index_to_write = self._index\n                # If on GPU, convert to CPU before saving\n                if self._on_gpu and hasattr(faiss, \"index_gpu_to_cpu\"):\n                    index_to_write = faiss.index_gpu_to_cpu(self._index)\n                faiss.write_index(index_to_write, str(self.index_path))\n                self._logger.info(f\"Saved index to {self.index_path}\")\n            except Exception as e:\n                self._logger.warning(\n                    f\"Failed to save GPU index directly, attempting CPU fallback: {e}\"\n                )\n                try:\n                    cpu_index = faiss.index_gpu_to_cpu(self._index)\n                    faiss.write_index(cpu_index, str(self.index_path))\n                    self._logger.info(\n                        f\"Saved index to {self.index_path} (CPU fallback)\"\n                    )\n                except Exception as e2:\n                    self._logger.error(f\"Failed to save FAISS index: {e2}\")\n\n        # Save chunk IDs\n        with open(self.chunk_id_path, \"wb\") as f:\n            pickle.dump(self._chunk_ids, f)\n\n        # Save model metadata for dimension validation (if embedder available)\n        if self.embedder is not None:\n            model_info_path = self.index_path.parent / \"model_info.json\"\n            try:\n                import json\n\n                model_info = {\n                    \"model_name\": self.embedder.model_name,\n                    \"embedding_dimension\": self.embedder.get_model_info()[\n                        \"embedding_dimension\"\n                    ],\n                    \"created_at\": (\n                        str(self.index_path.stat().st_mtime)\n                        if self.index_path.exists()\n                        else None\n                    ),\n                }\n                with open(model_info_path, \"w\") as f:\n                    json.dump(model_info, f, indent=2)\n                self._logger.debug(f\"Saved model info to {model_info_path}\")\n            except Exception as e:\n                self._logger.debug(f\"Failed to save model info (non-critical): {e}\")\n\n        self._update_stats()\n\n    def load(self) -> bool:\n        \"\"\"\n        Public method to load index (for compatibility with other index classes).\n\n        Returns:\n            bool: True if index was loaded successfully or already exists, False otherwise\n        \"\"\"\n        # Index is already loaded in __init__, so just check if it exists\n        if self._index is not None and len(self._chunk_ids) > 0:\n            return True\n\n        # Try to reload if index file exists but index is None\n        if self.index_path.exists():\n            try:\n                self._load_index()\n                return self._index is not None\n            except Exception as e:\n                self._logger.error(f\"Failed to reload index: {e}\")\n                return False\n\n        return False\n\n    def _update_stats(self):\n        \"\"\"Update index statistics.\"\"\"\n        stats = {\n            \"total_chunks\": len(self._chunk_ids),\n            \"index_size\": self._index.ntotal if self._index else 0,\n            \"embedding_dimension\": self._index.d if self._index else 0,\n            \"index_type\": type(self._index).__name__ if self._index else \"None\",\n        }\n\n        # Add file and folder statistics\n        file_counts = {}\n        folder_counts = {}\n        chunk_type_counts = {}\n        tag_counts = {}\n\n        for chunk_id in self._chunk_ids:\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                continue\n\n            metadata = metadata_entry[\"metadata\"]\n\n            # Count by file\n            file_path = metadata.get(\"relative_path\", \"unknown\")\n            file_counts[file_path] = file_counts.get(file_path, 0) + 1\n\n            # Count by folder\n            for folder in metadata.get(\"folder_structure\", []):\n                folder_counts[folder] = folder_counts.get(folder, 0) + 1\n\n            # Count by chunk type\n            chunk_type = metadata.get(\"chunk_type\", \"unknown\")\n            chunk_type_counts[chunk_type] = chunk_type_counts.get(chunk_type, 0) + 1\n\n            # Count by tags\n            for tag in metadata.get(\"tags\", []):\n                tag_counts[tag] = tag_counts.get(tag, 0) + 1\n\n        stats.update(\n            {\n                \"files_indexed\": len(file_counts),\n                \"top_folders\": dict(\n                    sorted(folder_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n                ),\n                \"chunk_types\": chunk_type_counts,\n                \"top_tags\": dict(\n                    sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n                ),\n            }\n        )\n\n        # Save stats\n        with open(self.stats_path, \"w\") as f:\n            json.dump(stats, f, indent=2)\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get index statistics.\"\"\"\n        if self.stats_path.exists():\n            with open(self.stats_path, \"r\") as f:\n                return json.load(f)\n        else:\n            return {\n                \"total_chunks\": 0,\n                \"index_size\": 0,\n                \"embedding_dimension\": 0,\n                \"files_indexed\": 0,\n            }\n\n    def get_index_size(self) -> int:\n        \"\"\"Get the number of chunks in the index.\"\"\"\n        return len(self._chunk_ids)\n\n    def validate_index_consistency(self) -> Tuple[bool, List[str]]:\n        \"\"\"Validate consistency between FAISS index, chunk_ids, and metadata.\n\n        This method checks for:\n        1. FAISS index size matches chunk_ids list length\n        2. All chunk_ids have corresponding metadata entries\n        3. All metadata entries have valid index_ids\n        4. No orphaned vectors in FAISS\n\n        Returns:\n            Tuple of (is_valid, list_of_issues)\n        \"\"\"\n        issues = []\n\n        # Check if index exists\n        if self._index is None:\n            if len(self._chunk_ids) > 0:\n                issues.append(\n                    f\"FAISS index is None but chunk_ids list has {len(self._chunk_ids)} entries\"\n                )\n            if len(self.metadata_db) > 0:\n                issues.append(\n                    f\"FAISS index is None but metadata has {len(self.metadata_db)} entries\"\n                )\n            return len(issues) == 0, issues\n\n        # Check 1: FAISS index size matches chunk_ids length\n        faiss_size = self._index.ntotal\n        chunk_ids_size = len(self._chunk_ids)\n        if faiss_size != chunk_ids_size:\n            issues.append(\n                f\"FAISS index size ({faiss_size}) != chunk_ids length ({chunk_ids_size})\"\n            )\n\n        # Check 2: All chunk_ids have metadata entries\n        missing_metadata = []\n        for i, chunk_id in enumerate(self._chunk_ids):\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                missing_metadata.append(f\"{chunk_id} (position {i})\")\n            elif \"index_id\" in metadata_entry:\n                # Check 3: Metadata index_id is valid\n                index_id = metadata_entry[\"index_id\"]\n                if index_id != i:\n                    issues.append(\n                        f\"Chunk {chunk_id} has index_id {index_id} but is at position {i}\"\n                    )\n                if index_id >= faiss_size:\n                    issues.append(\n                        f\"Chunk {chunk_id} has index_id {index_id} >= FAISS size {faiss_size}\"\n                    )\n\n        if missing_metadata:\n            issues.append(\n                f\"Missing metadata for {len(missing_metadata)} chunks: \"\n                f\"{', '.join(missing_metadata[:5])}\"\n                + (f\" ... and {len(missing_metadata) - 5} more\" if len(missing_metadata) > 5 else \"\")\n            )\n\n        # Check 4: Metadata database size consistency\n        metadata_size = len(self.metadata_db)\n        if metadata_size != chunk_ids_size:\n            issues.append(\n                f\"Metadata database size ({metadata_size}) != chunk_ids length ({chunk_ids_size})\"\n            )\n\n        is_valid = len(issues) == 0\n\n        if is_valid:\n            self._logger.info(\n                f\"Index consistency validated: {faiss_size} vectors, \"\n                f\"{chunk_ids_size} chunk IDs, {metadata_size} metadata entries\"\n            )\n        else:\n            self._logger.warning(\n                f\"Index consistency validation failed with {len(issues)} issues\"\n            )\n            for issue in issues:\n                self._logger.warning(f\"  - {issue}\")\n\n        return is_valid, issues\n\n    def clear_index(self):\n        \"\"\"Clear the entire index and metadata.\"\"\"\n        # Close database connection\n        if self._metadata_db is not None:\n            self._metadata_db.close()\n            self._metadata_db = None\n\n        # Remove files\n        for file_path in [\n            self.index_path,\n            self.metadata_path,\n            self.chunk_id_path,\n            self.stats_path,\n        ]:\n            if file_path.exists():\n                file_path.unlink()\n\n        # Reset in-memory state\n        self._index = None\n        self._chunk_ids = []\n\n        self._logger.info(\"Index cleared\")\n\n    def check_memory_requirements(\n        self, num_new_vectors: int, dimension: int\n    ) -> Dict[str, Any]:\n        \"\"\"Check if there's enough memory for adding new vectors.\"\"\"\n        # Get current memory status\n        available = get_available_memory()\n\n        # Estimate memory needed for new vectors\n        estimate_index_memory_usage(num_new_vectors, dimension)\n\n        # Check current index size\n        current_size = self.get_index_size()\n        total_vectors_after = current_size + num_new_vectors\n\n        # Estimate total memory after adding vectors\n        total_estimated = estimate_index_memory_usage(total_vectors_after, dimension)\n\n        # Determine if we should use GPU or CPU\n        prefer_gpu = self._gpu_is_available()\n        target_memory = (\n            available[\"gpu_available\"] if prefer_gpu else available[\"system_available\"]\n        )\n\n        # Safety margin: require 20% more available memory than estimated\n        safety_factor = 1.2\n        required_memory = int(total_estimated[\"total\"] * safety_factor)\n\n        memory_check = {\n            \"available_memory\": available,\n            \"estimated_usage\": total_estimated,\n            \"required_memory\": required_memory,\n            \"current_vectors\": current_size,\n            \"new_vectors\": num_new_vectors,\n            \"total_vectors_after\": total_vectors_after,\n            \"prefer_gpu\": prefer_gpu,\n            \"sufficient_memory\": target_memory >= required_memory,\n            \"memory_utilization\": (\n                required_memory / target_memory if target_memory > 0 else float(\"inf\")\n            ),\n        }\n\n        # Log warning if memory is tight\n        if not memory_check[\"sufficient_memory\"]:\n            self._logger.warning(\n                f\"Insufficient memory: need {required_memory // (1024**2):.1f}MB, \"\n                f\"have {target_memory // (1024**2):.1f}MB \"\n                f\"({'GPU' if prefer_gpu else 'CPU'})\"\n            )\n        elif memory_check[\"memory_utilization\"] > 0.8:\n            self._logger.warning(\n                f\"High memory utilization: {memory_check['memory_utilization']:.1%} \"\n                f\"of available {'GPU' if prefer_gpu else 'CPU'} memory\"\n            )\n\n        return memory_check\n\n    def get_memory_status(self) -> Dict[str, Any]:\n        \"\"\"Get current memory usage status.\"\"\"\n        available = get_available_memory()\n        current_size = self.get_index_size()\n\n        status = {\n            \"available_memory\": available,\n            \"current_index_size\": current_size,\n            \"is_gpu_enabled\": self._on_gpu,\n            \"gpu_available\": self._gpu_is_available(),\n        }\n\n        # Estimate current index memory usage if we have vectors\n        if current_size > 0 and self._index is not None:\n            # Estimate dimension from index if available\n            try:\n                dimension = (\n                    self._index.d if hasattr(self._index, \"d\") else 768\n                )  # Default dimension\n                estimated = estimate_index_memory_usage(current_size, dimension)\n                status[\"estimated_index_memory\"] = estimated\n            except Exception as e:\n                self._logger.debug(f\"Could not estimate index memory usage: {e}\")\n\n        return status\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit - cleanup resources.\"\"\"\n        if self._metadata_db is not None:\n            self._metadata_db.close()\n            self._metadata_db = None\n        return False  # Don't suppress exceptions\n\n    def __del__(self):\n        \"\"\"Cleanup when object is destroyed.\"\"\"\n        if self._metadata_db is not None:\n            self._metadata_db.close()",
              "content_preview": "class CodeIndexManager:\n    \"\"\"Manages FAISS vector index and metadata storage for code chunks.\"\"\"\n\n    def __init__(self, storage_dir: str, embedder=None):\n        self.storage_dir = Path(storage_dir...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015806214827501837,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "search\\indexer.py:775-855:method:validate_index_consistency",
            "score": 9.745,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 775,
              "end_line": 855,
              "name": "validate_index_consistency",
              "parent_name": "CodeIndexManager",
              "docstring": "Validate consistency between FAISS index, chunk_ids, and metadata.\n\n        This method checks for:\n        1. FAISS index size matches chunk_ids list length\n        2. All chunk_ids have corresponding metadata entries\n        3. All metadata entries have valid index_ids\n        4. No orphaned vectors in FAISS\n\n        Returns:\n            Tuple of (is_valid, list_of_issues)",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def validate_index_consistency(self) -> Tuple[bool, List[str]]:\n        \"\"\"Validate consistency between FAISS index, chunk_ids, and metadata.\n\n        This method checks for:\n        1. FAISS index size matches chunk_ids list length\n        2. All chunk_ids have corresponding metadata entries\n        3. All metadata entries have valid index_ids\n        4. No orphaned vectors in FAISS\n\n        Returns:\n            Tuple of (is_valid, list_of_issues)\n        \"\"\"\n        issues = []\n\n        # Check if index exists\n        if self._index is None:\n            if len(self._chunk_ids) > 0:\n                issues.append(\n                    f\"FAISS index is None but chunk_ids list has {len(self._chunk_ids)} entries\"\n                )\n            if len(self.metadata_db) > 0:\n                issues.append(\n                    f\"FAISS index is None but metadata has {len(self.metadata_db)} entries\"\n                )\n            return len(issues) == 0, issues\n\n        # Check 1: FAISS index size matches chunk_ids length\n        faiss_size = self._index.ntotal\n        chunk_ids_size = len(self._chunk_ids)\n        if faiss_size != chunk_ids_size:\n            issues.append(\n                f\"FAISS index size ({faiss_size}) != chunk_ids length ({chunk_ids_size})\"\n            )\n\n        # Check 2: All chunk_ids have metadata entries\n        missing_metadata = []\n        for i, chunk_id in enumerate(self._chunk_ids):\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                missing_metadata.append(f\"{chunk_id} (position {i})\")\n            elif \"index_id\" in metadata_entry:\n                # Check 3: Metadata index_id is valid\n                index_id = metadata_entry[\"index_id\"]\n                if index_id != i:\n                    issues.append(\n                        f\"Chunk {chunk_id} has index_id {index_id} but is at position {i}\"\n                    )\n                if index_id >= faiss_size:\n                    issues.append(\n                        f\"Chunk {chunk_id} has index_id {index_id} >= FAISS size {faiss_size}\"\n                    )\n\n        if missing_metadata:\n            issues.append(\n                f\"Missing metadata for {len(missing_metadata)} chunks: \"\n                f\"{', '.join(missing_metadata[:5])}\"\n                + (f\" ... and {len(missing_metadata) - 5} more\" if len(missing_metadata) > 5 else \"\")\n            )\n\n        # Check 4: Metadata database size consistency\n        metadata_size = len(self.metadata_db)\n        if metadata_size != chunk_ids_size:\n            issues.append(\n                f\"Metadata database size ({metadata_size}) != chunk_ids length ({chunk_ids_size})\"\n            )\n\n        is_valid = len(issues) == 0\n\n        if is_valid:\n            self._logger.info(\n                f\"Index consistency validated: {faiss_size} vectors, \"\n                f\"{chunk_ids_size} chunk IDs, {metadata_size} metadata entries\"\n            )\n        else:\n            self._logger.warning(\n                f\"Index consistency validation failed with {len(issues)} issues\"\n            )\n            for issue in issues:\n                self._logger.warning(f\"  - {issue}\")\n\n        return is_valid, issues",
              "content_preview": "def validate_index_consistency(self) -> Tuple[bool, List[str]]:\n        \"\"\"Validate consistency between FAISS index, chunk_ids, and metadata.\n\n        This method checks for:\n        1. FAISS index si...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01572420634920635,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "search\\indexer.py:113-118:decorated_definition:index",
            "score": 8.588,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 113,
              "end_line": 118,
              "name": "index",
              "parent_name": "CodeIndexManager",
              "docstring": "Lazy loading of FAISS index.",
              "decorators": [
                "@property"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@property\n    def index(self):\n        \"\"\"Lazy loading of FAISS index.\"\"\"\n        if self._index is None:\n            self._load_index()\n        return self._index",
              "content_preview": "@property\n    def index(self):\n        \"\"\"Lazy loading of FAISS index.\"\"\"\n        if self._index is None:\n            self._load_index()\n        return self._index",
              "project_name": "claude-context-local",
              "rrf_score": 0.015205223880597014,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "search\\indexer.py:172-194:method:create_index",
            "score": 6.75,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 172,
              "end_line": 194,
              "name": "create_index",
              "parent_name": "CodeIndexManager",
              "docstring": "Create a new FAISS index.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def create_index(self, embedding_dimension: int, index_type: str = \"flat\"):\n        \"\"\"Create a new FAISS index.\"\"\"\n        if index_type == \"flat\":\n            # Simple flat index for exact search\n            self._index = faiss.IndexFlatIP(\n                embedding_dimension\n            )  # Inner product (cosine similarity)\n        elif index_type == \"ivf\":\n            # IVF index for faster approximate search on large datasets\n            quantizer = faiss.IndexFlatIP(embedding_dimension)\n            n_centroids = min(\n                100, max(10, embedding_dimension // 8)\n            )  # Adaptive number of centroids\n            self._index = faiss.IndexIVFFlat(\n                quantizer, embedding_dimension, n_centroids\n            )\n        else:\n            raise ValueError(f\"Unsupported index type: {index_type}\")\n\n        self._logger.info(\n            f\"Created {index_type} index with dimension {embedding_dimension}\"\n        )\n        self._maybe_move_index_to_gpu()",
              "content_preview": "def create_index(self, embedding_dimension: int, index_type: str = \"flat\"):\n        \"\"\"Create a new FAISS index.\"\"\"\n        if index_type == \"flat\":\n            # Simple flat index for exact search\n  ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015157612340710933,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "search\\hybrid_searcher.py:1139-1208:method:remove_multiple_files",
          "search\\indexer.py:172-194:method:create_index",
          "search\\indexer.py:113-118:decorated_definition:index",
          "search\\indexer.py:775-855:method:validate_index_consistency",
          "search\\indexer.py:272-286:method:_maybe_move_index_to_gpu",
          "search\\indexer.py:629-679:method:save_index",
          "search\\indexer.py:129-170:method:_load_index",
          "search\\indexer.py:53-74:function:estimate_index_memory_usage",
          "search\\indexer.py:77-976:class:CodeIndexManager",
          "scripts\\__init__.py:1-2:module"
        ]
      },
      "multi_hop": {
        "time_ms": 55.97,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "search\\indexer.py:53-74:function:estimate_index_memory_usage",
            "score": 16.298,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "function",
              "start_line": 53,
              "end_line": 74,
              "name": "estimate_index_memory_usage",
              "parent_name": null,
              "docstring": "Estimate memory usage for FAISS index in bytes.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def estimate_index_memory_usage(\n    num_vectors: int, dimension: int, index_type: str = \"flat\"\n) -> Dict[str, int]:\n    \"\"\"Estimate memory usage for FAISS index in bytes.\"\"\"\n    # Base vector storage (float32 = 4 bytes per element)\n    vector_memory = num_vectors * dimension * 4\n\n    # FAISS overhead depends on index type\n    if index_type.lower() == \"flat\":\n        # Flat index: minimal overhead\n        overhead = vector_memory * 0.1  # ~10% overhead\n    else:\n        # IVF/other indexes: more overhead for centroids, inverted lists\n        overhead = vector_memory * 0.3  # ~30% overhead\n\n    total_memory = int(vector_memory + overhead)\n\n    return {\n        \"vectors\": int(vector_memory),\n        \"overhead\": int(overhead),\n        \"total\": total_memory,\n    }",
              "content_preview": "def estimate_index_memory_usage(\n    num_vectors: int, dimension: int, index_type: str = \"flat\"\n) -> Dict[str, int]:\n    \"\"\"Estimate memory usage for FAISS index in bytes.\"\"\"\n    # Base vector storage...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016234796404019036,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "search\\indexer.py:775-855:method:validate_index_consistency",
            "score": 9.745,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 775,
              "end_line": 855,
              "name": "validate_index_consistency",
              "parent_name": "CodeIndexManager",
              "docstring": "Validate consistency between FAISS index, chunk_ids, and metadata.\n\n        This method checks for:\n        1. FAISS index size matches chunk_ids list length\n        2. All chunk_ids have corresponding metadata entries\n        3. All metadata entries have valid index_ids\n        4. No orphaned vectors in FAISS\n\n        Returns:\n            Tuple of (is_valid, list_of_issues)",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def validate_index_consistency(self) -> Tuple[bool, List[str]]:\n        \"\"\"Validate consistency between FAISS index, chunk_ids, and metadata.\n\n        This method checks for:\n        1. FAISS index size matches chunk_ids list length\n        2. All chunk_ids have corresponding metadata entries\n        3. All metadata entries have valid index_ids\n        4. No orphaned vectors in FAISS\n\n        Returns:\n            Tuple of (is_valid, list_of_issues)\n        \"\"\"\n        issues = []\n\n        # Check if index exists\n        if self._index is None:\n            if len(self._chunk_ids) > 0:\n                issues.append(\n                    f\"FAISS index is None but chunk_ids list has {len(self._chunk_ids)} entries\"\n                )\n            if len(self.metadata_db) > 0:\n                issues.append(\n                    f\"FAISS index is None but metadata has {len(self.metadata_db)} entries\"\n                )\n            return len(issues) == 0, issues\n\n        # Check 1: FAISS index size matches chunk_ids length\n        faiss_size = self._index.ntotal\n        chunk_ids_size = len(self._chunk_ids)\n        if faiss_size != chunk_ids_size:\n            issues.append(\n                f\"FAISS index size ({faiss_size}) != chunk_ids length ({chunk_ids_size})\"\n            )\n\n        # Check 2: All chunk_ids have metadata entries\n        missing_metadata = []\n        for i, chunk_id in enumerate(self._chunk_ids):\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                missing_metadata.append(f\"{chunk_id} (position {i})\")\n            elif \"index_id\" in metadata_entry:\n                # Check 3: Metadata index_id is valid\n                index_id = metadata_entry[\"index_id\"]\n                if index_id != i:\n                    issues.append(\n                        f\"Chunk {chunk_id} has index_id {index_id} but is at position {i}\"\n                    )\n                if index_id >= faiss_size:\n                    issues.append(\n                        f\"Chunk {chunk_id} has index_id {index_id} >= FAISS size {faiss_size}\"\n                    )\n\n        if missing_metadata:\n            issues.append(\n                f\"Missing metadata for {len(missing_metadata)} chunks: \"\n                f\"{', '.join(missing_metadata[:5])}\"\n                + (f\" ... and {len(missing_metadata) - 5} more\" if len(missing_metadata) > 5 else \"\")\n            )\n\n        # Check 4: Metadata database size consistency\n        metadata_size = len(self.metadata_db)\n        if metadata_size != chunk_ids_size:\n            issues.append(\n                f\"Metadata database size ({metadata_size}) != chunk_ids length ({chunk_ids_size})\"\n            )\n\n        is_valid = len(issues) == 0\n\n        if is_valid:\n            self._logger.info(\n                f\"Index consistency validated: {faiss_size} vectors, \"\n                f\"{chunk_ids_size} chunk IDs, {metadata_size} metadata entries\"\n            )\n        else:\n            self._logger.warning(\n                f\"Index consistency validation failed with {len(issues)} issues\"\n            )\n            for issue in issues:\n                self._logger.warning(f\"  - {issue}\")\n\n        return is_valid, issues",
              "content_preview": "def validate_index_consistency(self) -> Tuple[bool, List[str]]:\n        \"\"\"Validate consistency between FAISS index, chunk_ids, and metadata.\n\n        This method checks for:\n        1. FAISS index si...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01572420634920635,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "search\\indexer.py:113-118:decorated_definition:index",
            "score": 8.588,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 113,
              "end_line": 118,
              "name": "index",
              "parent_name": "CodeIndexManager",
              "docstring": "Lazy loading of FAISS index.",
              "decorators": [
                "@property"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@property\n    def index(self):\n        \"\"\"Lazy loading of FAISS index.\"\"\"\n        if self._index is None:\n            self._load_index()\n        return self._index",
              "content_preview": "@property\n    def index(self):\n        \"\"\"Lazy loading of FAISS index.\"\"\"\n        if self._index is None:\n            self._load_index()\n        return self._index",
              "project_name": "claude-context-local",
              "rrf_score": 0.015205223880597014,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "scripts\\__init__.py:1-2:module",
            "score": 8.427,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\__init__.py",
              "relative_path": "scripts\\__init__.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "module",
              "start_line": 1,
              "end_line": 2,
              "name": null,
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "\"\"\"Scripts for code indexing and management.\"\"\"\n",
              "content_preview": "\"\"\"Scripts for code indexing and management.\"\"\"\n",
              "project_name": "claude-context-local",
              "rrf_score": 0.014604550379198267,
              "appears_in_lists": 2,
              "final_rank": 7
            }
          },
          {
            "doc_id": "search\\indexer.py:77-976:class:CodeIndexManager",
            "score": 7.372,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "class",
              "start_line": 77,
              "end_line": 976,
              "name": "CodeIndexManager",
              "parent_name": null,
              "docstring": "Manages FAISS vector index and metadata storage for code chunks.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class CodeIndexManager:\n    \"\"\"Manages FAISS vector index and metadata storage for code chunks.\"\"\"\n\n    def __init__(self, storage_dir: str, embedder=None):\n        self.storage_dir = Path(storage_dir)\n        self.storage_dir.mkdir(parents=True, exist_ok=True)\n\n        # File paths\n        self.index_path = self.storage_dir / \"code.index\"\n        self.metadata_path = self.storage_dir / \"metadata.db\"\n        self.chunk_id_path = self.storage_dir / \"chunk_ids.pkl\"\n        self.stats_path = self.storage_dir / \"stats.json\"\n\n        # Initialize components\n        self._index = None\n        self._metadata_db = None\n        self._chunk_ids = []\n        self._logger = logging.getLogger(__name__)\n        self._on_gpu = False\n        self.embedder = embedder  # Optional embedder for dimension validation\n\n        # Check dependencies\n        self._check_dependencies()\n\n    def _check_dependencies(self):\n        \"\"\"Check if required dependencies are available.\"\"\"\n        if faiss is None:\n            raise ImportError(\n                \"faiss-cpu not found. Install with: pip install faiss-cpu\"\n            )\n\n        if SqliteDict is None:\n            raise ImportError(\n                \"sqlitedict not found. Install with: pip install sqlitedict\"\n            )\n\n    @property\n    def index(self):\n        \"\"\"Lazy loading of FAISS index.\"\"\"\n        if self._index is None:\n            self._load_index()\n        return self._index\n\n    @property\n    def metadata_db(self):\n        \"\"\"Lazy loading of metadata database.\"\"\"\n        if self._metadata_db is None:\n            self._metadata_db = SqliteDict(\n                str(self.metadata_path), autocommit=False, journal_mode=\"WAL\"\n            )\n        return self._metadata_db\n\n    def _load_index(self):\n        \"\"\"Load existing FAISS index or create new one.\"\"\"\n        if self.index_path.exists():\n            self._logger.info(f\"Loading existing index from {self.index_path}\")\n            self._index = faiss.read_index(str(self.index_path))\n\n            # Validate index dimension matches current model (if embedder provided)\n            if self.embedder is not None:\n                try:\n                    stored_dim = self._index.d\n                    current_model_dim = self.embedder.get_model_info()[\n                        \"embedding_dimension\"\n                    ]\n\n                    if stored_dim != current_model_dim:\n                        self._logger.warning(\n                            f\"Index dimension mismatch detected!\\n\"\n                            f\"  Stored index: {stored_dim} dimensions\\n\"\n                            f\"  Current model: {current_model_dim} dimensions\\n\"\n                            f\"  Model: {self.embedder.model_name}\\n\"\n                            f\"This index was created with a different embedding model.\\n\"\n                            f\"Creating new index for current model...\"\n                        )\n                        # Clear the incompatible index\n                        self._index = None\n                        self._chunk_ids = []\n                        return  # Will create new index when embeddings are added\n                except Exception as e:\n                    self._logger.debug(f\"Could not validate index dimension: {e}\")\n\n            # If GPU support is available, optionally move to GPU for runtime speed\n            self._maybe_move_index_to_gpu()\n\n            # Load chunk IDs\n            if self.chunk_id_path.exists():\n                with open(self.chunk_id_path, \"rb\") as f:\n                    self._chunk_ids = pickle.load(f)\n        else:\n            self._logger.info(\"Creating new index\")\n            # Create a new index - we'll initialize it when we get the first embedding\n            self._index = None\n            self._chunk_ids = []\n\n    def create_index(self, embedding_dimension: int, index_type: str = \"flat\"):\n        \"\"\"Create a new FAISS index.\"\"\"\n        if index_type == \"flat\":\n            # Simple flat index for exact search\n            self._index = faiss.IndexFlatIP(\n                embedding_dimension\n            )  # Inner product (cosine similarity)\n        elif index_type == \"ivf\":\n            # IVF index for faster approximate search on large datasets\n            quantizer = faiss.IndexFlatIP(embedding_dimension)\n            n_centroids = min(\n                100, max(10, embedding_dimension // 8)\n            )  # Adaptive number of centroids\n            self._index = faiss.IndexIVFFlat(\n                quantizer, embedding_dimension, n_centroids\n            )\n        else:\n            raise ValueError(f\"Unsupported index type: {index_type}\")\n\n        self._logger.info(\n            f\"Created {index_type} index with dimension {embedding_dimension}\"\n        )\n        self._maybe_move_index_to_gpu()\n\n    def add_embeddings(self, embedding_results: List[EmbeddingResult]) -> None:\n        \"\"\"Add embeddings to the index and metadata to the database.\"\"\"\n        if not embedding_results:\n            return\n\n        # Check memory requirements before proceeding\n        embedding_dim = embedding_results[0].embedding.shape[0]\n        num_new_vectors = len(embedding_results)\n\n        memory_check = self.check_memory_requirements(num_new_vectors, embedding_dim)\n\n        # Abort if insufficient memory to prevent OOM\n        if not memory_check[\"sufficient_memory\"]:\n            raise MemoryError(\n                f\"Insufficient memory to add {num_new_vectors} vectors. \"\n                f\"Need {memory_check['required_memory'] // (1024**2):.1f}MB, \"\n                f\"have {memory_check['available_memory']['gpu_available' if memory_check['prefer_gpu'] else 'system_available'] // (1024**2):.1f}MB. \"\n                f\"Consider indexing in smaller batches or freeing memory.\"\n            )\n\n        # Initialize index if needed\n        if self._index is None:\n            # Default to flat index for better recall - only use IVF for very large datasets\n            index_type = \"ivf\" if num_new_vectors > 10000 else \"flat\"\n            self.create_index(embedding_dim, index_type)\n\n        # Prepare embeddings and metadata\n        embeddings = np.array([result.embedding for result in embedding_results])\n\n        # Normalize embeddings for cosine similarity\n        faiss.normalize_L2(embeddings)\n\n        # Train IVF index if needed\n        if hasattr(self._index, \"is_trained\") and not self._index.is_trained:\n            self._logger.info(\"Training IVF index...\")\n            self._index.train(embeddings)\n\n        # Add to FAISS index\n        start_id = len(self._chunk_ids)\n        self._index.add(embeddings)\n\n        # Store metadata and update chunk IDs\n        for i, result in enumerate(embedding_results):\n            chunk_id = result.chunk_id\n            self._chunk_ids.append(chunk_id)\n\n            # Store in metadata database\n            self.metadata_db[chunk_id] = {\n                \"index_id\": start_id + i,\n                \"metadata\": result.metadata,\n            }\n\n        self._logger.info(f\"Added {len(embedding_results)} embeddings to index\")\n\n        # Commit metadata in a single transaction for performance\n        try:\n            self.metadata_db.commit()\n        except Exception:\n            # If commit is unavailable for some reason, continue without failing\n            pass\n\n        # Update statistics\n        self._update_stats()\n\n    def _gpu_is_available(self) -> bool:\n        \"\"\"Check if GPU FAISS support is available and GPUs are present.\"\"\"\n        try:\n            if not hasattr(faiss, \"StandardGpuResources\"):\n                return False\n            get_num_gpus = getattr(faiss, \"get_num_gpus\", None)\n            if get_num_gpus is None:\n                return False\n            return get_num_gpus() > 0\n        except Exception:\n            return False\n\n    def _maybe_move_index_to_gpu(self) -> None:\n        \"\"\"Move the current index to GPU if supported. No-op if already on GPU or unsupported.\"\"\"\n        if self._index is None or self._on_gpu:\n            return\n        if not self._gpu_is_available():\n            return\n        try:\n            # Move index to all GPUs for faster add/search\n            self._index = faiss.index_cpu_to_all_gpus(self._index)\n            self._on_gpu = True\n            self._logger.info(\"FAISS index moved to GPU(s)\")\n        except Exception as e:\n            self._logger.warning(\n                f\"Failed to move FAISS index to GPU, continuing on CPU: {e}\"\n            )\n\n    def search(\n        self,\n        query_embedding: np.ndarray,\n        k: int = 5,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[Tuple[str, float, Dict[str, Any]]]:\n        \"\"\"Search for similar code chunks.\"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n        logger.info(f\"Index manager search called with k={k}, filters={filters}\")\n\n        # Use property to trigger lazy loading\n        index = self.index\n        if index is None or index.ntotal == 0:\n            logger.warning(\n                f\"Index is empty or None. Index: {index}, ntotal: {index.ntotal if index else 'N/A'}\"\n            )\n            return []\n\n        logger.info(f\"Index has {index.ntotal} total vectors\")\n\n        # Normalize query embedding\n        query_embedding = query_embedding.reshape(1, -1)\n        faiss.normalize_L2(query_embedding)\n\n        # Search in FAISS index\n        search_k = min(k * 3, index.ntotal)  # Get more results for filtering\n        similarities, indices = index.search(query_embedding, search_k)\n\n        results = []\n        for _i, (similarity, index_id) in enumerate(\n            zip(similarities[0], indices[0], strict=False)\n        ):\n            if index_id == -1:  # No more results\n                break\n\n            chunk_id = self._chunk_ids[index_id]\n            metadata_entry = self.metadata_db.get(chunk_id)\n\n            if metadata_entry is None:\n                continue\n\n            metadata = metadata_entry[\"metadata\"]\n\n            # Apply filters\n            if filters and not self._matches_filters(metadata, filters):\n                continue\n\n            results.append((chunk_id, float(similarity), metadata))\n\n            if len(results) >= k:\n                break\n\n        return results\n\n    def _matches_filters(\n        self, metadata: Dict[str, Any], filters: Dict[str, Any]\n    ) -> bool:\n        \"\"\"Check if metadata matches the provided filters.\"\"\"\n        for key, value in filters.items():\n            if key == \"file_pattern\":\n                # Pattern matching for file paths\n                if not any(\n                    pattern in metadata.get(\"relative_path\", \"\") for pattern in value\n                ):\n                    return False\n            elif key == \"chunk_type\":\n                # Exact match for chunk type\n                if metadata.get(\"chunk_type\") != value:\n                    return False\n            elif key == \"tags\":\n                # Tag intersection\n                chunk_tags = set(metadata.get(\"tags\", []))\n                required_tags = set(value if isinstance(value, list) else [value])\n                if not required_tags.intersection(chunk_tags):\n                    return False\n            elif key == \"folder_structure\":\n                # Check if any of the required folders are in the path\n                chunk_folders = set(metadata.get(\"folder_structure\", []))\n                required_folders = set(value if isinstance(value, list) else [value])\n                if not required_folders.intersection(chunk_folders):\n                    return False\n            elif key in metadata:\n                # Direct metadata comparison\n                if metadata[key] != value:\n                    return False\n\n        return True\n\n    def get_chunk_by_id(self, chunk_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Retrieve chunk metadata by ID.\"\"\"\n        metadata_entry = self.metadata_db.get(chunk_id)\n        return metadata_entry[\"metadata\"] if metadata_entry else None\n\n    def get_similar_chunks(\n        self, chunk_id: str, k: int = 5\n    ) -> List[Tuple[str, float, Dict[str, Any]]]:\n        \"\"\"Find chunks similar to a given chunk.\"\"\"\n        metadata_entry = self.metadata_db.get(chunk_id)\n        if not metadata_entry:\n            return []\n\n        index_id = metadata_entry[\"index_id\"]\n        if self._index is None or index_id >= self._index.ntotal:\n            return []\n\n        # Get the embedding for this chunk\n        embedding = self._index.reconstruct(index_id)\n\n        # Search for similar chunks (excluding the original)\n        results = self.search(embedding, k + 1)\n\n        # Filter out the original chunk\n        return [(cid, sim, meta) for cid, sim, meta in results if cid != chunk_id][:k]\n\n    def remove_file_chunks(\n        self, file_path: str, project_name: Optional[str] = None\n    ) -> int:\n        \"\"\"Remove all chunks from a specific file.\n\n        Args:\n            file_path: Path to the file (relative or absolute)\n            project_name: Optional project name filter\n\n        Returns:\n            Number of chunks removed\n        \"\"\"\n        chunks_to_remove = []\n\n        # Find chunks to remove\n        for chunk_id in self._chunk_ids:\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                continue\n\n            metadata = metadata_entry[\"metadata\"]\n\n            # Check if this chunk belongs to the file\n            chunk_file = metadata.get(\"file_path\") or metadata.get(\"relative_path\")\n            if not chunk_file:\n                continue\n\n            # Check if paths match (handle both relative and absolute)\n            if file_path in chunk_file or chunk_file in file_path:\n                # Check project name if provided\n                if project_name and metadata.get(\"project_name\") != project_name:\n                    continue\n                chunks_to_remove.append(chunk_id)\n\n        # Remove chunks from metadata\n        for chunk_id in chunks_to_remove:\n            del self.metadata_db[chunk_id]\n\n        # Note: We don't remove from FAISS index directly as it's complex\n        # Instead, we'll rebuild the index periodically or on demand\n\n        self._logger.info(f\"Removed {len(chunks_to_remove)} chunks from {file_path}\")\n\n        # Commit removals in batch\n        try:\n            self.metadata_db.commit()\n        except Exception:\n            pass\n        return len(chunks_to_remove)\n\n    def remove_multiple_files(\n        self, file_paths: set, project_name: Optional[str] = None\n    ) -> int:\n        \"\"\"Remove chunks from multiple files in a single pass.\n\n        This is much faster than calling remove_file_chunks() repeatedly,\n        as it only scans through all chunks once instead of once per file.\n\n        IMPORTANT: This method properly removes vectors from FAISS index by rebuilding it,\n        which prevents index corruption and access violations.\n\n        Args:\n            file_paths: Set of file paths to remove\n            project_name: Optional project name filter\n\n        Returns:\n            Total number of chunks removed\n        \"\"\"\n        if not file_paths:\n            return 0\n\n        chunks_to_remove_ids = set()\n        chunks_to_remove_positions = []\n\n        # Single pass to identify chunks to remove\n        for position, chunk_id in enumerate(self._chunk_ids):\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                continue\n\n            metadata = metadata_entry[\"metadata\"]\n\n            # Check if this chunk belongs to any of the files\n            chunk_file = metadata.get(\"file_path\") or metadata.get(\"relative_path\")\n            if not chunk_file:\n                continue\n\n            # Check if chunk matches any file in the set\n            for file_path in file_paths:\n                if file_path in chunk_file or chunk_file in file_path:\n                    # Check project name if provided\n                    if project_name and metadata.get(\"project_name\") != project_name:\n                        continue\n                    chunks_to_remove_ids.add(chunk_id)\n                    chunks_to_remove_positions.append(position)\n                    break  # Found match, no need to check other files\n\n        if not chunks_to_remove_ids:\n            self._logger.info(\"No chunks found to remove\")\n            return 0\n\n        self._logger.info(\n            f\"Removing {len(chunks_to_remove_ids)} chunks from {len(file_paths)} files\"\n        )\n\n        try:\n            # Rebuild FAISS index without removed chunks\n            if self._index is not None and self._index.ntotal > 0:\n                # Get positions to keep (all except those being removed)\n                positions_to_remove_set = set(chunks_to_remove_positions)\n                positions_to_keep = [\n                    i for i in range(len(self._chunk_ids))\n                    if i not in positions_to_remove_set\n                ]\n\n                if positions_to_keep:\n                    # Reconstruct embeddings for chunks we want to keep\n                    embeddings_to_keep = []\n                    for pos in positions_to_keep:\n                        try:\n                            embedding = self._index.reconstruct(int(pos))\n                            embeddings_to_keep.append(embedding)\n                        except Exception as e:\n                            self._logger.warning(\n                                f\"Failed to reconstruct embedding at position {pos}: {e}\"\n                            )\n                            continue\n\n                    if embeddings_to_keep:\n                        # Create new index with kept embeddings\n                        embeddings_array = np.array(embeddings_to_keep, dtype=np.float32)\n\n                        # Get embedding dimension\n                        embedding_dim = embeddings_array.shape[1]\n\n                        # Determine index type from current index\n                        index_type = \"flat\"  # Default\n                        if hasattr(self._index, \"metric_type\"):\n                            # Preserve metric type\n                            index_type = \"flat\"\n\n                        # Check if we were on GPU\n                        was_on_gpu = self._on_gpu\n\n                        # Create new CPU index\n                        new_index = faiss.IndexFlatIP(embedding_dim)\n\n                        # Normalize embeddings (we use cosine similarity)\n                        faiss.normalize_L2(embeddings_array)\n\n                        # Add kept embeddings to new index\n                        new_index.add(embeddings_array)\n\n                        # Replace old index\n                        if self._on_gpu:\n                            # Clear GPU memory from old index\n                            del self._index\n                            if torch and torch.cuda.is_available():\n                                torch.cuda.empty_cache()\n\n                        self._index = new_index\n                        self._on_gpu = False\n\n                        # Move to GPU if it was on GPU before\n                        if was_on_gpu:\n                            self._maybe_move_index_to_gpu()\n\n                        self._logger.info(\n                            f\"Rebuilt FAISS index: {self._index.ntotal} vectors \"\n                            f\"(removed {len(chunks_to_remove_ids)})\"\n                        )\n                    else:\n                        # No embeddings to keep, clear the index\n                        self._logger.warning(\n                            \"No valid embeddings to keep, clearing index\"\n                        )\n                        self.clear_index()\n                        return len(chunks_to_remove_ids)\n                else:\n                    # All chunks removed, clear the index\n                    self._logger.info(\"All chunks removed, clearing index\")\n                    self.clear_index()\n                    return len(chunks_to_remove_ids)\n\n            # Update chunk_ids list (remove chunks at removed positions)\n            new_chunk_ids = [\n                chunk_id for i, chunk_id in enumerate(self._chunk_ids)\n                if i not in set(chunks_to_remove_positions)\n            ]\n            self._chunk_ids = new_chunk_ids\n\n            # Remove chunks from metadata and update index_ids\n            for chunk_id in chunks_to_remove_ids:\n                if chunk_id in self.metadata_db:\n                    del self.metadata_db[chunk_id]\n\n            # Update metadata with new index positions\n            for new_pos, chunk_id in enumerate(self._chunk_ids):\n                if chunk_id in self.metadata_db:\n                    metadata_entry = self.metadata_db[chunk_id]\n                    metadata_entry[\"index_id\"] = new_pos\n                    self.metadata_db[chunk_id] = metadata_entry\n\n            # Commit all changes\n            try:\n                self.metadata_db.commit()\n            except Exception as e:\n                self._logger.warning(f\"Failed to commit metadata changes: {e}\")\n\n            self._logger.info(\n                f\"Successfully batch removed {len(chunks_to_remove_ids)} chunks from {len(file_paths)} files\"\n            )\n\n            return len(chunks_to_remove_ids)\n\n        except Exception as e:\n            self._logger.error(f\"Failed to batch remove chunks: {e}\")\n            import traceback\n            self._logger.error(traceback.format_exc())\n            # Don't leave index in corrupted state - if rebuild fails, clear it\n            self._logger.warning(\"Batch removal failed, clearing index to prevent corruption\")\n            self.clear_index()\n            raise\n\n    def save_index(self):\n        \"\"\"Save the FAISS index and chunk IDs to disk.\"\"\"\n        if self._index is not None:\n            try:\n                index_to_write = self._index\n                # If on GPU, convert to CPU before saving\n                if self._on_gpu and hasattr(faiss, \"index_gpu_to_cpu\"):\n                    index_to_write = faiss.index_gpu_to_cpu(self._index)\n                faiss.write_index(index_to_write, str(self.index_path))\n                self._logger.info(f\"Saved index to {self.index_path}\")\n            except Exception as e:\n                self._logger.warning(\n                    f\"Failed to save GPU index directly, attempting CPU fallback: {e}\"\n                )\n                try:\n                    cpu_index = faiss.index_gpu_to_cpu(self._index)\n                    faiss.write_index(cpu_index, str(self.index_path))\n                    self._logger.info(\n                        f\"Saved index to {self.index_path} (CPU fallback)\"\n                    )\n                except Exception as e2:\n                    self._logger.error(f\"Failed to save FAISS index: {e2}\")\n\n        # Save chunk IDs\n        with open(self.chunk_id_path, \"wb\") as f:\n            pickle.dump(self._chunk_ids, f)\n\n        # Save model metadata for dimension validation (if embedder available)\n        if self.embedder is not None:\n            model_info_path = self.index_path.parent / \"model_info.json\"\n            try:\n                import json\n\n                model_info = {\n                    \"model_name\": self.embedder.model_name,\n                    \"embedding_dimension\": self.embedder.get_model_info()[\n                        \"embedding_dimension\"\n                    ],\n                    \"created_at\": (\n                        str(self.index_path.stat().st_mtime)\n                        if self.index_path.exists()\n                        else None\n                    ),\n                }\n                with open(model_info_path, \"w\") as f:\n                    json.dump(model_info, f, indent=2)\n                self._logger.debug(f\"Saved model info to {model_info_path}\")\n            except Exception as e:\n                self._logger.debug(f\"Failed to save model info (non-critical): {e}\")\n\n        self._update_stats()\n\n    def load(self) -> bool:\n        \"\"\"\n        Public method to load index (for compatibility with other index classes).\n\n        Returns:\n            bool: True if index was loaded successfully or already exists, False otherwise\n        \"\"\"\n        # Index is already loaded in __init__, so just check if it exists\n        if self._index is not None and len(self._chunk_ids) > 0:\n            return True\n\n        # Try to reload if index file exists but index is None\n        if self.index_path.exists():\n            try:\n                self._load_index()\n                return self._index is not None\n            except Exception as e:\n                self._logger.error(f\"Failed to reload index: {e}\")\n                return False\n\n        return False\n\n    def _update_stats(self):\n        \"\"\"Update index statistics.\"\"\"\n        stats = {\n            \"total_chunks\": len(self._chunk_ids),\n            \"index_size\": self._index.ntotal if self._index else 0,\n            \"embedding_dimension\": self._index.d if self._index else 0,\n            \"index_type\": type(self._index).__name__ if self._index else \"None\",\n        }\n\n        # Add file and folder statistics\n        file_counts = {}\n        folder_counts = {}\n        chunk_type_counts = {}\n        tag_counts = {}\n\n        for chunk_id in self._chunk_ids:\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                continue\n\n            metadata = metadata_entry[\"metadata\"]\n\n            # Count by file\n            file_path = metadata.get(\"relative_path\", \"unknown\")\n            file_counts[file_path] = file_counts.get(file_path, 0) + 1\n\n            # Count by folder\n            for folder in metadata.get(\"folder_structure\", []):\n                folder_counts[folder] = folder_counts.get(folder, 0) + 1\n\n            # Count by chunk type\n            chunk_type = metadata.get(\"chunk_type\", \"unknown\")\n            chunk_type_counts[chunk_type] = chunk_type_counts.get(chunk_type, 0) + 1\n\n            # Count by tags\n            for tag in metadata.get(\"tags\", []):\n                tag_counts[tag] = tag_counts.get(tag, 0) + 1\n\n        stats.update(\n            {\n                \"files_indexed\": len(file_counts),\n                \"top_folders\": dict(\n                    sorted(folder_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n                ),\n                \"chunk_types\": chunk_type_counts,\n                \"top_tags\": dict(\n                    sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n                ),\n            }\n        )\n\n        # Save stats\n        with open(self.stats_path, \"w\") as f:\n            json.dump(stats, f, indent=2)\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get index statistics.\"\"\"\n        if self.stats_path.exists():\n            with open(self.stats_path, \"r\") as f:\n                return json.load(f)\n        else:\n            return {\n                \"total_chunks\": 0,\n                \"index_size\": 0,\n                \"embedding_dimension\": 0,\n                \"files_indexed\": 0,\n            }\n\n    def get_index_size(self) -> int:\n        \"\"\"Get the number of chunks in the index.\"\"\"\n        return len(self._chunk_ids)\n\n    def validate_index_consistency(self) -> Tuple[bool, List[str]]:\n        \"\"\"Validate consistency between FAISS index, chunk_ids, and metadata.\n\n        This method checks for:\n        1. FAISS index size matches chunk_ids list length\n        2. All chunk_ids have corresponding metadata entries\n        3. All metadata entries have valid index_ids\n        4. No orphaned vectors in FAISS\n\n        Returns:\n            Tuple of (is_valid, list_of_issues)\n        \"\"\"\n        issues = []\n\n        # Check if index exists\n        if self._index is None:\n            if len(self._chunk_ids) > 0:\n                issues.append(\n                    f\"FAISS index is None but chunk_ids list has {len(self._chunk_ids)} entries\"\n                )\n            if len(self.metadata_db) > 0:\n                issues.append(\n                    f\"FAISS index is None but metadata has {len(self.metadata_db)} entries\"\n                )\n            return len(issues) == 0, issues\n\n        # Check 1: FAISS index size matches chunk_ids length\n        faiss_size = self._index.ntotal\n        chunk_ids_size = len(self._chunk_ids)\n        if faiss_size != chunk_ids_size:\n            issues.append(\n                f\"FAISS index size ({faiss_size}) != chunk_ids length ({chunk_ids_size})\"\n            )\n\n        # Check 2: All chunk_ids have metadata entries\n        missing_metadata = []\n        for i, chunk_id in enumerate(self._chunk_ids):\n            metadata_entry = self.metadata_db.get(chunk_id)\n            if not metadata_entry:\n                missing_metadata.append(f\"{chunk_id} (position {i})\")\n            elif \"index_id\" in metadata_entry:\n                # Check 3: Metadata index_id is valid\n                index_id = metadata_entry[\"index_id\"]\n                if index_id != i:\n                    issues.append(\n                        f\"Chunk {chunk_id} has index_id {index_id} but is at position {i}\"\n                    )\n                if index_id >= faiss_size:\n                    issues.append(\n                        f\"Chunk {chunk_id} has index_id {index_id} >= FAISS size {faiss_size}\"\n                    )\n\n        if missing_metadata:\n            issues.append(\n                f\"Missing metadata for {len(missing_metadata)} chunks: \"\n                f\"{', '.join(missing_metadata[:5])}\"\n                + (f\" ... and {len(missing_metadata) - 5} more\" if len(missing_metadata) > 5 else \"\")\n            )\n\n        # Check 4: Metadata database size consistency\n        metadata_size = len(self.metadata_db)\n        if metadata_size != chunk_ids_size:\n            issues.append(\n                f\"Metadata database size ({metadata_size}) != chunk_ids length ({chunk_ids_size})\"\n            )\n\n        is_valid = len(issues) == 0\n\n        if is_valid:\n            self._logger.info(\n                f\"Index consistency validated: {faiss_size} vectors, \"\n                f\"{chunk_ids_size} chunk IDs, {metadata_size} metadata entries\"\n            )\n        else:\n            self._logger.warning(\n                f\"Index consistency validation failed with {len(issues)} issues\"\n            )\n            for issue in issues:\n                self._logger.warning(f\"  - {issue}\")\n\n        return is_valid, issues\n\n    def clear_index(self):\n        \"\"\"Clear the entire index and metadata.\"\"\"\n        # Close database connection\n        if self._metadata_db is not None:\n            self._metadata_db.close()\n            self._metadata_db = None\n\n        # Remove files\n        for file_path in [\n            self.index_path,\n            self.metadata_path,\n            self.chunk_id_path,\n            self.stats_path,\n        ]:\n            if file_path.exists():\n                file_path.unlink()\n\n        # Reset in-memory state\n        self._index = None\n        self._chunk_ids = []\n\n        self._logger.info(\"Index cleared\")\n\n    def check_memory_requirements(\n        self, num_new_vectors: int, dimension: int\n    ) -> Dict[str, Any]:\n        \"\"\"Check if there's enough memory for adding new vectors.\"\"\"\n        # Get current memory status\n        available = get_available_memory()\n\n        # Estimate memory needed for new vectors\n        estimate_index_memory_usage(num_new_vectors, dimension)\n\n        # Check current index size\n        current_size = self.get_index_size()\n        total_vectors_after = current_size + num_new_vectors\n\n        # Estimate total memory after adding vectors\n        total_estimated = estimate_index_memory_usage(total_vectors_after, dimension)\n\n        # Determine if we should use GPU or CPU\n        prefer_gpu = self._gpu_is_available()\n        target_memory = (\n            available[\"gpu_available\"] if prefer_gpu else available[\"system_available\"]\n        )\n\n        # Safety margin: require 20% more available memory than estimated\n        safety_factor = 1.2\n        required_memory = int(total_estimated[\"total\"] * safety_factor)\n\n        memory_check = {\n            \"available_memory\": available,\n            \"estimated_usage\": total_estimated,\n            \"required_memory\": required_memory,\n            \"current_vectors\": current_size,\n            \"new_vectors\": num_new_vectors,\n            \"total_vectors_after\": total_vectors_after,\n            \"prefer_gpu\": prefer_gpu,\n            \"sufficient_memory\": target_memory >= required_memory,\n            \"memory_utilization\": (\n                required_memory / target_memory if target_memory > 0 else float(\"inf\")\n            ),\n        }\n\n        # Log warning if memory is tight\n        if not memory_check[\"sufficient_memory\"]:\n            self._logger.warning(\n                f\"Insufficient memory: need {required_memory // (1024**2):.1f}MB, \"\n                f\"have {target_memory // (1024**2):.1f}MB \"\n                f\"({'GPU' if prefer_gpu else 'CPU'})\"\n            )\n        elif memory_check[\"memory_utilization\"] > 0.8:\n            self._logger.warning(\n                f\"High memory utilization: {memory_check['memory_utilization']:.1%} \"\n                f\"of available {'GPU' if prefer_gpu else 'CPU'} memory\"\n            )\n\n        return memory_check\n\n    def get_memory_status(self) -> Dict[str, Any]:\n        \"\"\"Get current memory usage status.\"\"\"\n        available = get_available_memory()\n        current_size = self.get_index_size()\n\n        status = {\n            \"available_memory\": available,\n            \"current_index_size\": current_size,\n            \"is_gpu_enabled\": self._on_gpu,\n            \"gpu_available\": self._gpu_is_available(),\n        }\n\n        # Estimate current index memory usage if we have vectors\n        if current_size > 0 and self._index is not None:\n            # Estimate dimension from index if available\n            try:\n                dimension = (\n                    self._index.d if hasattr(self._index, \"d\") else 768\n                )  # Default dimension\n                estimated = estimate_index_memory_usage(current_size, dimension)\n                status[\"estimated_index_memory\"] = estimated\n            except Exception as e:\n                self._logger.debug(f\"Could not estimate index memory usage: {e}\")\n\n        return status\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit - cleanup resources.\"\"\"\n        if self._metadata_db is not None:\n            self._metadata_db.close()\n            self._metadata_db = None\n        return False  # Don't suppress exceptions\n\n    def __del__(self):\n        \"\"\"Cleanup when object is destroyed.\"\"\"\n        if self._metadata_db is not None:\n            self._metadata_db.close()",
              "content_preview": "class CodeIndexManager:\n    \"\"\"Manages FAISS vector index and metadata storage for code chunks.\"\"\"\n\n    def __init__(self, storage_dir: str, embedder=None):\n        self.storage_dir = Path(storage_dir...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015806214827501837,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          }
        ],
        "all_doc_ids": [
          "search\\hybrid_searcher.py:1139-1208:method:remove_multiple_files",
          "search\\indexer.py:172-194:method:create_index",
          "search\\indexer.py:113-118:decorated_definition:index",
          "search\\indexer.py:775-855:method:validate_index_consistency",
          "search\\indexer.py:272-286:method:_maybe_move_index_to_gpu",
          "search\\indexer.py:629-679:method:save_index",
          "search\\indexer.py:129-170:method:_load_index",
          "search\\indexer.py:53-74:function:estimate_index_memory_usage",
          "search\\indexer.py:77-976:class:CodeIndexManager",
          "scripts\\__init__.py:1-2:module"
        ],
        "unique_discoveries": []
      },
      "comparison": {
        "time_overhead_ms": 43.88,
        "time_overhead_pct": 362.9,
        "top5_overlap_count": 4,
        "top5_overlap_pct": 80.0,
        "unique_discovery_count": 0,
        "value_rating": "NONE"
      }
    },
    {
      "query": "indexing and storage workflow",
      "single_hop": {
        "time_ms": 15.91,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "mcp_server\\server.py:641-761:decorated_definition:index_directory",
            "score": 7.681,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 641,
              "end_line": 761,
              "name": "index_directory",
              "parent_name": null,
              "docstring": "SETUP REQUIRED: Index a codebase for semantic search. Must run this before\n    using search_code on a new project. Supports Python, JavaScript, TypeScript, JSX, TSX, and Svelte.\n\n    WHEN TO USE:\n    - First time analyzing a new codebase\n    - After significant code changes that might affect search results\n    - When switching to a different project\n\n    PROCESS:\n    - Uses Merkle trees to detect file changes efficiently\n    - Only reprocesses changed/new files (incremental mode)\n    - Parses code files using AST (Python) and tree-sitter (JS/TS/JSX/TSX/Svelte)\n    - Chunks code into semantic units (functions, classes, methods)\n    - Generates 768-dimensional embeddings using EmbeddingGemma-300m\n    - Builds FAISS vector index for fast similarity search\n    - Stores metadata in SQLite database\n\n    Args:\n        directory_path: Absolute path to project root\n        project_name: Optional name for organization (defaults to directory name)\n        file_patterns: File patterns to include (default: all supported extensions)\n        incremental: Use incremental indexing if snapshot exists (default: True)\n\n    Returns:\n        JSON with indexing statistics and success status\n\n    Note: Incremental indexing is much faster for updates. Full reindex on first run.",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef index_directory(\n    directory_path: str,\n    project_name: str = None,\n    file_patterns: List[str] = None,\n    incremental: bool = True,\n) -> str:\n    \"\"\"\n    SETUP REQUIRED: Index a codebase for semantic search. Must run this before\n    using search_code on a new project. Supports Python, JavaScript, TypeScript, JSX, TSX, and Svelte.\n\n    WHEN TO USE:\n    - First time analyzing a new codebase\n    - After significant code changes that might affect search results\n    - When switching to a different project\n\n    PROCESS:\n    - Uses Merkle trees to detect file changes efficiently\n    - Only reprocesses changed/new files (incremental mode)\n    - Parses code files using AST (Python) and tree-sitter (JS/TS/JSX/TSX/Svelte)\n    - Chunks code into semantic units (functions, classes, methods)\n    - Generates 768-dimensional embeddings using EmbeddingGemma-300m\n    - Builds FAISS vector index for fast similarity search\n    - Stores metadata in SQLite database\n\n    Args:\n        directory_path: Absolute path to project root\n        project_name: Optional name for organization (defaults to directory name)\n        file_patterns: File patterns to include (default: all supported extensions)\n        incremental: Use incremental indexing if snapshot exists (default: True)\n\n    Returns:\n        JSON with indexing statistics and success status\n\n    Note: Incremental indexing is much faster for updates. Full reindex on first run.\n    \"\"\"\n    try:\n        from search.incremental_indexer import IncrementalIndexer\n\n        # Start model preload early to overlap with Merkle/IO work\n        _maybe_start_model_preload()\n\n        directory_path = Path(directory_path).resolve()\n        if not directory_path.exists():\n            return json.dumps({\"error\": f\"Directory does not exist: {directory_path}\"})\n\n        if not directory_path.is_dir():\n            return json.dumps({\"error\": f\"Path is not a directory: {directory_path}\"})\n\n        project_name = project_name or directory_path.name\n        logger.info(f\"Indexing directory: {directory_path} (incremental={incremental})\")\n\n        # Initialize incremental indexer - use HybridSearcher if hybrid search is enabled\n        config = get_search_config()\n        if config.enable_hybrid_search:\n            # Use HybridSearcher for indexing when hybrid search is enabled\n            project_storage = get_project_storage_dir(str(directory_path))\n            storage_dir = project_storage / \"index\"\n            indexer = HybridSearcher(\n                storage_dir=str(storage_dir),\n                embedder=get_embedder(),\n                bm25_weight=config.bm25_weight,\n                dense_weight=config.dense_weight,\n                rrf_k=config.rrf_k_parameter,\n                max_workers=2,\n            )\n            logger.info(\n                \"Using HybridSearcher for indexing to populate both BM25 and dense indices\"\n            )\n        else:\n            indexer = get_index_manager(str(directory_path))\n            logger.info(\"Using CodeIndexManager for dense-only indexing\")\n\n        embedder = get_embedder()\n        chunker = MultiLanguageChunker(str(directory_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer, embedder=embedder, chunker=chunker\n        )\n\n        # Update current project tracker (CRITICAL for multi-project isolation - Bug #2 fix)\n        global _current_project\n        _current_project = str(directory_path)\n        logger.info(\n            f\"[PER_MODEL_INDICES] Updated _current_project to: {_current_project}\"\n        )\n\n        # Perform indexing\n        result = incremental_indexer.incremental_index(\n            str(directory_path), project_name, force_full=not incremental\n        )\n\n        # Get updated statistics\n        stats = incremental_indexer.get_indexing_stats(str(directory_path))\n\n        response = {\n            \"success\": result.success,\n            \"directory\": str(directory_path),\n            \"project_name\": project_name,\n            \"incremental\": incremental and result.files_modified > 0,\n            \"files_added\": result.files_added,\n            \"files_removed\": result.files_removed,\n            \"files_modified\": result.files_modified,\n            \"chunks_added\": result.chunks_added,\n            \"chunks_removed\": result.chunks_removed,\n            \"time_taken\": round(result.time_taken, 2),\n            \"index_stats\": stats,\n        }\n\n        if result.error:\n            response[\"error\"] = result.error\n\n        logger.info(\n            f\"Indexing completed. Added: {result.files_added}, Modified: {result.files_modified}, Time: {result.time_taken:.2f}s\"\n        )\n        return json.dumps(response, indent=2)\n\n    except (OSError, IOError, ValueError, TypeError, RuntimeError, MemoryError) as e:\n        error_msg = f\"Indexing failed: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})",
              "content_preview": "@mcp.tool()\ndef index_directory(\n    directory_path: str,\n    project_name: str = None,\n    file_patterns: List[str] = None,\n    incremental: bool = True,\n) -> str:\n    \"\"\"\n    SETUP REQUIRED: Index a...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015677655677655677,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_full_flow.py:115-172:method:test_real_project_indexing_and_search",
            "score": 6.15,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 115,
              "end_line": 172,
              "name": "test_real_project_indexing_and_search",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test indexing and searching the real Python project.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_real_project_indexing_and_search(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test indexing and searching the real Python project.\"\"\"\n        # Step 1: Chunk the project\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        # Limit chunks for test performance\n        test_chunks = all_chunks[:20]\n        assert len(test_chunks) > 10, \"Should have enough chunks for testing\"\n\n        # Step 2: Create embeddings\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Step 3: Index the embeddings\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(embeddings)\n\n        assert len(index_manager._chunk_ids) == len(embeddings)\n\n        # Step 4: Test various searches\n        query_embedding = np.random.random(768).astype(np.float32)\n\n        # Basic search\n        results = index_manager.search(query_embedding, k=5)\n        assert len(results) > 0\n        assert len(results) <= 5\n\n        # Test that results have correct structure\n        for chunk_id, similarity, metadata in results:\n            assert chunk_id in [e.chunk_id for e in embeddings]\n            assert 0.0 <= similarity <= 1.0\n            assert isinstance(metadata, dict)\n            assert \"name\" in metadata\n            assert \"chunk_type\" in metadata\n            assert \"file_path\" in metadata\n\n        # Test filtering by chunk type\n        function_results = index_manager.search(\n            query_embedding, k=10, filters={\"chunk_type\": \"function\"}\n        )\n        for _chunk_id, _similarity, metadata in function_results:\n            assert metadata[\"chunk_type\"] == \"function\"\n\n        # Test filtering by file pattern\n        auth_results = index_manager.search(\n            query_embedding, k=10, filters={\"file_pattern\": [\"auth\"]}\n        )\n        for _chunk_id, _similarity, metadata in auth_results:\n            assert \"auth\" in metadata.get(\"file_path\", \"\") or \"auth\" in metadata.get(\n                \"relative_path\", \"\"\n            )",
              "content_preview": "def test_real_project_indexing_and_search(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test indexing and searching the real Python project.\"\"\"\n        # Step 1: Chunk the proje...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014409937888198759,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "search\\__init__.py:1-2:module",
            "score": 0.691,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\__init__.py",
              "relative_path": "search\\__init__.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "module",
              "start_line": 1,
              "end_line": 2,
              "name": null,
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "\"\"\"Search and indexing module.\"\"\"\n",
              "content_preview": "\"\"\"Search and indexing module.\"\"\"\n",
              "project_name": "claude-context-local",
              "rrf_score": 0.009836065573770493,
              "appears_in_lists": 1,
              "final_rank": 3
            }
          },
          {
            "doc_id": "scripts\\__init__.py:1-2:module",
            "score": 0.681,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\__init__.py",
              "relative_path": "scripts\\__init__.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "module",
              "start_line": 1,
              "end_line": 2,
              "name": null,
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "\"\"\"Scripts for code indexing and management.\"\"\"\n",
              "content_preview": "\"\"\"Scripts for code indexing and management.\"\"\"\n",
              "project_name": "claude-context-local",
              "rrf_score": 0.009677419354838708,
              "appears_in_lists": 1,
              "final_rank": 4
            }
          },
          {
            "doc_id": "tests\\integration\\test_incremental_indexing.py:25-364:class:TestIncrementalIndexing",
            "score": 0.621,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_incremental_indexing.py",
              "relative_path": "tests\\integration\\test_incremental_indexing.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "class",
              "start_line": 25,
              "end_line": 364,
              "name": "TestIncrementalIndexing",
              "parent_name": null,
              "docstring": "Test incremental indexing functionality.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestIncrementalIndexing(TestCase):\n    \"\"\"Test incremental indexing functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.test_path = Path(self.temp_dir) / \"test_project\"\n        self.test_path.mkdir()\n\n        # Storage for snapshots\n        self.snapshot_dir = Path(self.temp_dir) / \"snapshots\"\n        self.snapshot_manager = SnapshotManager(self.snapshot_dir)\n\n        # Storage for index\n        self.index_dir = Path(self.temp_dir) / \"index\"\n        self.index_dir.mkdir()\n\n        self.create_initial_codebase()\n\n    def tearDown(self):\n        \"\"\"Clean up test environment.\"\"\"\n        import shutil\n\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def create_initial_codebase(self):\n        \"\"\"Create initial Python codebase.\"\"\"\n        # Main module\n        (self.test_path / \"main.py\").write_text(\n            '''\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Hello World\")\n    return 0\n\nif __name__ == \"__main__\":\n    main()\n'''\n        )\n\n        # Utils module\n        (self.test_path / \"utils.py\").write_text(\n            '''\ndef helper(x, y):\n    \"\"\"Helper function.\"\"\"\n    return x + y\n\nclass Calculator:\n    \"\"\"Simple calculator.\"\"\"\n\n    def add(self, a, b):\n        return a + b\n\n    def subtract(self, a, b):\n        return a - b\n'''\n        )\n\n        # Create subdirectory\n        (self.test_path / \"lib\").mkdir()\n        (self.test_path / \"lib\" / \"database.py\").write_text(\n            '''\nclass Database:\n    \"\"\"Database connection.\"\"\"\n\n    def connect(self):\n        \"\"\"Connect to database.\"\"\"\n        pass\n\n    def query(self, sql):\n        \"\"\"Execute query.\"\"\"\n        return []\n'''\n        )\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_full_index(self):\n        \"\"\"Test full indexing of a codebase.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # First index should be full\n        result = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result.success\n        assert result.files_added > 0\n        assert result.chunks_added > 0\n        assert result.files_removed == 0\n        assert result.files_modified == 0\n\n        # Verify snapshot was created\n        assert self.snapshot_manager.has_snapshot(str(self.test_path))\n\n    def test_no_changes(self):\n        \"\"\"Test indexing when no changes occur.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # First index\n        result1 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result1.success\n\n        # Second index with no changes\n        result2 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result2.success\n        assert result2.files_added == 0\n        assert result2.files_removed == 0\n        assert result2.files_modified == 0\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_file_modification(self):\n        \"\"\"Test incremental indexing when files are modified.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # Initial index\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Modify a file\n        (self.test_path / \"utils.py\").write_text(\n            '''\ndef helper(x, y):\n    \"\"\"Updated helper function.\"\"\"\n    return x * y  # Changed from addition to multiplication\n\nclass Calculator:\n    \"\"\"Enhanced calculator.\"\"\"\n\n    def add(self, a, b):\n        return a + b\n\n    def multiply(self, a, b):\n        \"\"\"New method.\"\"\"\n        return a * b\n\n    def subtract(self, a, b):\n        return a - b\n'''\n        )\n\n        # Incremental index\n        result2 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result2.success\n        assert result2.files_modified == 1\n        assert result2.files_added == 0\n        assert result2.files_removed == 0\n        # Should have removed old chunks and added new ones\n        assert result2.chunks_removed > 0\n        assert result2.chunks_added > 0\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_file_addition(self):\n        \"\"\"Test incremental indexing when files are added.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # Initial index\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Add a new file\n        (self.test_path / \"new_module.py\").write_text(\n            '''\ndef new_function():\n    \"\"\"A new function.\"\"\"\n    return \"new\"\n\nclass NewClass:\n    \"\"\"A new class.\"\"\"\n    pass\n'''\n        )\n\n        # Incremental index\n        result2 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result2.success\n        assert result2.files_added == 1\n        assert result2.files_removed == 0\n        assert result2.files_modified == 0\n        assert result2.chunks_added > 0\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_file_deletion(self):\n        \"\"\"Test incremental indexing when files are deleted.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # Initial index\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Delete a file\n        (self.test_path / \"utils.py\").unlink()\n\n        # Incremental index\n        result2 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result2.success\n        assert result2.files_added == 0\n        assert result2.files_removed == 1\n        assert result2.files_modified == 0\n        assert result2.chunks_removed > 0\n\n    def test_change_detection(self):\n        \"\"\"Test change detection using Merkle trees.\"\"\"\n        detector = ChangeDetector(self.snapshot_manager)\n\n        # Build initial DAG\n        dag1 = MerkleDAG(str(self.test_path))\n        dag1.build()\n        self.snapshot_manager.save_snapshot(dag1)\n\n        # No changes should be detected\n        assert not detector.quick_check(str(self.test_path))\n\n        # Modify a file\n        time.sleep(0.1)  # Ensure different timestamp\n        (self.test_path / \"main.py\").write_text(\"# Modified\\n\")\n\n        # Changes should be detected\n        assert detector.quick_check(str(self.test_path))\n\n        # Get detailed changes\n        changes, new_dag = detector.detect_changes_from_snapshot(str(self.test_path))\n\n        assert changes.has_changes()\n        assert \"main.py\" in changes.modified\n\n    def test_needs_reindex(self):\n        \"\"\"Test checking if reindex is needed.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # Should need index initially\n        assert incremental_indexer.needs_reindex(str(self.test_path))\n\n        # Index the project\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Should not need reindex immediately\n        assert not incremental_indexer.needs_reindex(\n            str(self.test_path), max_age_minutes=1440\n        )  # 24 hours\n\n        # Modify a file\n        (self.test_path / \"main.py\").write_text(\"# Changed\\n\")\n\n        # Should need reindex after change\n        assert incremental_indexer.needs_reindex(str(self.test_path))\n\n    def test_indexing_stats(self):\n        \"\"\"Test getting indexing statistics.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # No stats initially\n        stats = incremental_indexer.get_indexing_stats(str(self.test_path))\n        assert stats is None\n\n        # Index the project\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Get stats\n        stats = incremental_indexer.get_indexing_stats(str(self.test_path))\n\n        assert stats is not None\n        assert stats[\"project_name\"] == \"test_project\"\n        assert stats[\"file_count\"] > 0\n        assert \"last_snapshot\" in stats\n        assert \"current_chunks\" in stats",
              "content_preview": "class TestIncrementalIndexing(TestCase):\n    \"\"\"Test incremental indexing functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n    ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.009375,
              "appears_in_lists": 1,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "tests\\unit\\test_bm25_index.py:164-187:method:test_save_and_load",
          "tests\\integration\\test_full_flow.py:115-172:method:test_real_project_indexing_and_search",
          "search\\hybrid_searcher.py:235-299:method:index_documents",
          "tools\\batch_index.py:19-137:function:main",
          "mcp_server\\server.py:641-761:decorated_definition:index_directory",
          "search\\__init__.py:1-2:module",
          "tests\\unit\\test_hybrid_search.py:356-386:decorated_definition:test_save_and_load_indices",
          "tests\\integration\\test_incremental_indexing.py:25-364:class:TestIncrementalIndexing",
          "scripts\\__init__.py:1-2:module",
          "search\\hybrid_searcher.py:60-157:method:__init__"
        ]
      },
      "multi_hop": {
        "time_ms": 48.45,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\integration\\test_full_flow.py:574-611:method:test_performance_with_large_codebase",
            "score": 8.579,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 574,
              "end_line": 611,
              "name": "test_performance_with_large_codebase",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test performance metrics with a larger codebase simulation.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_performance_with_large_codebase(self, test_project_path, mock_storage_dir):\n        \"\"\"Test performance metrics with a larger codebase simulation.\"\"\"\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        # Collect all chunks\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        # Duplicate chunks to simulate larger codebase\n        large_chunks = all_chunks * 10  # Simulate 10x larger codebase\n\n        # Measure indexing time\n        start_time = time.time()\n\n        embeddings = self._create_embeddings_from_chunks(large_chunks)\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(embeddings)\n\n        indexing_time = time.time() - start_time\n\n        # Measure search time\n        query_embedding = np.random.random(768).astype(np.float32)\n\n        start_time = time.time()\n        index_manager.search(query_embedding, k=10)\n        search_time = time.time() - start_time\n\n        # Performance assertions\n        assert indexing_time < 60, f\"Indexing took too long: {indexing_time}s\"\n        assert search_time < 1, f\"Search took too long: {search_time}s\"\n\n        print(\n            f\"Performance stats: Indexed {len(embeddings)} chunks in {indexing_time:.2f}s\"\n        )\n        print(f\"Search completed in {search_time:.3f}s\")",
              "content_preview": "def test_performance_with_large_codebase(self, test_project_path, mock_storage_dir):\n        \"\"\"Test performance metrics with a larger codebase simulation.\"\"\"\n        chunker = MultiLanguageChunker(st...",
              "project_name": "claude-context-local",
              "rrf_score": 0.013167388167388168,
              "appears_in_lists": 2,
              "final_rank": 7
            }
          },
          {
            "doc_id": "mcp_server\\server.py:641-761:decorated_definition:index_directory",
            "score": 7.681,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 641,
              "end_line": 761,
              "name": "index_directory",
              "parent_name": null,
              "docstring": "SETUP REQUIRED: Index a codebase for semantic search. Must run this before\n    using search_code on a new project. Supports Python, JavaScript, TypeScript, JSX, TSX, and Svelte.\n\n    WHEN TO USE:\n    - First time analyzing a new codebase\n    - After significant code changes that might affect search results\n    - When switching to a different project\n\n    PROCESS:\n    - Uses Merkle trees to detect file changes efficiently\n    - Only reprocesses changed/new files (incremental mode)\n    - Parses code files using AST (Python) and tree-sitter (JS/TS/JSX/TSX/Svelte)\n    - Chunks code into semantic units (functions, classes, methods)\n    - Generates 768-dimensional embeddings using EmbeddingGemma-300m\n    - Builds FAISS vector index for fast similarity search\n    - Stores metadata in SQLite database\n\n    Args:\n        directory_path: Absolute path to project root\n        project_name: Optional name for organization (defaults to directory name)\n        file_patterns: File patterns to include (default: all supported extensions)\n        incremental: Use incremental indexing if snapshot exists (default: True)\n\n    Returns:\n        JSON with indexing statistics and success status\n\n    Note: Incremental indexing is much faster for updates. Full reindex on first run.",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef index_directory(\n    directory_path: str,\n    project_name: str = None,\n    file_patterns: List[str] = None,\n    incremental: bool = True,\n) -> str:\n    \"\"\"\n    SETUP REQUIRED: Index a codebase for semantic search. Must run this before\n    using search_code on a new project. Supports Python, JavaScript, TypeScript, JSX, TSX, and Svelte.\n\n    WHEN TO USE:\n    - First time analyzing a new codebase\n    - After significant code changes that might affect search results\n    - When switching to a different project\n\n    PROCESS:\n    - Uses Merkle trees to detect file changes efficiently\n    - Only reprocesses changed/new files (incremental mode)\n    - Parses code files using AST (Python) and tree-sitter (JS/TS/JSX/TSX/Svelte)\n    - Chunks code into semantic units (functions, classes, methods)\n    - Generates 768-dimensional embeddings using EmbeddingGemma-300m\n    - Builds FAISS vector index for fast similarity search\n    - Stores metadata in SQLite database\n\n    Args:\n        directory_path: Absolute path to project root\n        project_name: Optional name for organization (defaults to directory name)\n        file_patterns: File patterns to include (default: all supported extensions)\n        incremental: Use incremental indexing if snapshot exists (default: True)\n\n    Returns:\n        JSON with indexing statistics and success status\n\n    Note: Incremental indexing is much faster for updates. Full reindex on first run.\n    \"\"\"\n    try:\n        from search.incremental_indexer import IncrementalIndexer\n\n        # Start model preload early to overlap with Merkle/IO work\n        _maybe_start_model_preload()\n\n        directory_path = Path(directory_path).resolve()\n        if not directory_path.exists():\n            return json.dumps({\"error\": f\"Directory does not exist: {directory_path}\"})\n\n        if not directory_path.is_dir():\n            return json.dumps({\"error\": f\"Path is not a directory: {directory_path}\"})\n\n        project_name = project_name or directory_path.name\n        logger.info(f\"Indexing directory: {directory_path} (incremental={incremental})\")\n\n        # Initialize incremental indexer - use HybridSearcher if hybrid search is enabled\n        config = get_search_config()\n        if config.enable_hybrid_search:\n            # Use HybridSearcher for indexing when hybrid search is enabled\n            project_storage = get_project_storage_dir(str(directory_path))\n            storage_dir = project_storage / \"index\"\n            indexer = HybridSearcher(\n                storage_dir=str(storage_dir),\n                embedder=get_embedder(),\n                bm25_weight=config.bm25_weight,\n                dense_weight=config.dense_weight,\n                rrf_k=config.rrf_k_parameter,\n                max_workers=2,\n            )\n            logger.info(\n                \"Using HybridSearcher for indexing to populate both BM25 and dense indices\"\n            )\n        else:\n            indexer = get_index_manager(str(directory_path))\n            logger.info(\"Using CodeIndexManager for dense-only indexing\")\n\n        embedder = get_embedder()\n        chunker = MultiLanguageChunker(str(directory_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer, embedder=embedder, chunker=chunker\n        )\n\n        # Update current project tracker (CRITICAL for multi-project isolation - Bug #2 fix)\n        global _current_project\n        _current_project = str(directory_path)\n        logger.info(\n            f\"[PER_MODEL_INDICES] Updated _current_project to: {_current_project}\"\n        )\n\n        # Perform indexing\n        result = incremental_indexer.incremental_index(\n            str(directory_path), project_name, force_full=not incremental\n        )\n\n        # Get updated statistics\n        stats = incremental_indexer.get_indexing_stats(str(directory_path))\n\n        response = {\n            \"success\": result.success,\n            \"directory\": str(directory_path),\n            \"project_name\": project_name,\n            \"incremental\": incremental and result.files_modified > 0,\n            \"files_added\": result.files_added,\n            \"files_removed\": result.files_removed,\n            \"files_modified\": result.files_modified,\n            \"chunks_added\": result.chunks_added,\n            \"chunks_removed\": result.chunks_removed,\n            \"time_taken\": round(result.time_taken, 2),\n            \"index_stats\": stats,\n        }\n\n        if result.error:\n            response[\"error\"] = result.error\n\n        logger.info(\n            f\"Indexing completed. Added: {result.files_added}, Modified: {result.files_modified}, Time: {result.time_taken:.2f}s\"\n        )\n        return json.dumps(response, indent=2)\n\n    except (OSError, IOError, ValueError, TypeError, RuntimeError, MemoryError) as e:\n        error_msg = f\"Indexing failed: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})",
              "content_preview": "@mcp.tool()\ndef index_directory(\n    directory_path: str,\n    project_name: str = None,\n    file_patterns: List[str] = None,\n    incremental: bool = True,\n) -> str:\n    \"\"\"\n    SETUP REQUIRED: Index a...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015677655677655677,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_full_flow.py:441-516:method:test_incremental_indexing_with_merkle",
            "score": 6.91,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 441,
              "end_line": 516,
              "name": "test_incremental_indexing_with_merkle",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test incremental indexing using Merkle tree change detection.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_incremental_indexing_with_merkle(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test incremental indexing using Merkle tree change detection.\"\"\"\n        # Initial indexing\n        chunker = MultiLanguageChunker(str(test_project_path))\n        initial_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            initial_chunks.extend(chunks)\n\n        initial_embeddings = self._create_embeddings_from_chunks(initial_chunks)\n\n        # Create initial index\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(initial_embeddings)\n\n        initial_count = len(index_manager._chunk_ids)\n\n        # Save the initial index\n        index_manager.save_index()\n\n        # Create Merkle snapshot manager and save initial state\n        snapshot_manager = SnapshotManager(str(mock_storage_dir))\n        merkle_dag = MerkleDAG(str(test_project_path))\n        merkle_dag.build()  # Build the DAG first\n        snapshot_manager.save_snapshot(merkle_dag)\n\n        # Simulate file changes by creating a temporary modified project\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_project = Path(temp_dir) / \"modified_project\"\n            shutil.copytree(test_project_path, temp_project)\n\n            # Modify a file to trigger incremental update\n            auth_file = temp_project / \"src\" / \"auth\" / \"authenticator.py\"\n            if auth_file.exists():\n                content = auth_file.read_text()\n                # Add a new function\n                new_function = \"\\n\\ndef new_auth_function():\\n    '''New authentication function.'''\\n    return True\\n\"\n                auth_file.write_text(content + new_function)\n\n            # Create new DAG for modified project\n            new_dag = MerkleDAG(str(temp_project))\n            new_dag.build()  # Build the DAG first\n\n            # Detect changes using ChangeDetector\n            detector = ChangeDetector()\n            changes = detector.detect_changes(merkle_dag, new_dag)\n\n            # Should detect at least one modified file\n            assert len(changes.modified) > 0 or len(changes.added) > 0\n\n            # Process only changed files (incremental indexing)\n            # Create a new chunker for the temp project\n            temp_chunker = MultiLanguageChunker(str(temp_project))\n            changed_chunks = []\n            for file_path in changes.modified + changes.added:\n                # The file_path from MerkleDAG is relative, construct full path\n                full_path = temp_project / file_path\n                if full_path.exists():\n                    chunks = temp_chunker.chunk_file(str(full_path))\n                    changed_chunks.extend(chunks)\n\n            # Should have found new chunks\n            assert len(changed_chunks) > 0\n\n            # Create embeddings for changed chunks\n            new_embeddings = self._create_embeddings_from_chunks(changed_chunks)\n\n            # Add new embeddings incrementally\n            index_manager.add_embeddings(new_embeddings)\n\n            # Should have more chunks now\n            assert len(index_manager._chunk_ids) > initial_count",
              "content_preview": "def test_incremental_indexing_with_merkle(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test incremental indexing using Merkle tree change detection.\"\"\"\n        # Initial indexi...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01328722242446305,
              "appears_in_lists": 2,
              "final_rank": 6
            }
          },
          {
            "doc_id": "tests\\integration\\test_mcp_indexing.py:36-96:decorated_definition:test_mcp_index_directory_path",
            "score": 6.636,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_mcp_indexing.py",
              "relative_path": "tests\\integration\\test_mcp_indexing.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 36,
              "end_line": 96,
              "name": "test_mcp_index_directory_path",
              "parent_name": "TestMCPIndexing",
              "docstring": "Test indexing following the exact MCP tool implementation path.",
              "decorators": [
                "@pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_mcp_index_directory_path(self, test_project_path, mock_storage_dir):\n        \"\"\"Test indexing following the exact MCP tool implementation path.\"\"\"\n\n        # This follows the exact implementation in mcp_server/server.py index_directory()\n        directory_path = Path(test_project_path).resolve()\n        project_name = directory_path.name\n        incremental = False  # Force full index for test\n\n        # Initialize components EXACTLY as in the MCP tool\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(\n            str(directory_path)\n        )  # Initialize with project root\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=index_manager, embedder=embedder, chunker=chunker\n        )\n\n        # Perform indexing - this is the exact call from the MCP tool\n        result = incremental_indexer.incremental_index(\n            str(directory_path), project_name, force_full=not incremental\n        )\n\n        # Get updated statistics - same as MCP tool\n        stats = incremental_indexer.get_indexing_stats(str(directory_path))\n\n        # Build response exactly as MCP tool does\n        response = {\n            \"success\": result.success,\n            \"directory\": str(directory_path),\n            \"project_name\": project_name,\n            \"incremental\": incremental and result.files_modified > 0,\n            \"files_added\": result.files_added,\n            \"files_removed\": result.files_removed,\n            \"files_modified\": result.files_modified,\n            \"chunks_added\": result.chunks_added,\n            \"chunks_removed\": result.chunks_removed,\n            \"time_taken\": round(result.time_taken, 2),\n            \"index_stats\": stats,\n        }\n\n        if result.error:\n            response[\"error\"] = result.error\n\n        # Assertions - the real tool should work!\n        assert result.success, f\"Indexing failed: {result.error}\"\n        assert result.files_added > 0, \"Should have indexed some files\"\n\n        assert result.chunks_added > 0, \"Should have created chunks from the files\"\n\n        # Verify the response structure matches what MCP returns\n        assert response[\"success\"] is True\n        assert response[\"files_added\"] > 0\n        assert response[\"chunks_added\"] > 0\n\n        # Cleanup embedder to free GPU memory\n        embedder.cleanup()\n\n        print(f\"MCP Response: {json.dumps(response, indent=2)}\")",
              "content_preview": "@pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_mcp_index_directory_path(self, test_project_path, mock_storage_dir):\n        \"\"\"Test indexing following ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012390508042681957,
              "appears_in_lists": 2,
              "final_rank": 9
            }
          },
          {
            "doc_id": "tests\\integration\\test_full_flow.py:115-172:method:test_real_project_indexing_and_search",
            "score": 6.15,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 115,
              "end_line": 172,
              "name": "test_real_project_indexing_and_search",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test indexing and searching the real Python project.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_real_project_indexing_and_search(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test indexing and searching the real Python project.\"\"\"\n        # Step 1: Chunk the project\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        # Limit chunks for test performance\n        test_chunks = all_chunks[:20]\n        assert len(test_chunks) > 10, \"Should have enough chunks for testing\"\n\n        # Step 2: Create embeddings\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Step 3: Index the embeddings\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(embeddings)\n\n        assert len(index_manager._chunk_ids) == len(embeddings)\n\n        # Step 4: Test various searches\n        query_embedding = np.random.random(768).astype(np.float32)\n\n        # Basic search\n        results = index_manager.search(query_embedding, k=5)\n        assert len(results) > 0\n        assert len(results) <= 5\n\n        # Test that results have correct structure\n        for chunk_id, similarity, metadata in results:\n            assert chunk_id in [e.chunk_id for e in embeddings]\n            assert 0.0 <= similarity <= 1.0\n            assert isinstance(metadata, dict)\n            assert \"name\" in metadata\n            assert \"chunk_type\" in metadata\n            assert \"file_path\" in metadata\n\n        # Test filtering by chunk type\n        function_results = index_manager.search(\n            query_embedding, k=10, filters={\"chunk_type\": \"function\"}\n        )\n        for _chunk_id, _similarity, metadata in function_results:\n            assert metadata[\"chunk_type\"] == \"function\"\n\n        # Test filtering by file pattern\n        auth_results = index_manager.search(\n            query_embedding, k=10, filters={\"file_pattern\": [\"auth\"]}\n        )\n        for _chunk_id, _similarity, metadata in auth_results:\n            assert \"auth\" in metadata.get(\"file_path\", \"\") or \"auth\" in metadata.get(\n                \"relative_path\", \"\"\n            )",
              "content_preview": "def test_real_project_indexing_and_search(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test indexing and searching the real Python project.\"\"\"\n        # Step 1: Chunk the proje...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014409937888198759,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          }
        ],
        "all_doc_ids": [
          "tests\\integration\\test_full_flow.py:574-611:method:test_performance_with_large_codebase",
          "tests\\integration\\test_full_flow.py:115-172:method:test_real_project_indexing_and_search",
          "search\\hybrid_searcher.py:235-299:method:index_documents",
          "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
          "tests\\unit\\test_hybrid_search.py:139-162:decorated_definition:test_index_documents",
          "tests\\integration\\test_mcp_indexing.py:36-96:decorated_definition:test_mcp_index_directory_path",
          "mcp_server\\server.py:641-761:decorated_definition:index_directory",
          "tests\\integration\\test_full_flow.py:441-516:method:test_incremental_indexing_with_merkle",
          "tests\\integration\\test_mcp_indexing.py:98-166:method:test_incremental_indexing_mcp_path",
          "tests\\integration\\test_incremental_indexing.py:25-364:class:TestIncrementalIndexing"
        ],
        "unique_discoveries": [
          "tests\\integration\\test_full_flow.py:574-611:method:test_performance_with_large_codebase",
          "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
          "tests\\unit\\test_hybrid_search.py:139-162:decorated_definition:test_index_documents",
          "tests\\integration\\test_mcp_indexing.py:36-96:decorated_definition:test_mcp_index_directory_path",
          "tests\\integration\\test_full_flow.py:441-516:method:test_incremental_indexing_with_merkle",
          "tests\\integration\\test_mcp_indexing.py:98-166:method:test_incremental_indexing_mcp_path"
        ]
      },
      "comparison": {
        "time_overhead_ms": 32.54,
        "time_overhead_pct": 204.5,
        "top5_overlap_count": 2,
        "top5_overlap_pct": 40.0,
        "unique_discovery_count": 6,
        "value_rating": "HIGH"
      }
    },
    {
      "query": "search result reranking methods",
      "single_hop": {
        "time_ms": 26.03,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\unit\\test_reranker.py:150-174:method:test_rerank_simple_method",
            "score": 7.958,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_reranker.py",
              "relative_path": "tests\\unit\\test_reranker.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 150,
              "end_line": 174,
              "name": "test_rerank_simple_method",
              "parent_name": "TestRRFReranker",
              "docstring": "Test the simplified reranking method.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_rerank_simple_method(self):\n        \"\"\"Test the simplified reranking method.\"\"\"\n        bm25_tuples = [\n            (\"doc1\", 0.9, {\"type\": \"function\"}),\n            (\"doc2\", 0.7, {\"type\": \"class\"}),\n            (\"doc3\", 0.5, {\"type\": \"variable\"}),\n        ]\n\n        dense_tuples = [\n            (\"doc2\", 0.8, {\"type\": \"class\"}),\n            (\"doc4\", 0.6, {\"type\": \"function\"}),\n            (\"doc1\", 0.4, {\"type\": \"function\"}),\n        ]\n\n        results = self.reranker.rerank_simple(\n            bm25_results=bm25_tuples,\n            dense_results=dense_tuples,\n            max_results=10,\n            bm25_weight=0.4,\n            dense_weight=0.6,\n        )\n\n        assert isinstance(results, list)\n        assert all(isinstance(r, SearchResult) for r in results)\n        assert all(r.source == \"hybrid\" for r in results)",
              "content_preview": "def test_rerank_simple_method(self):\n        \"\"\"Test the simplified reranking method.\"\"\"\n        bm25_tuples = [\n            (\"doc1\", 0.9, {\"type\": \"function\"}),\n            (\"doc2\", 0.7, {\"type\": \"cl...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015831265508684862,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\unit\\test_reranker.py:35-350:class:TestRRFReranker",
            "score": 7.776,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_reranker.py",
              "relative_path": "tests\\unit\\test_reranker.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "class",
              "start_line": 35,
              "end_line": 350,
              "name": "TestRRFReranker",
              "parent_name": null,
              "docstring": "Test RRF reranker functionality.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestRRFReranker:\n    \"\"\"Test RRF reranker functionality.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.reranker = RRFReranker(k=100, alpha=0.5)\n\n        # Sample search results from different sources\n        self.bm25_results = [\n            SearchResult(\"doc1\", 0.9, {\"type\": \"function\"}, \"bm25\", 1),\n            SearchResult(\"doc2\", 0.7, {\"type\": \"class\"}, \"bm25\", 2),\n            SearchResult(\"doc3\", 0.5, {\"type\": \"variable\"}, \"bm25\", 3),\n        ]\n\n        self.dense_results = [\n            SearchResult(\"doc2\", 0.8, {\"type\": \"class\"}, \"dense\", 1),\n            SearchResult(\"doc4\", 0.6, {\"type\": \"function\"}, \"dense\", 2),\n            SearchResult(\"doc1\", 0.4, {\"type\": \"function\"}, \"dense\", 3),\n        ]\n\n    def test_initialization(self):\n        \"\"\"Test reranker initialization.\"\"\"\n        reranker = RRFReranker(k=50, alpha=0.7)\n\n        assert reranker.k == 50\n        assert reranker.alpha == 0.7\n\n    def test_empty_results_reranking(self):\n        \"\"\"Test reranking with empty result lists.\"\"\"\n        results = self.reranker.rerank([], max_results=10)\n        assert results == []\n\n        results = self.reranker.rerank([[], []], max_results=10)\n        assert results == []\n\n    def test_single_list_reranking(self):\n        \"\"\"Test reranking with a single result list.\"\"\"\n        results = self.reranker.rerank(\n            [self.bm25_results], weights=[1.0], max_results=10\n        )\n\n        assert len(results) == len(self.bm25_results)\n        assert all(result.source == \"hybrid\" for result in results)\n        assert all(\"rrf_score\" in result.metadata for result in results)\n\n        # Should maintain relative order for single list\n        assert results[0].doc_id == \"doc1\"  # Best BM25 score\n\n    def test_multiple_lists_reranking(self):\n        \"\"\"Test reranking with multiple result lists.\"\"\"\n        results = self.reranker.rerank(\n            [self.bm25_results, self.dense_results], weights=[0.6, 0.4], max_results=10\n        )\n\n        # Should get unique documents\n        doc_ids = [r.doc_id for r in results]\n        assert len(doc_ids) == len(set(doc_ids))\n\n        # Should have RRF scores\n        assert all(\"rrf_score\" in result.metadata for result in results)\n        assert all(result.source == \"hybrid\" for result in results)\n\n        # RRF scores should be in descending order\n        rrf_scores = [r.metadata[\"rrf_score\"] for r in results]\n        assert rrf_scores == sorted(rrf_scores, reverse=True)\n\n    def test_weight_normalization(self):\n        \"\"\"Test that weights are properly normalized.\"\"\"\n        # Test with unnormalized weights\n        results1 = self.reranker.rerank(\n            [self.bm25_results, self.dense_results],\n            weights=[2.0, 2.0],  # Should be normalized to [0.5, 0.5]\n            max_results=10,\n        )\n\n        results2 = self.reranker.rerank(\n            [self.bm25_results, self.dense_results],\n            weights=[1.0, 1.0],  # Already normalized\n            max_results=10,\n        )\n\n        # Results should be the same (within floating point precision)\n        assert len(results1) == len(results2)\n        for r1, r2 in zip(results1, results2, strict=False):\n            assert r1.doc_id == r2.doc_id\n            assert abs(r1.metadata[\"rrf_score\"] - r2.metadata[\"rrf_score\"]) < 1e-6\n\n    def test_max_results_limiting(self):\n        \"\"\"Test that max_results parameter works.\"\"\"\n        results = self.reranker.rerank(\n            [self.bm25_results, self.dense_results], max_results=2\n        )\n\n        assert len(results) <= 2\n\n    def test_document_overlap_handling(self):\n        \"\"\"Test handling of documents that appear in multiple lists.\"\"\"\n        results = self.reranker.rerank(\n            [self.bm25_results, self.dense_results], max_results=10\n        )\n\n        # doc1 and doc2 appear in both lists\n        doc1_results = [r for r in results if r.doc_id == \"doc1\"]\n        doc2_results = [r for r in results if r.doc_id == \"doc2\"]\n\n        assert len(doc1_results) == 1\n        assert len(doc2_results) == 1\n\n        # Should track appearances in metadata\n        for result in results:\n            if result.doc_id in [\"doc1\", \"doc2\"]:\n                assert result.metadata[\"appears_in_lists\"] == 2\n            else:\n                assert result.metadata[\"appears_in_lists\"] == 1\n\n    def test_rerank_simple_method(self):\n        \"\"\"Test the simplified reranking method.\"\"\"\n        bm25_tuples = [\n            (\"doc1\", 0.9, {\"type\": \"function\"}),\n            (\"doc2\", 0.7, {\"type\": \"class\"}),\n            (\"doc3\", 0.5, {\"type\": \"variable\"}),\n        ]\n\n        dense_tuples = [\n            (\"doc2\", 0.8, {\"type\": \"class\"}),\n            (\"doc4\", 0.6, {\"type\": \"function\"}),\n            (\"doc1\", 0.4, {\"type\": \"function\"}),\n        ]\n\n        results = self.reranker.rerank_simple(\n            bm25_results=bm25_tuples,\n            dense_results=dense_tuples,\n            max_results=10,\n            bm25_weight=0.4,\n            dense_weight=0.6,\n        )\n\n        assert isinstance(results, list)\n        assert all(isinstance(r, SearchResult) for r in results)\n        assert all(r.source == \"hybrid\" for r in results)\n\n    def test_rrf_score_calculation(self):\n        \"\"\"Test RRF score calculation logic.\"\"\"\n        # Create simple test case where we can verify calculation\n        list1 = [SearchResult(\"doc1\", 1.0, {}, \"list1\", 1)]\n        list2 = [SearchResult(\"doc1\", 0.5, {}, \"list2\", 1)]\n\n        results = self.reranker.rerank(\n            [list1, list2], weights=[0.5, 0.5], max_results=1\n        )\n\n        # RRF score should be: 0.5 * (1/(100+1)) + 0.5 * (1/(100+1))\n        expected_rrf = 0.5 * (1.0 / 101) + 0.5 * (1.0 / 101)\n        actual_rrf = results[0].metadata[\"rrf_score\"]\n\n        assert abs(actual_rrf - expected_rrf) < 1e-6\n\n    def test_analyze_fusion_quality(self):\n        \"\"\"Test fusion quality analysis.\"\"\"\n        results = self.reranker.rerank(\n            [self.bm25_results, self.dense_results], max_results=10\n        )\n\n        analysis = self.reranker.analyze_fusion_quality(results)\n\n        assert \"total_results\" in analysis\n        assert \"high_quality_count\" in analysis\n        assert \"diversity_score\" in analysis\n        assert \"coverage_balance\" in analysis\n        assert \"source_distribution\" in analysis\n        assert \"avg_rrf_score\" in analysis\n\n        assert analysis[\"total_results\"] == len(results)\n        assert 0 <= analysis[\"diversity_score\"] <= 1.0\n        assert 0 <= analysis[\"coverage_balance\"] <= 1.0\n\n    def test_fusion_quality_empty_results(self):\n        \"\"\"Test fusion quality analysis with empty results.\"\"\"\n        analysis = self.reranker.analyze_fusion_quality([])\n\n        assert analysis[\"total_results\"] == 0\n        assert analysis[\"high_quality_count\"] == 0\n        assert analysis[\"diversity_score\"] == 0.0\n        assert analysis[\"coverage_balance\"] == 0.0\n\n    def test_parameter_tuning(self):\n        \"\"\"Test parameter tuning functionality.\"\"\"\n        results_lists = [self.bm25_results, self.dense_results]\n\n        tuning_result = self.reranker.tune_parameters(\n            results_lists,\n            k_values=[50, 100],\n            weight_combinations=[[0.5, 0.5], [0.7, 0.3]],\n        )\n\n        assert \"best_params\" in tuning_result\n        assert \"best_score\" in tuning_result\n        assert \"tested_configurations\" in tuning_result\n\n        assert \"k\" in tuning_result[\"best_params\"]\n        assert \"weights\" in tuning_result[\"best_params\"]\n        assert (\n            tuning_result[\"tested_configurations\"] == 4\n        )  # 2 k_values * 2 weight combinations\n\n    def test_different_k_values(self):\n        \"\"\"Test reranking with different k values.\"\"\"\n        reranker_low_k = RRFReranker(k=1)  # Very low k\n        reranker_high_k = RRFReranker(k=1000)  # Very high k\n\n        results_low = reranker_low_k.rerank(\n            [self.bm25_results, self.dense_results], max_results=10\n        )\n\n        results_high = reranker_high_k.rerank(\n            [self.bm25_results, self.dense_results], max_results=10\n        )\n\n        # Different k values should produce different RRF scores\n        low_scores = [r.metadata[\"rrf_score\"] for r in results_low]\n        high_scores = [r.metadata[\"rrf_score\"] for r in results_high]\n\n        # Check that the results exist and have different scores\n        assert len(low_scores) > 0, \"No results from low k reranker\"\n        assert len(high_scores) > 0, \"No results from high k reranker\"\n\n        # The test should just verify that reranking works with different k values\n        # The exact score differences may be very small, so just check that reranking completed\n        assert all(\n            isinstance(score, (int, float)) for score in low_scores\n        ), \"Low scores should be numeric\"\n        assert all(\n            isinstance(score, (int, float)) for score in high_scores\n        ), \"High scores should be numeric\"\n\n    def test_edge_case_single_document(self):\n        \"\"\"Test with single document in results.\"\"\"\n        single_result = [SearchResult(\"doc1\", 1.0, {}, \"source1\")]\n\n        results = self.reranker.rerank([single_result], max_results=10)\n\n        assert len(results) == 1\n        assert results[0].doc_id == \"doc1\"\n        assert results[0].source == \"hybrid\"\n\n    def test_identical_documents_different_sources(self):\n        \"\"\"Test handling of identical documents from different sources.\"\"\"\n        list1 = [SearchResult(\"doc1\", 0.9, {\"source_score\": 0.9}, \"source1\")]\n        list2 = [SearchResult(\"doc1\", 0.7, {\"source_score\": 0.7}, \"source2\")]\n\n        results = self.reranker.rerank([list1, list2], max_results=10)\n\n        assert len(results) == 1\n        assert results[0].doc_id == \"doc1\"\n        assert results[0].score == 0.9  # Should keep higher score\n        assert results[0].metadata[\"appears_in_lists\"] == 2\n\n    def test_weight_validation(self):\n        \"\"\"Test validation of weights parameter.\"\"\"\n        # Test mismatched weights length\n        with pytest.raises(ValueError):\n            self.reranker.rerank(\n                [self.bm25_results, self.dense_results],\n                weights=[0.5],  # Only one weight for two lists\n                max_results=10,\n            )\n\n    def test_zero_weights_handling(self):\n        \"\"\"Test handling of zero weights.\"\"\"\n        # All weights zero\n        results = self.reranker.rerank(\n            [self.bm25_results, self.dense_results], weights=[0.0, 0.0], max_results=10\n        )\n\n        # Should default to equal weights\n        assert len(results) > 0\n\n    def test_standard_deviation_calculation(self):\n        \"\"\"Test standard deviation calculation in analysis.\"\"\"\n        results = [\n            SearchResult(\"doc1\", 1.0, {\"rrf_score\": 0.1}, \"hybrid\"),\n            SearchResult(\"doc2\", 0.8, {\"rrf_score\": 0.2}, \"hybrid\"),\n            SearchResult(\"doc3\", 0.6, {\"rrf_score\": 0.3}, \"hybrid\"),\n        ]\n\n        analysis = self.reranker.analyze_fusion_quality(results)\n        std_dev = analysis[\"rrf_score_std\"]\n\n        # Manual calculation: scores = [0.1, 0.2, 0.3], mean = 0.2\n        expected_std = ((0.1 - 0.2) ** 2 + (0.2 - 0.2) ** 2 + (0.3 - 0.2) ** 2) / 3\n        expected_std = expected_std**0.5\n\n        assert abs(std_dev - expected_std) < 1e-6\n\n    def test_performance_with_large_lists(self):\n        \"\"\"Test performance with larger result lists.\"\"\"\n        import time\n\n        # Create larger result lists\n        large_list1 = [\n            SearchResult(f\"doc{i}\", 1.0 - i * 0.01, {}, \"source1\", i + 1)\n            for i in range(100)\n        ]\n        large_list2 = [\n            SearchResult(f\"doc{i}\", 0.9 - i * 0.01, {}, \"source2\", i + 1)\n            for i in range(50, 150)\n        ]\n\n        start_time = time.time()\n        results = self.reranker.rerank([large_list1, large_list2], max_results=50)\n        end_time = time.time()\n\n        # Should complete reasonably quickly (less than 1 second)\n        assert end_time - start_time < 1.0\n        assert len(results) == 50\n        assert all(\"rrf_score\" in r.metadata for r in results)",
              "content_preview": "class TestRRFReranker:\n    \"\"\"Test RRF reranker functionality.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.reranker = RRFReranker(k=100, alpha=0.5)\n\n        # Samp...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015172101449275362,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "search\\reranker.py:139-176:method:rerank_simple",
            "score": 7.817,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\reranker.py",
              "relative_path": "search\\reranker.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 139,
              "end_line": 176,
              "name": "rerank_simple",
              "parent_name": "RRFReranker",
              "docstring": "Simple reranking for BM25 + dense vector results.\n\n        Args:\n            bm25_results: BM25 results as (doc_id, score, metadata)\n            dense_results: Dense results as (doc_id, score, metadata)\n            max_results: Maximum results to return\n            bm25_weight: Weight for BM25 results (0.0 to 1.0)\n            dense_weight: Weight for dense results (0.0 to 1.0)\n\n        Returns:\n            Combined and reranked results",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def rerank_simple(\n        self,\n        bm25_results: List[Tuple[str, float, Dict]],\n        dense_results: List[Tuple[str, float, Dict]],\n        max_results: int = 10,\n        bm25_weight: float = 0.4,\n        dense_weight: float = 0.6,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Simple reranking for BM25 + dense vector results.\n\n        Args:\n            bm25_results: BM25 results as (doc_id, score, metadata)\n            dense_results: Dense results as (doc_id, score, metadata)\n            max_results: Maximum results to return\n            bm25_weight: Weight for BM25 results (0.0 to 1.0)\n            dense_weight: Weight for dense results (0.0 to 1.0)\n\n        Returns:\n            Combined and reranked results\n        \"\"\"\n        # Convert tuples to SearchResult objects\n        bm25_search_results = [\n            SearchResult(doc_id=doc_id, score=score, metadata=metadata, source=\"bm25\")\n            for doc_id, score, metadata in bm25_results\n        ]\n\n        dense_search_results = [\n            SearchResult(doc_id=doc_id, score=score, metadata=metadata, source=\"dense\")\n            for doc_id, score, metadata in dense_results\n        ]\n\n        # Use main rerank method\n        return self.rerank(\n            results_lists=[bm25_search_results, dense_search_results],\n            weights=[bm25_weight, dense_weight],\n            max_results=max_results,\n        )",
              "content_preview": "def rerank_simple(\n        self,\n        bm25_results: List[Tuple[str, float, Dict]],\n        dense_results: List[Tuple[str, float, Dict]],\n        max_results: int = 10,\n        bm25_weight: float = ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015113122171945702,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:301-426:method:search",
            "score": 7.826,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 301,
              "end_line": 426,
              "name": "search",
              "parent_name": "HybridSearcher",
              "docstring": "Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)\n        \"\"\"\n        # Check if indices are ready based on search mode\n        if search_mode == \"bm25\":\n            if self.bm25_index.is_empty:\n                self._logger.warning(\"BM25 search requested but BM25 index is empty\")\n                return []\n        elif search_mode == \"semantic\":\n            if not self.dense_index.index or self.dense_index.index.ntotal == 0:\n                self._logger.warning(\n                    \"Semantic search requested but dense index is empty\"\n                )\n                return []\n        else:  # hybrid\n            if not self.is_ready:\n                self._logger.warning(\"Hybrid search not ready - indices may be empty\")\n                return []\n\n        self._logger.debug(f\"{search_mode.title()} search for: '{query}' (k={k})\")\n\n        start_time = time.time()\n\n        # Handle different search modes\n        if search_mode == \"bm25\":\n            # BM25-only search\n            bm25_results = self._search_bm25(query, k, min_bm25_score)\n            # Convert BM25 results to SearchResult format\n            final_results = self._convert_bm25_to_search_results(bm25_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        elif search_mode == \"semantic\":\n            # Dense-only search\n            dense_results = self._search_dense(query, k, filters)\n            # Convert dense results to SearchResult format\n            final_results = self._convert_dense_to_search_results(dense_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        else:  # hybrid mode\n            search_k = k * 2  # Get more results for better reranking\n\n            if use_parallel and not self._is_shutdown:\n                # Parallel execution\n                bm25_results, dense_results = self._parallel_search(\n                    query, search_k, min_bm25_score, filters\n                )\n            else:\n                # Sequential execution\n                bm25_results, dense_results = self._sequential_search(\n                    query, search_k, min_bm25_score, filters\n                )\n\n            # Rerank results\n            rerank_start = time.time()\n            self._logger.debug(\n                f\"[RERANK] Using weights: BM25={self.bm25_weight}, Dense={self.dense_weight}, \"\n                f\"BM25_results={len(bm25_results)}, Dense_results={len(dense_results)}\"\n            )\n            final_results = self.reranker.rerank_simple(\n                bm25_results=bm25_results,\n                dense_results=dense_results,\n                max_results=k,\n                bm25_weight=self.bm25_weight,\n                dense_weight=self.dense_weight,\n            )\n            rerank_time = time.time() - rerank_start\n            self._logger.debug(\n                f\"[RERANK] Produced {len(final_results)} results in {rerank_time:.3f}s\"\n            )\n\n        # Update statistics\n        total_time = time.time() - start_time\n        self._search_stats[\"total_searches\"] += 1\n        self._search_stats[\"rerank_time\"] += rerank_time\n\n        if use_parallel:\n            parallel_time = max(\n                self._search_stats.get(\"last_bm25_time\", 0),\n                self._search_stats.get(\"last_dense_time\", 0),\n            )\n            sequential_time = self._search_stats.get(\n                \"last_bm25_time\", 0\n            ) + self._search_stats.get(\"last_dense_time\", 0)\n            if sequential_time > 0:\n                efficiency = 1.0 - (parallel_time / sequential_time)\n                self._search_stats[\"parallel_efficiency\"] = efficiency\n\n        # Mode-specific logging\n        if search_mode == \"bm25\":\n            self._logger.debug(\n                f\"BM25 search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        elif search_mode == \"semantic\":\n            self._logger.debug(\n                f\"Semantic search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        else:  # hybrid\n            self._logger.debug(\n                f\"Hybrid search complete: {len(final_results)} results, \"\n                f\"BM25: {len(bm25_results)}, Dense: {len(dense_results)}, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n\n        return final_results",
              "content_preview": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014793678665496048,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "tests\\unit\\test_reranker.py:83-99:method:test_multiple_lists_reranking",
            "score": 7.422,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_reranker.py",
              "relative_path": "tests\\unit\\test_reranker.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 83,
              "end_line": 99,
              "name": "test_multiple_lists_reranking",
              "parent_name": "TestRRFReranker",
              "docstring": "Test reranking with multiple result lists.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multiple_lists_reranking(self):\n        \"\"\"Test reranking with multiple result lists.\"\"\"\n        results = self.reranker.rerank(\n            [self.bm25_results, self.dense_results], weights=[0.6, 0.4], max_results=10\n        )\n\n        # Should get unique documents\n        doc_ids = [r.doc_id for r in results]\n        assert len(doc_ids) == len(set(doc_ids))\n\n        # Should have RRF scores\n        assert all(\"rrf_score\" in result.metadata for result in results)\n        assert all(result.source == \"hybrid\" for result in results)\n\n        # RRF scores should be in descending order\n        rrf_scores = [r.metadata[\"rrf_score\"] for r in results]\n        assert rrf_scores == sorted(rrf_scores, reverse=True)",
              "content_preview": "def test_multiple_lists_reranking(self):\n        \"\"\"Test reranking with multiple result lists.\"\"\"\n        results = self.reranker.rerank(\n            [self.bm25_results, self.dense_results], weights=[...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014669509594882727,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "search\\reranker.py:139-176:method:rerank_simple",
          "tests\\unit\\test_reranker.py:62-68:method:test_empty_results_reranking",
          "tests\\integration\\test_hybrid_search_integration.py:323-355:decorated_definition:test_hybrid_reranking_combines_results",
          "tests\\unit\\test_reranker.py:240-268:method:test_different_k_values",
          "search\\hybrid_searcher.py:301-426:method:search",
          "tests\\unit\\test_reranker.py:35-350:class:TestRRFReranker",
          "tests\\unit\\test_reranker.py:83-99:method:test_multiple_lists_reranking",
          "tests\\unit\\test_reranker.py:150-174:method:test_rerank_simple_method",
          "search\\hybrid_searcher.py:606-666:method:_rerank_by_query",
          "tests\\unit\\test_reranker.py:70-81:method:test_single_list_reranking"
        ]
      },
      "multi_hop": {
        "time_ms": 41.47,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\integration\\test_hybrid_search_integration.py:323-355:decorated_definition:test_hybrid_reranking_combines_results",
            "score": 8.761,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_hybrid_search_integration.py",
              "relative_path": "tests\\integration\\test_hybrid_search_integration.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 323,
              "end_line": 355,
              "name": "test_hybrid_reranking_combines_results",
              "parent_name": "TestHybridSearchIntegration",
              "docstring": "Test that hybrid reranking properly combines BM25 and dense results.",
              "decorators": [
                "@pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_reranking_combines_results(self):\n        \"\"\"Test that hybrid reranking properly combines BM25 and dense results.\"\"\"\n        self.initialize_components()\n\n        # Index the project\n        result = self.incremental_indexer.incremental_index(\n            str(self.project_dir), project_name=\"test_project\", force_full=True\n        )\n        assert result.success, \"Indexing must succeed\"\n\n        query = \"calculate sum\"\n\n        # Get results from both searches\n        bm25_results = self.hybrid_searcher._search_bm25(query, k=10, min_score=0.0)\n        dense_results = self.hybrid_searcher._search_dense(query, k=10, filters=None)\n\n        # Get hybrid results\n        hybrid_results = self.hybrid_searcher.search(query, k=5, use_parallel=False)\n\n        # Hybrid results should exist\n        assert len(hybrid_results) > 0, \"Hybrid search should return results\"\n\n        # Check that hybrid results contain documents from both searches\n        hybrid_doc_ids = {result.doc_id for result in hybrid_results}\n        bm25_doc_ids = {doc_id for doc_id, _, _ in bm25_results}\n        dense_doc_ids = {doc_id for doc_id, _, _ in dense_results}\n\n        # At least some hybrid results should come from BM25 or dense\n        assert (\n            len(hybrid_doc_ids & bm25_doc_ids) > 0\n            or len(hybrid_doc_ids & dense_doc_ids) > 0\n        ), \"Hybrid results should include documents from BM25 or dense searches\"",
              "content_preview": "@pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_hybrid_reranking_combines_results(self):\n        \"\"\"Test that hybrid reranking properly combines BM25 an...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01424968474148802,
              "appears_in_lists": 2,
              "final_rank": 8
            }
          },
          {
            "doc_id": "tests\\unit\\test_reranker.py:62-68:method:test_empty_results_reranking",
            "score": 8.313,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_reranker.py",
              "relative_path": "tests\\unit\\test_reranker.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 62,
              "end_line": 68,
              "name": "test_empty_results_reranking",
              "parent_name": "TestRRFReranker",
              "docstring": "Test reranking with empty result lists.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_empty_results_reranking(self):\n        \"\"\"Test reranking with empty result lists.\"\"\"\n        results = self.reranker.rerank([], max_results=10)\n        assert results == []\n\n        results = self.reranker.rerank([[], []], max_results=10)\n        assert results == []",
              "content_preview": "def test_empty_results_reranking(self):\n        \"\"\"Test reranking with empty result lists.\"\"\"\n        results = self.reranker.rerank([], max_results=10)\n        assert results == []\n\n        results =...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014451612903225806,
              "appears_in_lists": 2,
              "final_rank": 7
            }
          },
          {
            "doc_id": "tests\\unit\\test_reranker.py:70-81:method:test_single_list_reranking",
            "score": 8.056,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_reranker.py",
              "relative_path": "tests\\unit\\test_reranker.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 70,
              "end_line": 81,
              "name": "test_single_list_reranking",
              "parent_name": "TestRRFReranker",
              "docstring": "Test reranking with a single result list.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_single_list_reranking(self):\n        \"\"\"Test reranking with a single result list.\"\"\"\n        results = self.reranker.rerank(\n            [self.bm25_results], weights=[1.0], max_results=10\n        )\n\n        assert len(results) == len(self.bm25_results)\n        assert all(result.source == \"hybrid\" for result in results)\n        assert all(\"rrf_score\" in result.metadata for result in results)\n\n        # Should maintain relative order for single list\n        assert results[0].doc_id == \"doc1\"  # Best BM25 score",
              "content_preview": "def test_single_list_reranking(self):\n        \"\"\"Test reranking with a single result list.\"\"\"\n        results = self.reranker.rerank(\n            [self.bm25_results], weights=[1.0], max_results=10\n   ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014583333333333334,
              "appears_in_lists": 2,
              "final_rank": 6
            }
          },
          {
            "doc_id": "tests\\unit\\test_reranker.py:150-174:method:test_rerank_simple_method",
            "score": 7.958,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_reranker.py",
              "relative_path": "tests\\unit\\test_reranker.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 150,
              "end_line": 174,
              "name": "test_rerank_simple_method",
              "parent_name": "TestRRFReranker",
              "docstring": "Test the simplified reranking method.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_rerank_simple_method(self):\n        \"\"\"Test the simplified reranking method.\"\"\"\n        bm25_tuples = [\n            (\"doc1\", 0.9, {\"type\": \"function\"}),\n            (\"doc2\", 0.7, {\"type\": \"class\"}),\n            (\"doc3\", 0.5, {\"type\": \"variable\"}),\n        ]\n\n        dense_tuples = [\n            (\"doc2\", 0.8, {\"type\": \"class\"}),\n            (\"doc4\", 0.6, {\"type\": \"function\"}),\n            (\"doc1\", 0.4, {\"type\": \"function\"}),\n        ]\n\n        results = self.reranker.rerank_simple(\n            bm25_results=bm25_tuples,\n            dense_results=dense_tuples,\n            max_results=10,\n            bm25_weight=0.4,\n            dense_weight=0.6,\n        )\n\n        assert isinstance(results, list)\n        assert all(isinstance(r, SearchResult) for r in results)\n        assert all(r.source == \"hybrid\" for r in results)",
              "content_preview": "def test_rerank_simple_method(self):\n        \"\"\"Test the simplified reranking method.\"\"\"\n        bm25_tuples = [\n            (\"doc1\", 0.9, {\"type\": \"function\"}),\n            (\"doc2\", 0.7, {\"type\": \"cl...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015831265508684862,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "search\\config.py:30-98:decorated_definition:SearchConfig",
            "score": 7.913,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\config.py",
              "relative_path": "search\\config.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 30,
              "end_line": 98,
              "name": "SearchConfig",
              "parent_name": null,
              "docstring": "Configuration for search behavior.",
              "decorators": [
                "@dataclass"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@dataclass\nclass SearchConfig:\n    \"\"\"Configuration for search behavior.\"\"\"\n\n    # Embedding Model Configuration\n    embedding_model_name: str = \"google/embeddinggemma-300m\"\n    model_dimension: int = 768\n    embedding_batch_size: int = 128  # Dynamic based on model, see MODEL_REGISTRY\n\n    # Search Mode Configuration\n    default_search_mode: str = \"hybrid\"  # hybrid, semantic, bm25, auto\n    enable_hybrid_search: bool = True\n\n    # Hybrid Search Weights\n    bm25_weight: float = 0.4\n    dense_weight: float = 0.6\n\n    # Performance Settings\n    use_parallel_search: bool = True\n    max_parallel_workers: int = 2\n\n    # BM25 Configuration\n    bm25_k_parameter: int = 100\n    bm25_use_stopwords: bool = True\n    min_bm25_score: float = 0.1\n\n    # Reranking Configuration\n    rrf_k_parameter: int = 100\n    enable_result_reranking: bool = True\n\n    # GPU Configuration\n    prefer_gpu: bool = True\n    gpu_memory_threshold: float = 0.8\n\n    # Auto-reindexing\n    enable_auto_reindex: bool = True\n    max_index_age_minutes: float = 5.0\n\n    # Multi-hop Search Configuration\n    enable_multi_hop: bool = True\n    multi_hop_count: int = 2\n    multi_hop_expansion: float = 0.3\n\n    # Search Result Limits\n    default_k: int = 5\n    max_k: int = 50\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"SearchConfig\":\n        \"\"\"Create from dictionary.\"\"\"\n        # Auto-update dimension and batch size if model is in registry\n        if \"embedding_model_name\" in data:\n            model_config = get_model_config(data[\"embedding_model_name\"])\n            if model_config:\n                data[\"model_dimension\"] = model_config[\"dimension\"]\n                # Only auto-set batch size if not explicitly provided\n                if \"embedding_batch_size\" not in data:\n                    data[\"embedding_batch_size\"] = model_config.get(\n                        \"recommended_batch_size\", 128\n                    )\n\n        # Filter only known fields to avoid TypeError\n        valid_fields = {f.name for f in cls.__dataclass_fields__.values()}\n        filtered_data = {k: v for k, v in data.items() if k in valid_fields}\n        return cls(**filtered_data)",
              "content_preview": "@dataclass\nclass SearchConfig:\n    \"\"\"Configuration for search behavior.\"\"\"\n\n    # Embedding Model Configuration\n    embedding_model_name: str = \"google/embeddinggemma-300m\"\n    model_dimension: int =...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01272727272727273,
              "appears_in_lists": 2,
              "final_rank": 11
            }
          }
        ],
        "all_doc_ids": [
          "search\\reranker.py:139-176:method:rerank_simple",
          "tests\\integration\\test_hybrid_search_integration.py:323-355:decorated_definition:test_hybrid_reranking_combines_results",
          "tests\\unit\\test_reranker.py:62-68:method:test_empty_results_reranking",
          "tests\\unit\\test_reranker.py:240-268:method:test_different_k_values",
          "search\\hybrid_searcher.py:301-426:method:search",
          "tests\\unit\\test_reranker.py:35-350:class:TestRRFReranker",
          "tests\\unit\\test_reranker.py:83-99:method:test_multiple_lists_reranking",
          "tests\\unit\\test_reranker.py:150-174:method:test_rerank_simple_method",
          "search\\config.py:30-98:decorated_definition:SearchConfig",
          "tests\\unit\\test_reranker.py:70-81:method:test_single_list_reranking"
        ],
        "unique_discoveries": [
          "search\\config.py:30-98:decorated_definition:SearchConfig"
        ]
      },
      "comparison": {
        "time_overhead_ms": 15.44,
        "time_overhead_pct": 59.3,
        "top5_overlap_count": 1,
        "top5_overlap_pct": 20.0,
        "unique_discovery_count": 1,
        "value_rating": "LOW"
      }
    },
    {
      "query": "GPU memory optimization techniques",
      "single_hop": {
        "time_ms": 18.31,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "search\\hybrid_searcher.py:21-54:class:GPUMemoryMonitor",
            "score": 13.557,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "class",
              "start_line": 21,
              "end_line": 54,
              "name": "GPUMemoryMonitor",
              "parent_name": null,
              "docstring": "Monitor GPU memory usage for optimal batch sizing.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class GPUMemoryMonitor:\n    \"\"\"Monitor GPU memory usage for optimal batch sizing.\"\"\"\n\n    def __init__(self):\n        self._logger = logging.getLogger(__name__)\n\n    def get_available_memory(self) -> Dict[str, int]:\n        \"\"\"Get available memory in bytes.\"\"\"\n        memory_info = {\"gpu_available\": 0, \"gpu_total\": 0, \"gpu_utilization\": 0.0}\n\n        if torch and torch.cuda.is_available():\n            try:\n                device = torch.cuda.current_device()\n                gpu_memory = torch.cuda.mem_get_info(device)\n                memory_info[\"gpu_available\"] = gpu_memory[0]\n                memory_info[\"gpu_total\"] = gpu_memory[1]\n                memory_info[\"gpu_utilization\"] = 1.0 - (gpu_memory[0] / gpu_memory[1])\n            except Exception as e:\n                self._logger.warning(f\"Failed to get GPU memory info: {e}\")\n\n        return memory_info\n\n    def can_use_gpu(self, required_memory: int = 1024 * 1024 * 1024) -> bool:\n        \"\"\"Check if GPU can be used for operations.\"\"\"\n        if not torch or not torch.cuda.is_available():\n            return False\n\n        memory_info = self.get_available_memory()\n        return memory_info[\"gpu_available\"] > required_memory\n\n    def estimate_batch_memory(self, batch_size: int, embedding_dim: int = 768) -> int:\n        \"\"\"Estimate memory usage for a batch.\"\"\"\n        # float32 = 4 bytes, plus overhead\n        return batch_size * embedding_dim * 4 * 2  # 2x safety margin",
              "content_preview": "class GPUMemoryMonitor:\n    \"\"\"Monitor GPU memory usage for optimal batch sizing.\"\"\"\n\n    def __init__(self):\n        self._logger = logging.getLogger(__name__)\n\n    def get_available_memory(self) -> ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016185271922976842,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "search\\indexer.py:30-50:function:get_available_memory",
            "score": 13.727,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "function",
              "start_line": 30,
              "end_line": 50,
              "name": "get_available_memory",
              "parent_name": null,
              "docstring": "Get available system and GPU memory in bytes.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def get_available_memory() -> Dict[str, int]:\n    \"\"\"Get available system and GPU memory in bytes.\"\"\"\n    memory_info = {\n        \"system_total\": psutil.virtual_memory().total,\n        \"system_available\": psutil.virtual_memory().available,\n        \"gpu_total\": 0,\n        \"gpu_available\": 0,\n    }\n\n    # Get GPU memory if CUDA available\n    if torch and torch.cuda.is_available():\n        try:\n            gpu_props = torch.cuda.get_device_properties(0)\n            memory_info[\"gpu_total\"] = gpu_props.total_memory\n            memory_info[\"gpu_available\"] = (\n                gpu_props.total_memory - torch.cuda.memory_allocated(0)\n            )\n        except Exception:\n            pass\n\n    return memory_info",
              "content_preview": "def get_available_memory() -> Dict[str, int]:\n    \"\"\"Get available system and GPU memory in bytes.\"\"\"\n    memory_info = {\n        \"system_total\": psutil.virtual_memory().total,\n        \"system_availab...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01597542242703533,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\unit\\test_hybrid_search.py:11-62:class:TestGPUMemoryMonitor",
            "score": 11.706,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_hybrid_search.py",
              "relative_path": "tests\\unit\\test_hybrid_search.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "class",
              "start_line": 11,
              "end_line": 62,
              "name": "TestGPUMemoryMonitor",
              "parent_name": null,
              "docstring": "Test GPU memory monitoring functionality.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestGPUMemoryMonitor:\n    \"\"\"Test GPU memory monitoring functionality.\"\"\"\n\n    def test_memory_info_without_gpu(self):\n        \"\"\"Test memory info when GPU is not available.\"\"\"\n        with patch(\"search.hybrid_searcher.torch\", None):\n            monitor = GPUMemoryMonitor()\n            info = monitor.get_available_memory()\n\n            assert info[\"gpu_available\"] == 0\n            assert info[\"gpu_total\"] == 0\n            assert info[\"gpu_utilization\"] == 0.0\n\n    @patch(\"search.hybrid_searcher.torch\")\n    def test_memory_info_with_gpu(self, mock_torch):\n        \"\"\"Test memory info when GPU is available.\"\"\"\n        mock_torch.cuda.is_available.return_value = True\n        mock_torch.cuda.current_device.return_value = 0\n        mock_torch.cuda.mem_get_info.return_value = (\n            4 * 1024**3,\n            8 * 1024**3,\n        )  # 4GB free, 8GB total\n\n        monitor = GPUMemoryMonitor()\n        info = monitor.get_available_memory()\n\n        assert info[\"gpu_available\"] == 4 * 1024**3\n        assert info[\"gpu_total\"] == 8 * 1024**3\n        assert info[\"gpu_utilization\"] == 0.5  # 50% utilized\n\n    @patch(\"search.hybrid_searcher.torch\")\n    def test_can_use_gpu(self, mock_torch):\n        \"\"\"Test GPU availability checking.\"\"\"\n        mock_torch.cuda.is_available.return_value = True\n        mock_torch.cuda.mem_get_info.return_value = (\n            2 * 1024**3,\n            8 * 1024**3,\n        )  # 2GB available\n\n        monitor = GPUMemoryMonitor()\n\n        assert monitor.can_use_gpu(1024**3)  # 1GB requirement - should pass\n        assert not monitor.can_use_gpu(3 * 1024**3)  # 3GB requirement - should fail\n\n    def test_batch_memory_estimation(self):\n        \"\"\"Test batch memory estimation.\"\"\"\n        monitor = GPUMemoryMonitor()\n\n        # Test memory estimation\n        memory = monitor.estimate_batch_memory(100, 768)\n        expected = 100 * 768 * 4 * 2  # batch_size * dim * float32 * safety_margin\n        assert memory == expected",
              "content_preview": "class TestGPUMemoryMonitor:\n    \"\"\"Test GPU memory monitoring functionality.\"\"\"\n\n    def test_memory_info_without_gpu(self):\n        \"\"\"Test memory info when GPU is not available.\"\"\"\n        with patc...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015647568608570053,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:27-41:method:get_available_memory",
            "score": 13.776,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 27,
              "end_line": 41,
              "name": "get_available_memory",
              "parent_name": "GPUMemoryMonitor",
              "docstring": "Get available memory in bytes.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def get_available_memory(self) -> Dict[str, int]:\n        \"\"\"Get available memory in bytes.\"\"\"\n        memory_info = {\"gpu_available\": 0, \"gpu_total\": 0, \"gpu_utilization\": 0.0}\n\n        if torch and torch.cuda.is_available():\n            try:\n                device = torch.cuda.current_device()\n                gpu_memory = torch.cuda.mem_get_info(device)\n                memory_info[\"gpu_available\"] = gpu_memory[0]\n                memory_info[\"gpu_total\"] = gpu_memory[1]\n                memory_info[\"gpu_utilization\"] = 1.0 - (gpu_memory[0] / gpu_memory[1])\n            except Exception as e:\n                self._logger.warning(f\"Failed to get GPU memory info: {e}\")\n\n        return memory_info",
              "content_preview": "def get_available_memory(self) -> Dict[str, int]:\n        \"\"\"Get available memory in bytes.\"\"\"\n        memory_info = {\"gpu_available\": 0, \"gpu_total\": 0, \"gpu_utilization\": 0.0}\n\n        if torch and ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015512600929777343,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:43-49:method:can_use_gpu",
            "score": 11.641,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 43,
              "end_line": 49,
              "name": "can_use_gpu",
              "parent_name": "GPUMemoryMonitor",
              "docstring": "Check if GPU can be used for operations.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def can_use_gpu(self, required_memory: int = 1024 * 1024 * 1024) -> bool:\n        \"\"\"Check if GPU can be used for operations.\"\"\"\n        if not torch or not torch.cuda.is_available():\n            return False\n\n        memory_info = self.get_available_memory()\n        return memory_info[\"gpu_available\"] > required_memory",
              "content_preview": "def can_use_gpu(self, required_memory: int = 1024 * 1024 * 1024) -> bool:\n        \"\"\"Check if GPU can be used for operations.\"\"\"\n        if not torch or not torch.cuda.is_available():\n            retu...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015113122171945702,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "mcp_server\\server.py:1137-1214:decorated_definition:get_memory_status",
          "search\\hybrid_searcher.py:27-41:method:get_available_memory",
          "search\\indexer.py:30-50:function:get_available_memory",
          "tests\\unit\\test_hybrid_search.py:24-39:decorated_definition:test_memory_info_with_gpu",
          "search\\hybrid_searcher.py:43-49:method:can_use_gpu",
          "search\\hybrid_searcher.py:21-54:class:GPUMemoryMonitor",
          "tests\\unit\\test_hybrid_search.py:11-62:class:TestGPUMemoryMonitor",
          "tests\\unit\\test_hybrid_search.py:14-22:method:test_memory_info_without_gpu",
          "search\\indexer.py:880-934:method:check_memory_requirements",
          "mcp_server\\server.py:197-234:function:_cleanup_previous_resources"
        ]
      },
      "multi_hop": {
        "time_ms": 27.21,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "search\\hybrid_searcher.py:27-41:method:get_available_memory",
            "score": 13.776,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 27,
              "end_line": 41,
              "name": "get_available_memory",
              "parent_name": "GPUMemoryMonitor",
              "docstring": "Get available memory in bytes.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def get_available_memory(self) -> Dict[str, int]:\n        \"\"\"Get available memory in bytes.\"\"\"\n        memory_info = {\"gpu_available\": 0, \"gpu_total\": 0, \"gpu_utilization\": 0.0}\n\n        if torch and torch.cuda.is_available():\n            try:\n                device = torch.cuda.current_device()\n                gpu_memory = torch.cuda.mem_get_info(device)\n                memory_info[\"gpu_available\"] = gpu_memory[0]\n                memory_info[\"gpu_total\"] = gpu_memory[1]\n                memory_info[\"gpu_utilization\"] = 1.0 - (gpu_memory[0] / gpu_memory[1])\n            except Exception as e:\n                self._logger.warning(f\"Failed to get GPU memory info: {e}\")\n\n        return memory_info",
              "content_preview": "def get_available_memory(self) -> Dict[str, int]:\n        \"\"\"Get available memory in bytes.\"\"\"\n        memory_info = {\"gpu_available\": 0, \"gpu_total\": 0, \"gpu_utilization\": 0.0}\n\n        if torch and ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015512600929777343,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "search\\indexer.py:30-50:function:get_available_memory",
            "score": 13.727,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "function",
              "start_line": 30,
              "end_line": 50,
              "name": "get_available_memory",
              "parent_name": null,
              "docstring": "Get available system and GPU memory in bytes.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def get_available_memory() -> Dict[str, int]:\n    \"\"\"Get available system and GPU memory in bytes.\"\"\"\n    memory_info = {\n        \"system_total\": psutil.virtual_memory().total,\n        \"system_available\": psutil.virtual_memory().available,\n        \"gpu_total\": 0,\n        \"gpu_available\": 0,\n    }\n\n    # Get GPU memory if CUDA available\n    if torch and torch.cuda.is_available():\n        try:\n            gpu_props = torch.cuda.get_device_properties(0)\n            memory_info[\"gpu_total\"] = gpu_props.total_memory\n            memory_info[\"gpu_available\"] = (\n                gpu_props.total_memory - torch.cuda.memory_allocated(0)\n            )\n        except Exception:\n            pass\n\n    return memory_info",
              "content_preview": "def get_available_memory() -> Dict[str, int]:\n    \"\"\"Get available system and GPU memory in bytes.\"\"\"\n    memory_info = {\n        \"system_total\": psutil.virtual_memory().total,\n        \"system_availab...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01597542242703533,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:21-54:class:GPUMemoryMonitor",
            "score": 13.557,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "class",
              "start_line": 21,
              "end_line": 54,
              "name": "GPUMemoryMonitor",
              "parent_name": null,
              "docstring": "Monitor GPU memory usage for optimal batch sizing.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class GPUMemoryMonitor:\n    \"\"\"Monitor GPU memory usage for optimal batch sizing.\"\"\"\n\n    def __init__(self):\n        self._logger = logging.getLogger(__name__)\n\n    def get_available_memory(self) -> Dict[str, int]:\n        \"\"\"Get available memory in bytes.\"\"\"\n        memory_info = {\"gpu_available\": 0, \"gpu_total\": 0, \"gpu_utilization\": 0.0}\n\n        if torch and torch.cuda.is_available():\n            try:\n                device = torch.cuda.current_device()\n                gpu_memory = torch.cuda.mem_get_info(device)\n                memory_info[\"gpu_available\"] = gpu_memory[0]\n                memory_info[\"gpu_total\"] = gpu_memory[1]\n                memory_info[\"gpu_utilization\"] = 1.0 - (gpu_memory[0] / gpu_memory[1])\n            except Exception as e:\n                self._logger.warning(f\"Failed to get GPU memory info: {e}\")\n\n        return memory_info\n\n    def can_use_gpu(self, required_memory: int = 1024 * 1024 * 1024) -> bool:\n        \"\"\"Check if GPU can be used for operations.\"\"\"\n        if not torch or not torch.cuda.is_available():\n            return False\n\n        memory_info = self.get_available_memory()\n        return memory_info[\"gpu_available\"] > required_memory\n\n    def estimate_batch_memory(self, batch_size: int, embedding_dim: int = 768) -> int:\n        \"\"\"Estimate memory usage for a batch.\"\"\"\n        # float32 = 4 bytes, plus overhead\n        return batch_size * embedding_dim * 4 * 2  # 2x safety margin",
              "content_preview": "class GPUMemoryMonitor:\n    \"\"\"Monitor GPU memory usage for optimal batch sizing.\"\"\"\n\n    def __init__(self):\n        self._logger = logging.getLogger(__name__)\n\n    def get_available_memory(self) -> ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016185271922976842,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "search\\indexer.py:880-934:method:check_memory_requirements",
            "score": 13.453,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 880,
              "end_line": 934,
              "name": "check_memory_requirements",
              "parent_name": "CodeIndexManager",
              "docstring": "Check if there's enough memory for adding new vectors.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def check_memory_requirements(\n        self, num_new_vectors: int, dimension: int\n    ) -> Dict[str, Any]:\n        \"\"\"Check if there's enough memory for adding new vectors.\"\"\"\n        # Get current memory status\n        available = get_available_memory()\n\n        # Estimate memory needed for new vectors\n        estimate_index_memory_usage(num_new_vectors, dimension)\n\n        # Check current index size\n        current_size = self.get_index_size()\n        total_vectors_after = current_size + num_new_vectors\n\n        # Estimate total memory after adding vectors\n        total_estimated = estimate_index_memory_usage(total_vectors_after, dimension)\n\n        # Determine if we should use GPU or CPU\n        prefer_gpu = self._gpu_is_available()\n        target_memory = (\n            available[\"gpu_available\"] if prefer_gpu else available[\"system_available\"]\n        )\n\n        # Safety margin: require 20% more available memory than estimated\n        safety_factor = 1.2\n        required_memory = int(total_estimated[\"total\"] * safety_factor)\n\n        memory_check = {\n            \"available_memory\": available,\n            \"estimated_usage\": total_estimated,\n            \"required_memory\": required_memory,\n            \"current_vectors\": current_size,\n            \"new_vectors\": num_new_vectors,\n            \"total_vectors_after\": total_vectors_after,\n            \"prefer_gpu\": prefer_gpu,\n            \"sufficient_memory\": target_memory >= required_memory,\n            \"memory_utilization\": (\n                required_memory / target_memory if target_memory > 0 else float(\"inf\")\n            ),\n        }\n\n        # Log warning if memory is tight\n        if not memory_check[\"sufficient_memory\"]:\n            self._logger.warning(\n                f\"Insufficient memory: need {required_memory // (1024**2):.1f}MB, \"\n                f\"have {target_memory // (1024**2):.1f}MB \"\n                f\"({'GPU' if prefer_gpu else 'CPU'})\"\n            )\n        elif memory_check[\"memory_utilization\"] > 0.8:\n            self._logger.warning(\n                f\"High memory utilization: {memory_check['memory_utilization']:.1%} \"\n                f\"of available {'GPU' if prefer_gpu else 'CPU'} memory\"\n            )\n\n        return memory_check",
              "content_preview": "def check_memory_requirements(\n        self, num_new_vectors: int, dimension: int\n    ) -> Dict[str, Any]:\n        \"\"\"Check if there's enough memory for adding new vectors.\"\"\"\n        # Get current me...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014358108108108109,
              "appears_in_lists": 2,
              "final_rank": 10
            }
          },
          {
            "doc_id": "search\\indexer.py:936-960:method:get_memory_status",
            "score": 12.684,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\indexer.py",
              "relative_path": "search\\indexer.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 936,
              "end_line": 960,
              "name": "get_memory_status",
              "parent_name": "CodeIndexManager",
              "docstring": "Get current memory usage status.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def get_memory_status(self) -> Dict[str, Any]:\n        \"\"\"Get current memory usage status.\"\"\"\n        available = get_available_memory()\n        current_size = self.get_index_size()\n\n        status = {\n            \"available_memory\": available,\n            \"current_index_size\": current_size,\n            \"is_gpu_enabled\": self._on_gpu,\n            \"gpu_available\": self._gpu_is_available(),\n        }\n\n        # Estimate current index memory usage if we have vectors\n        if current_size > 0 and self._index is not None:\n            # Estimate dimension from index if available\n            try:\n                dimension = (\n                    self._index.d if hasattr(self._index, \"d\") else 768\n                )  # Default dimension\n                estimated = estimate_index_memory_usage(current_size, dimension)\n                status[\"estimated_index_memory\"] = estimated\n            except Exception as e:\n                self._logger.debug(f\"Could not estimate index memory usage: {e}\")\n\n        return status",
              "content_preview": "def get_memory_status(self) -> Dict[str, Any]:\n        \"\"\"Get current memory usage status.\"\"\"\n        available = get_available_memory()\n        current_size = self.get_index_size()\n\n        status = ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.013382761816496757,
              "appears_in_lists": 2,
              "final_rank": 13
            }
          }
        ],
        "all_doc_ids": [
          "mcp_server\\server.py:1137-1214:decorated_definition:get_memory_status",
          "search\\indexer.py:936-960:method:get_memory_status",
          "tests\\conftest.py:238-256:decorated_definition:embedder_with_cleanup",
          "search\\hybrid_searcher.py:27-41:method:get_available_memory",
          "search\\indexer.py:30-50:function:get_available_memory",
          "search\\hybrid_searcher.py:43-49:method:can_use_gpu",
          "search\\hybrid_searcher.py:21-54:class:GPUMemoryMonitor",
          "tests\\unit\\test_hybrid_search.py:11-62:class:TestGPUMemoryMonitor",
          "tests\\unit\\test_hybrid_search.py:14-22:method:test_memory_info_without_gpu",
          "search\\indexer.py:880-934:method:check_memory_requirements"
        ],
        "unique_discoveries": [
          "tests\\conftest.py:238-256:decorated_definition:embedder_with_cleanup",
          "search\\indexer.py:936-960:method:get_memory_status"
        ]
      },
      "comparison": {
        "time_overhead_ms": 8.9,
        "time_overhead_pct": 48.6,
        "top5_overlap_count": 3,
        "top5_overlap_pct": 60.0,
        "unique_discovery_count": 2,
        "value_rating": "MEDIUM"
      }
    },
    {
      "query": "multi-language file support",
      "single_hop": {
        "time_ms": 30.8,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
            "score": 11.073,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 704,
              "end_line": 746,
              "name": "test_multi_language_indexing",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test indexing and searching multi-language project.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_language_indexing(self, multi_lang_project_path, mock_storage_dir):\n        \"\"\"Test indexing and searching multi-language project.\"\"\"\n        # Step 1: Chunk the multi-language project\n        chunker = MultiLanguageChunker(str(multi_lang_project_path))\n        all_chunks = []\n\n        # Get all supported files\n        for ext in [\".py\", \".js\", \".ts\", \".jsx\", \".tsx\", \".svelte\"]:\n            for file_path in multi_lang_project_path.glob(f\"*{ext}\"):\n                chunks = chunker.chunk_file(str(file_path))\n                all_chunks.extend(chunks)\n\n        # Should find chunks from multiple languages\n        assert (\n            len(all_chunks) > 5\n        ), f\"Should chunk multiple files, got {len(all_chunks)}\"\n\n        # Verify we have chunks from different file types\n        file_extensions = {Path(chunk.file_path).suffix for chunk in all_chunks}\n        assert (\n            len(file_extensions) >= 3\n        ), f\"Should support multiple languages, got {file_extensions}\"\n\n        # Step 2: Create embeddings and index\n        embeddings = self._create_embeddings_from_chunks(all_chunks)\n\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(embeddings)\n\n        # Step 3: Test searching across languages\n        query_embedding = np.random.random(768).astype(np.float32)\n        results = index_manager.search(query_embedding, k=10)\n\n        assert len(results) > 0\n\n        # Verify we can find chunks from different languages\n        result_extensions = {\n            Path(metadata[\"file_path\"]).suffix for _, _, metadata in results\n        }\n        assert (\n            len(result_extensions) >= 2\n        ), f\"Should find results from multiple languages, got {result_extensions}\"",
              "content_preview": "def test_multi_language_indexing(self, multi_lang_project_path, mock_storage_dir):\n        \"\"\"Test indexing and searching multi-language project.\"\"\"\n        # Step 1: Chunk the multi-language project\n...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015932377049180328,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_full_flow.py:27-30:decorated_definition:multi_lang_project_path",
            "score": 9.174,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 27,
              "end_line": 30,
              "name": "multi_lang_project_path",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Path to the multi-language test project.",
              "decorators": [
                "@pytest.fixture"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@pytest.fixture\n    def multi_lang_project_path(self):\n        \"\"\"Path to the multi-language test project.\"\"\"\n        return Path(__file__).parent.parent / \"test_data\" / \"multi_language\"",
              "content_preview": "@pytest.fixture\n    def multi_lang_project_path(self):\n        \"\"\"Path to the multi-language test project.\"\"\"\n        return Path(__file__).parent.parent / \"test_data\" / \"multi_language\"",
              "project_name": "claude-context-local",
              "rrf_score": 0.01568238213399504,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "chunking\\tree_sitter.py:1110-1124:method:is_supported",
            "score": 0.644,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\tree_sitter.py",
              "relative_path": "chunking\\tree_sitter.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "method",
              "start_line": 1110,
              "end_line": 1124,
              "name": "is_supported",
              "parent_name": "TreeSitterChunker",
              "docstring": "Check if a file type is supported.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            True if file type is supported",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def is_supported(self, file_path: str) -> bool:\n        \"\"\"Check if a file type is supported.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            True if file type is supported\n        \"\"\"\n        suffix = Path(file_path).suffix.lower()\n        if suffix not in self.LANGUAGE_MAP:\n            return False\n\n        language_name, _ = self.LANGUAGE_MAP[suffix]\n        return language_name in AVAILABLE_LANGUAGES",
              "content_preview": "def is_supported(self, file_path: str) -> bool:\n        \"\"\"Check if a file type is supported.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            True if file type is ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.009836065573770493,
              "appears_in_lists": 1,
              "final_rank": 3
            }
          },
          {
            "doc_id": "chunking\\multi_language_chunker.py:13-307:class:MultiLanguageChunker",
            "score": 0.644,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\chunking\\multi_language_chunker.py",
              "relative_path": "chunking\\multi_language_chunker.py",
              "folder_structure": [
                "chunking"
              ],
              "chunk_type": "class",
              "start_line": 13,
              "end_line": 307,
              "name": "MultiLanguageChunker",
              "parent_name": null,
              "docstring": "Unified chunker supporting multiple programming languages.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class MultiLanguageChunker:\n    \"\"\"Unified chunker supporting multiple programming languages.\"\"\"\n\n    # Supported extensions\n    SUPPORTED_EXTENSIONS = {\n        \".py\",  # Python\n        \".js\",  # JavaScript\n        \".jsx\",  # JSX\n        \".ts\",  # TypeScript\n        \".tsx\",  # TSX\n        \".svelte\",  # Svelte\n        \".go\",  # Go\n        \".rs\",  # Rust\n        \".java\",  # Java\n        \".c\",  # C\n        \".cpp\",  # C++\n        \".cc\",  # C++\n        \".cxx\",  # C++\n        \".c++\",  # C++\n        \".cs\",  # C#\n        \".glsl\",  # GLSL shader\n        \".frag\",  # Fragment shader\n        \".vert\",  # Vertex shader\n        \".comp\",  # Compute shader\n        \".geom\",  # Geometry shader\n        \".tesc\",  # Tessellation control shader\n        \".tese\",  # Tessellation evaluation shader\n    }\n\n    # Common large/build/tooling directories to skip during traversal\n    DEFAULT_IGNORED_DIRS = {\n        \"__pycache__\",\n        \".git\",\n        \".hg\",\n        \".svn\",\n        \".venv\",\n        \"venv\",\n        \"env\",\n        \".env\",\n        \".direnv\",\n        \"site-packages\",  # Python package installations\n        \"node_modules\",\n        \".pnpm-store\",\n        \".yarn\",\n        \".pytest_cache\",\n        \".mypy_cache\",\n        \".ruff_cache\",\n        \".pytype\",\n        \".ipynb_checkpoints\",\n        \"build\",\n        \"dist\",\n        \"out\",\n        \"public\",\n        \".next\",\n        \".nuxt\",\n        \".svelte-kit\",\n        \".angular\",\n        \".astro\",\n        \".vite\",\n        \".cache\",\n        \".parcel-cache\",\n        \".turbo\",\n        \"coverage\",\n        \".coverage\",\n        \".nyc_output\",\n        \".gradle\",\n        \".idea\",\n        \".vscode\",\n        \".docusaurus\",\n        \".vercel\",\n        \".serverless\",\n        \".terraform\",\n        \".mvn\",\n        \".tox\",\n        \"target\",\n        \"bin\",\n        \"obj\",\n        \"_archive\",\n    }\n\n    def __init__(self, root_path: Optional[str] = None):\n        \"\"\"Initialize multi-language chunker.\n\n        Args:\n            root_path: Optional root path for relative path calculation\n        \"\"\"\n        self.root_path = root_path\n        # Use AST chunker for Python (more mature implementation)\n        # Use tree-sitter for other languages\n        self.tree_sitter_chunker = TreeSitterChunker()\n\n    def is_supported(self, file_path: str) -> bool:\n        \"\"\"Check if file type is supported.\n\n        Args:\n            file_path: Path to file\n\n        Returns:\n            True if file type is supported\n        \"\"\"\n        suffix = Path(file_path).suffix.lower()\n        return suffix in self.SUPPORTED_EXTENSIONS\n\n    def chunk_file(self, file_path: str) -> List[CodeChunk]:\n        \"\"\"Chunk a file into semantic units.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            List of CodeChunk objects\n        \"\"\"\n        if not self.is_supported(file_path):\n            logger.debug(f\"File type not supported: {file_path}\")\n            return []\n\n        Path(file_path).suffix.lower()\n\n        # Use tree-sitter for all  languages\n        try:\n            tree_chunks = self.tree_sitter_chunker.chunk_file(file_path)\n            # Convert TreeSitterChunk to CodeChunk\n            return self._convert_tree_chunks(tree_chunks, file_path)\n        except Exception as e:\n            logger.error(f\"Failed to chunk file {file_path}: {e}\")\n            return []\n\n    def _convert_tree_chunks(\n        self, tree_chunks: List[TreeSitterChunk], file_path: str\n    ) -> List[CodeChunk]:\n        \"\"\"Convert tree-sitter chunks to CodeChunk format.\n\n        Args:\n            tree_chunks: List of TreeSitterChunk objects\n            file_path: Path to the source file\n\n        Returns:\n            List of CodeChunk objects\n        \"\"\"\n        code_chunks = []\n\n        for tchunk in tree_chunks:\n            # Extract metadata\n            name = tchunk.metadata.get(\"name\")\n            docstring = tchunk.metadata.get(\"docstring\")\n            decorators = tchunk.metadata.get(\"decorators\", [])\n\n            # Map tree-sitter node types to our chunk types\n            chunk_type_map = {\n                \"function_declaration\": \"function\",\n                \"function_definition\": \"function\",\n                \"arrow_function\": \"function\",\n                \"function\": \"function\",\n                \"function_item\": \"function\",  # Rust\n                \"method_declaration\": \"method\",  # Go, Java\n                \"method_definition\": \"method\",\n                \"class_declaration\": \"class\",\n                \"class_definition\": \"class\",\n                \"class_specifier\": \"class\",  # C++\n                \"interface_declaration\": \"interface\",\n                \"type_alias_declaration\": \"type\",\n                \"type_declaration\": \"type\",  # Go\n                \"enum_declaration\": \"enum\",\n                \"enum_specifier\": \"enum\",  # C\n                \"enum_item\": \"enum\",  # Rust\n                \"struct_declaration\": \"struct\",  # C#\n                \"struct_specifier\": \"struct\",  # C/C++\n                \"struct_item\": \"struct\",  # Rust\n                \"union_specifier\": \"union\",  # C/C++\n                \"namespace_definition\": \"namespace\",  # C++\n                \"namespace_declaration\": \"namespace\",  # C#\n                \"impl_item\": \"impl\",  # Rust\n                \"trait_item\": \"trait\",  # Rust\n                \"mod_item\": \"module\",  # Rust\n                \"macro_definition\": \"macro\",  # Rust\n                \"constructor_declaration\": \"constructor\",  # Java/C#\n                \"destructor_declaration\": \"destructor\",  # C#\n                \"property_declaration\": \"property\",  # C#\n                \"event_declaration\": \"event\",  # C#\n                \"template_declaration\": \"template\",  # C++\n                \"concept_definition\": \"concept\",  # C++\n                \"annotation_type_declaration\": \"annotation\",  # Java\n                \"script_element\": \"script\",  # Svelte\n                \"style_element\": \"style\",  # Svelte\n                \"variable_declaration\": \"variable\",  # GLSL uniforms, varying, attributes\n                \"preprocessor_define\": \"define\",  # GLSL preprocessor defines\n                \"preprocessor_function_def\": \"define\",  # GLSL preprocessor function defines\n                \"block_statement\": \"block\",  # GLSL code blocks\n                \"compound_statement\": \"block\",  # GLSL compound statements\n            }\n\n            chunk_type = chunk_type_map.get(tchunk.node_type, tchunk.node_type)\n\n            # Extract parent name and adjust chunk type for methods\n            parent_name = tchunk.metadata.get(\"parent_name\")\n\n            # If we have a parent_name and it's a function, it's actually a method\n            if parent_name and chunk_type == \"function\":\n                chunk_type = \"method\"\n\n            # Build folder structure from file path\n            path = Path(file_path)\n            folder_parts = []\n            if self.root_path:\n                try:\n                    rel_path = path.relative_to(self.root_path)\n                    folder_parts = list(rel_path.parent.parts)\n                except ValueError:\n                    folder_parts = [path.parent.name] if path.parent.name else []\n            else:\n                folder_parts = [path.parent.name] if path.parent.name else []\n\n            # Extract semantic tags from metadata\n            tags = []\n            if tchunk.metadata.get(\"is_async\"):\n                tags.append(\"async\")\n            if tchunk.metadata.get(\"is_generator\"):\n                tags.append(\"generator\")\n            if tchunk.metadata.get(\"is_export\"):\n                tags.append(\"export\")\n            if tchunk.metadata.get(\"has_generics\"):\n                tags.append(\"generic\")\n            if tchunk.metadata.get(\"is_component\"):\n                tags.append(\"component\")\n\n            # Add language tag\n            tags.append(tchunk.language)\n\n            # Create CodeChunk\n            chunk = CodeChunk(\n                file_path=str(path),\n                relative_path=(\n                    str(path.relative_to(self.root_path))\n                    if self.root_path\n                    else str(path)\n                ),\n                folder_structure=folder_parts,\n                chunk_type=chunk_type,\n                content=tchunk.content,\n                start_line=tchunk.start_line,\n                end_line=tchunk.end_line,\n                name=name,\n                parent_name=parent_name,\n                docstring=docstring,\n                decorators=decorators,\n                imports=[],  # Tree-sitter doesn't extract imports yet\n                complexity_score=0,  # Not calculated for tree-sitter chunks\n                tags=tags,\n            )\n\n            code_chunks.append(chunk)\n\n        return code_chunks\n\n    def chunk_directory(\n        self, directory_path: str, extensions: Optional[List[str]] = None\n    ) -> List[CodeChunk]:\n        \"\"\"Chunk all supported files in a directory.\n\n        Args:\n            directory_path: Path to directory\n            extensions: Optional list of extensions to process (default: all supported)\n\n        Returns:\n            List of CodeChunk objects from all files\n        \"\"\"\n        all_chunks = []\n        dir_path = Path(directory_path)\n\n        if not dir_path.exists() or not dir_path.is_dir():\n            logger.error(f\"Directory does not exist: {directory_path}\")\n            return []\n\n        # Use provided extensions or all supported\n        if extensions:\n            valid_extensions = set(extensions) & self.SUPPORTED_EXTENSIONS\n        else:\n            valid_extensions = self.SUPPORTED_EXTENSIONS\n\n        # Find all files with supported extensions\n        for ext in valid_extensions:\n            for file_path in dir_path.rglob(f\"*{ext}\"):\n                # Skip common large/build/tooling directories\n                if any(part in self.DEFAULT_IGNORED_DIRS for part in file_path.parts):\n                    continue\n\n                try:\n                    chunks = self.chunk_file(str(file_path))\n                    all_chunks.extend(chunks)\n                    logger.debug(f\"Chunked {len(chunks)} from {file_path}\")\n                except Exception as e:\n                    logger.warning(f\"Failed to chunk {file_path}: {e}\")\n\n        logger.info(f\"Total chunks from directory: {len(all_chunks)}\")\n        return all_chunks",
              "content_preview": "class MultiLanguageChunker:\n    \"\"\"Unified chunker supporting multiple programming languages.\"\"\"\n\n    # Supported extensions\n    SUPPORTED_EXTENSIONS = {\n        \".py\",  # Python\n        \".js\",  # Jav...",
              "project_name": "claude-context-local",
              "rrf_score": 0.009677419354838708,
              "appears_in_lists": 1,
              "final_rank": 4
            }
          },
          {
            "doc_id": "tests\\unit\\test_tree_sitter.py:177-187:method:test_language_detection",
            "score": 0.644,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_tree_sitter.py",
              "relative_path": "tests\\unit\\test_tree_sitter.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 177,
              "end_line": 187,
              "name": "test_language_detection",
              "parent_name": "TestTreeSitterChunker",
              "docstring": "Test language detection from file extension.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_language_detection(self):\n        \"\"\"Test language detection from file extension.\"\"\"\n        # Check if Python is supported (requires tree-sitter-python)\n        import chunking.tree_sitter as tsf\n\n        if \"python\" in tsf.AVAILABLE_LANGUAGES:\n            assert self.chunker.is_supported(\"test.py\")\n\n        # These won't be supported without their packages\n        assert not self.chunker.is_supported(\"test.txt\")\n        assert not self.chunker.is_supported(\"test.md\")",
              "content_preview": "def test_language_detection(self):\n        \"\"\"Test language detection from file extension.\"\"\"\n        # Check if Python is supported (requires tree-sitter-python)\n        import chunking.tree_sitter a...",
              "project_name": "claude-context-local",
              "rrf_score": 0.009523809523809523,
              "appears_in_lists": 1,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "tests\\unit\\test_multi_language.py:10-201:class:TestMultiLanguageChunker",
          "chunking\\tree_sitter.py:1110-1124:method:is_supported",
          "tests\\unit\\test_tree_sitter.py:177-187:method:test_language_detection",
          "chunking\\tree_sitter.py:1031-1066:method:get_chunker",
          "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
          "chunking\\tree_sitter.py:1126-1137:decorated_definition:get_supported_extensions",
          "tests\\unit\\test_tree_sitter.py:226-234:method:test_get_available_languages",
          "chunking\\multi_language_chunker.py:13-307:class:MultiLanguageChunker",
          "chunking\\multi_language_chunker.py:104-114:method:is_supported",
          "tests\\integration\\test_full_flow.py:27-30:decorated_definition:multi_lang_project_path"
        ]
      },
      "multi_hop": {
        "time_ms": 41.26,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
            "score": 11.073,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 704,
              "end_line": 746,
              "name": "test_multi_language_indexing",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test indexing and searching multi-language project.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_language_indexing(self, multi_lang_project_path, mock_storage_dir):\n        \"\"\"Test indexing and searching multi-language project.\"\"\"\n        # Step 1: Chunk the multi-language project\n        chunker = MultiLanguageChunker(str(multi_lang_project_path))\n        all_chunks = []\n\n        # Get all supported files\n        for ext in [\".py\", \".js\", \".ts\", \".jsx\", \".tsx\", \".svelte\"]:\n            for file_path in multi_lang_project_path.glob(f\"*{ext}\"):\n                chunks = chunker.chunk_file(str(file_path))\n                all_chunks.extend(chunks)\n\n        # Should find chunks from multiple languages\n        assert (\n            len(all_chunks) > 5\n        ), f\"Should chunk multiple files, got {len(all_chunks)}\"\n\n        # Verify we have chunks from different file types\n        file_extensions = {Path(chunk.file_path).suffix for chunk in all_chunks}\n        assert (\n            len(file_extensions) >= 3\n        ), f\"Should support multiple languages, got {file_extensions}\"\n\n        # Step 2: Create embeddings and index\n        embeddings = self._create_embeddings_from_chunks(all_chunks)\n\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(embeddings)\n\n        # Step 3: Test searching across languages\n        query_embedding = np.random.random(768).astype(np.float32)\n        results = index_manager.search(query_embedding, k=10)\n\n        assert len(results) > 0\n\n        # Verify we can find chunks from different languages\n        result_extensions = {\n            Path(metadata[\"file_path\"]).suffix for _, _, metadata in results\n        }\n        assert (\n            len(result_extensions) >= 2\n        ), f\"Should find results from multiple languages, got {result_extensions}\"",
              "content_preview": "def test_multi_language_indexing(self, multi_lang_project_path, mock_storage_dir):\n        \"\"\"Test indexing and searching multi-language project.\"\"\"\n        # Step 1: Chunk the multi-language project\n...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015932377049180328,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_full_flow.py:27-30:decorated_definition:multi_lang_project_path",
            "score": 9.174,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 27,
              "end_line": 30,
              "name": "multi_lang_project_path",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Path to the multi-language test project.",
              "decorators": [
                "@pytest.fixture"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@pytest.fixture\n    def multi_lang_project_path(self):\n        \"\"\"Path to the multi-language test project.\"\"\"\n        return Path(__file__).parent.parent / \"test_data\" / \"multi_language\"",
              "content_preview": "@pytest.fixture\n    def multi_lang_project_path(self):\n        \"\"\"Path to the multi-language test project.\"\"\"\n        return Path(__file__).parent.parent / \"test_data\" / \"multi_language\"",
              "project_name": "claude-context-local",
              "rrf_score": 0.01568238213399504,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "scripts\\verify_installation.py:350-391:method:test_tree_sitter_parsers",
            "score": 7.591,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\scripts\\verify_installation.py",
              "relative_path": "scripts\\verify_installation.py",
              "folder_structure": [
                "scripts"
              ],
              "chunk_type": "method",
              "start_line": 350,
              "end_line": 391,
              "name": "test_tree_sitter_parsers",
              "parent_name": "InstallationVerifier",
              "docstring": "Test tree-sitter parsers.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_tree_sitter_parsers(self) -> bool:\n        \"\"\"Test tree-sitter parsers.\"\"\"\n        print()\n        print(\"=== Tree-sitter Parsers Check ===\")\n        print()\n\n        parser_code = \"\"\"\ntry:\n    import tree_sitter_python\n    import tree_sitter_javascript\n    import tree_sitter_typescript\n    import tree_sitter_java\n    import tree_sitter_go\n    import tree_sitter_rust\n    import tree_sitter_c\n    import tree_sitter_cpp\n    import tree_sitter_c_sharp\n    import tree_sitter_glsl\n    print('All tree-sitter parsers available')\n    print('Supported: Python, JS, TS, Java, Go, Rust, C, C++, C#, GLSL')\nexcept ImportError as e:\n    print(f'Some parsers missing: {e}')\n    raise\n\"\"\"\n\n        success, output = self._run_python_test(parser_code, \"Multi-language Parsers\")\n\n        if success:\n            self._print_test_result(\n                \"Multi-language Parsers\",\n                True,\n                details=\"All tree-sitter parsers available\",\n            )\n            for line in output.strip().split(\"\\n\"):\n                if line.strip():\n                    print(f\"       {line.strip()}\")\n        else:\n            self._print_warning(\n                \"Multi-language Parsers\", \"Some tree-sitter parsers missing\", output\n            )\n\n        return True  # Don't fail the overall test for missing parsers",
              "content_preview": "def test_tree_sitter_parsers(self) -> bool:\n        \"\"\"Test tree-sitter parsers.\"\"\"\n        print()\n        print(\"=== Tree-sitter Parsers Check ===\")\n        print()\n\n        parser_code = \"\"\"\ntry:\n ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01287094547964113,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          },
          {
            "doc_id": "tests\\unit\\test_model_selection.py:131-185:class:TestCodeEmbedderModelSupport",
            "score": 4.408,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_model_selection.py",
              "relative_path": "tests\\unit\\test_model_selection.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "class",
              "start_line": 131,
              "end_line": 185,
              "name": "TestCodeEmbedderModelSupport",
              "parent_name": null,
              "docstring": "Test CodeEmbedder multi-model support.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestCodeEmbedderModelSupport:\n    \"\"\"Test CodeEmbedder multi-model support.\"\"\"\n\n    def test_default_model_is_gemma(self):\n        \"\"\"Test that default model is Gemma (backward compatibility).\"\"\"\n        embedder = CodeEmbedder()\n        assert \"gemma\" in embedder.model_name.lower()\n\n    def test_embedder_with_bge_m3(self):\n        \"\"\"Test creating embedder with BGE-M3.\"\"\"\n        embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n        assert embedder.model_name == \"BAAI/bge-m3\"\n\n    def test_get_supported_models(self):\n        \"\"\"Test getting list of supported models.\"\"\"\n        models = CodeEmbedder.get_supported_models()\n        assert isinstance(models, dict)\n        assert \"google/embeddinggemma-300m\" in models\n        assert \"BAAI/bge-m3\" in models\n\n    def test_model_config_detection_gemma(self):\n        \"\"\"Test automatic config detection for Gemma.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n        config = embedder._get_model_config()\n\n        assert config[\"dimension\"] == 768\n        assert config[\"prompt_name\"] == \"Retrieval-document\"\n\n    def test_model_config_detection_bge_m3(self):\n        \"\"\"Test automatic config detection for BGE-M3.\"\"\"\n        embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n        config = embedder._get_model_config()\n\n        assert config[\"dimension\"] == 1024\n        assert config[\"prompt_name\"] is None  # No prompts for BGE-M3\n\n    def test_model_config_detection_unknown_model(self):\n        \"\"\"Test fallback config for unknown models.\"\"\"\n        embedder = CodeEmbedder(model_name=\"unknown/test-model\")\n        config = embedder._get_model_config()\n\n        # Should fall back to defaults\n        assert config[\"dimension\"] == 768\n        assert config[\"prompt_name\"] is None\n\n    def test_model_config_caching(self):\n        \"\"\"Test that model config is cached after first access.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n\n        # First access\n        config1 = embedder._get_model_config()\n        # Second access should return cached version\n        config2 = embedder._get_model_config()\n\n        assert config1 is config2  # Same object reference",
              "content_preview": "class TestCodeEmbedderModelSupport:\n    \"\"\"Test CodeEmbedder multi-model support.\"\"\"\n\n    def test_default_model_is_gemma(self):\n        \"\"\"Test that default model is Gemma (backward compatibility).\"\"...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012310077519379846,
              "appears_in_lists": 2,
              "final_rank": 8
            }
          },
          {
            "doc_id": "tests\\integration\\test_glsl_without_embedder.py:13-137:function:test_glsl_indexing_without_embedder",
            "score": 3.608,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_glsl_without_embedder.py",
              "relative_path": "tests\\integration\\test_glsl_without_embedder.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "function",
              "start_line": 13,
              "end_line": 137,
              "name": "test_glsl_indexing_without_embedder",
              "parent_name": null,
              "docstring": "Test GLSL indexing without embedder dependencies.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_glsl_indexing_without_embedder():\n    \"\"\"Test GLSL indexing without embedder dependencies.\"\"\"\n    print(\"TESTING GLSL INDEXING WITHOUT EMBEDDER\")\n    print(\"=\" * 60)\n\n    try:\n        # Test 1: Direct chunker functionality\n        print(\"\\n1. TESTING MULTI-LANGUAGE CHUNKER\")\n        from chunking.multi_language_chunker import MultiLanguageChunker\n\n        project_path = str(Path(__file__).parent.parent / \"test_data\" / \"glsl_project\")\n        chunker = MultiLanguageChunker(project_path)\n\n        glsl_files = list(Path(project_path).glob(\"*\"))\n        supported_files = [f for f in glsl_files if chunker.is_supported(str(f))]\n\n        print(f\"   Total files in directory: {len(glsl_files)}\")\n        print(f\"   Supported GLSL files: {len(supported_files)}\")\n\n        total_chunks = 0\n        for file_path in supported_files:\n            chunks = chunker.chunk_file(str(file_path))\n            total_chunks += len(chunks)\n            print(f\"     {file_path.name}: {len(chunks)} chunks\")\n\n            for chunk in chunks:\n                print(\n                    f\"       - {chunk.chunk_type}: {chunk.name or 'unnamed'} (lines {chunk.start_line}-{chunk.end_line})\"\n                )\n\n        print(f\"   Total chunks generated: {total_chunks}\")\n\n        # Test 2: Test incremental indexer components without embedder\n        print(\"\\n2. TESTING INCREMENTAL INDEXER COMPONENTS\")\n\n        from merkle.merkle_dag import MerkleDAG\n\n        dag = MerkleDAG(project_path)\n        dag.build()\n        all_files = dag.get_all_files()\n        print(f\"   MerkleDAG found {len(all_files)} files: {all_files}\")\n\n        # Test filtering logic\n        supported_from_dag = [\n            f for f in all_files if chunker.is_supported(str(Path(project_path) / f))\n        ]\n        print(f\"   Supported files from DAG: {len(supported_from_dag)}\")\n        for f in supported_from_dag:\n            print(f\"     - {f}\")\n\n        # Test 3: Create mock indexer that skips embedder\n        print(\"\\n3. TESTING MOCK INDEXER WITHOUT EMBEDDER\")\n\n        class MockIndexer:\n            def __init__(self):\n                self.indexed_chunks = []\n\n            def add_chunks_without_embedding(self, chunks):\n                \"\"\"Add chunks without creating embeddings.\"\"\"\n                for chunk in chunks:\n                    self.indexed_chunks.append(\n                        {\n                            \"file_path\": chunk.file_path,\n                            \"chunk_type\": chunk.chunk_type,\n                            \"name\": chunk.name,\n                            \"content_preview\": (\n                                chunk.content[:100] + \"...\"\n                                if len(chunk.content) > 100\n                                else chunk.content\n                            ),\n                            \"start_line\": chunk.start_line,\n                            \"end_line\": chunk.end_line,\n                        }\n                    )\n                return len(chunks)\n\n            def get_stats(self):\n                return {\n                    \"total_chunks\": len(self.indexed_chunks),\n                    \"chunk_types\": list({c[\"chunk_type\"] for c in self.indexed_chunks}),\n                    \"files\": list({c[\"file_path\"] for c in self.indexed_chunks}),\n                }\n\n        mock_indexer = MockIndexer()\n\n        # Index all GLSL files\n        all_chunks = []\n        for file_path in supported_files:\n            chunks = chunker.chunk_file(str(file_path))\n            all_chunks.extend(chunks)\n\n        chunks_added = mock_indexer.add_chunks_without_embedding(all_chunks)\n        stats = mock_indexer.get_stats()\n\n        print(f\"   Mock indexer processed {chunks_added} chunks\")\n        print(f\"   Chunk types found: {stats['chunk_types']}\")\n        print(f\"   Files indexed: {len(stats['files'])}\")\n\n        # Test 4: Verify specific GLSL content\n        print(\"\\n4. TESTING GLSL CONTENT RECOGNITION\")\n\n        for chunk_info in mock_indexer.indexed_chunks:\n            print(f\"   {Path(chunk_info['file_path']).name}:\")\n            print(f\"     Type: {chunk_info['chunk_type']}\")\n            print(f\"     Name: {chunk_info['name']}\")\n            print(f\"     Lines: {chunk_info['start_line']}-{chunk_info['end_line']}\")\n            print(f\"     Preview: {chunk_info['content_preview']}\")\n            print()\n\n        # Convert to assertion for pytest compatibility\n        assert total_chunks > 0, f\"Expected GLSL chunks but got {total_chunks}\"\n        assert (\n            len(supported_files) > 0\n        ), f\"Expected supported GLSL files but got {len(supported_files)}\"\n        print(\n            f\"\\n[OK] SUCCESS: Found {len(supported_files)} supported files and {total_chunks} chunks\"\n        )\n\n    except Exception as e:\n        print(f\"ERROR: {e}\")\n        import traceback\n\n        traceback.print_exc()\n        # Convert to assertion failure for pytest compatibility\n        raise AssertionError(f\"GLSL indexing test failed: {e}\") from e",
              "content_preview": "def test_glsl_indexing_without_embedder():\n    \"\"\"Test GLSL indexing without embedder dependencies.\"\"\"\n    print(\"TESTING GLSL INDEXING WITHOUT EMBEDDER\")\n    print(\"=\" * 60)\n\n    try:\n        # Test ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01112383861524755,
              "appears_in_lists": 2,
              "final_rank": 10
            }
          }
        ],
        "all_doc_ids": [
          "tests\\unit\\test_multi_language.py:10-201:class:TestMultiLanguageChunker",
          "tests\\integration\\test_full_flow.py:19-746:class:TestFullSearchFlow",
          "chunking\\multi_language_chunker.py:116-138:method:chunk_file",
          "tests\\integration\\test_full_flow.py:704-746:method:test_multi_language_indexing",
          "tests\\unit\\test_model_selection.py:131-185:class:TestCodeEmbedderModelSupport",
          "chunking\\multi_language_chunker.py:13-307:class:MultiLanguageChunker",
          "scripts\\verify_installation.py:350-391:method:test_tree_sitter_parsers",
          "chunking\\multi_language_chunker.py:104-114:method:is_supported",
          "tests\\integration\\test_glsl_without_embedder.py:13-137:function:test_glsl_indexing_without_embedder",
          "tests\\integration\\test_full_flow.py:27-30:decorated_definition:multi_lang_project_path"
        ],
        "unique_discoveries": [
          "tests\\integration\\test_full_flow.py:19-746:class:TestFullSearchFlow",
          "tests\\unit\\test_model_selection.py:131-185:class:TestCodeEmbedderModelSupport",
          "scripts\\verify_installation.py:350-391:method:test_tree_sitter_parsers",
          "tests\\integration\\test_glsl_without_embedder.py:13-137:function:test_glsl_indexing_without_embedder",
          "chunking\\multi_language_chunker.py:116-138:method:chunk_file"
        ]
      },
      "comparison": {
        "time_overhead_ms": 10.46,
        "time_overhead_pct": 34.0,
        "top5_overlap_count": 2,
        "top5_overlap_pct": 40.0,
        "unique_discovery_count": 5,
        "value_rating": "HIGH"
      }
    },
    {
      "query": "incremental indexing with Merkle trees",
      "single_hop": {
        "time_ms": 19.19,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "merkle\\__init__.py:1-8:module",
            "score": 14.555,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\merkle\\__init__.py",
              "relative_path": "merkle\\__init__.py",
              "folder_structure": [
                "merkle"
              ],
              "chunk_type": "module",
              "start_line": 1,
              "end_line": 8,
              "name": null,
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "\"\"\"Merkle tree-based change detection for efficient incremental indexing.\"\"\"\n\nfrom .change_detector import ChangeDetector\nfrom .merkle_dag import MerkleDAG, MerkleNode\nfrom .snapshot_manager import SnapshotManager\n\n__all__ = [\"MerkleNode\", \"MerkleDAG\", \"SnapshotManager\", \"ChangeDetector\"]\n",
              "content_preview": "\"\"\"Merkle tree-based change detection for efficient incremental indexing.\"\"\"\n\nfrom .change_detector import ChangeDetector\nfrom .merkle_dag import MerkleDAG, MerkleNode\nfrom .snapshot_manager import Sn...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016234796404019036,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_full_flow.py:441-516:method:test_incremental_indexing_with_merkle",
            "score": 12.825,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 441,
              "end_line": 516,
              "name": "test_incremental_indexing_with_merkle",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test incremental indexing using Merkle tree change detection.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_incremental_indexing_with_merkle(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test incremental indexing using Merkle tree change detection.\"\"\"\n        # Initial indexing\n        chunker = MultiLanguageChunker(str(test_project_path))\n        initial_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            initial_chunks.extend(chunks)\n\n        initial_embeddings = self._create_embeddings_from_chunks(initial_chunks)\n\n        # Create initial index\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(initial_embeddings)\n\n        initial_count = len(index_manager._chunk_ids)\n\n        # Save the initial index\n        index_manager.save_index()\n\n        # Create Merkle snapshot manager and save initial state\n        snapshot_manager = SnapshotManager(str(mock_storage_dir))\n        merkle_dag = MerkleDAG(str(test_project_path))\n        merkle_dag.build()  # Build the DAG first\n        snapshot_manager.save_snapshot(merkle_dag)\n\n        # Simulate file changes by creating a temporary modified project\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_project = Path(temp_dir) / \"modified_project\"\n            shutil.copytree(test_project_path, temp_project)\n\n            # Modify a file to trigger incremental update\n            auth_file = temp_project / \"src\" / \"auth\" / \"authenticator.py\"\n            if auth_file.exists():\n                content = auth_file.read_text()\n                # Add a new function\n                new_function = \"\\n\\ndef new_auth_function():\\n    '''New authentication function.'''\\n    return True\\n\"\n                auth_file.write_text(content + new_function)\n\n            # Create new DAG for modified project\n            new_dag = MerkleDAG(str(temp_project))\n            new_dag.build()  # Build the DAG first\n\n            # Detect changes using ChangeDetector\n            detector = ChangeDetector()\n            changes = detector.detect_changes(merkle_dag, new_dag)\n\n            # Should detect at least one modified file\n            assert len(changes.modified) > 0 or len(changes.added) > 0\n\n            # Process only changed files (incremental indexing)\n            # Create a new chunker for the temp project\n            temp_chunker = MultiLanguageChunker(str(temp_project))\n            changed_chunks = []\n            for file_path in changes.modified + changes.added:\n                # The file_path from MerkleDAG is relative, construct full path\n                full_path = temp_project / file_path\n                if full_path.exists():\n                    chunks = temp_chunker.chunk_file(str(full_path))\n                    changed_chunks.extend(chunks)\n\n            # Should have found new chunks\n            assert len(changed_chunks) > 0\n\n            # Create embeddings for changed chunks\n            new_embeddings = self._create_embeddings_from_chunks(changed_chunks)\n\n            # Add new embeddings incrementally\n            index_manager.add_embeddings(new_embeddings)\n\n            # Should have more chunks now\n            assert len(index_manager._chunk_ids) > initial_count",
              "content_preview": "def test_incremental_indexing_with_merkle(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test incremental indexing using Merkle tree change detection.\"\"\"\n        # Initial indexi...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016185271922976842,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "mcp_server\\server.py:641-761:decorated_definition:index_directory",
            "score": 13.398,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 641,
              "end_line": 761,
              "name": "index_directory",
              "parent_name": null,
              "docstring": "SETUP REQUIRED: Index a codebase for semantic search. Must run this before\n    using search_code on a new project. Supports Python, JavaScript, TypeScript, JSX, TSX, and Svelte.\n\n    WHEN TO USE:\n    - First time analyzing a new codebase\n    - After significant code changes that might affect search results\n    - When switching to a different project\n\n    PROCESS:\n    - Uses Merkle trees to detect file changes efficiently\n    - Only reprocesses changed/new files (incremental mode)\n    - Parses code files using AST (Python) and tree-sitter (JS/TS/JSX/TSX/Svelte)\n    - Chunks code into semantic units (functions, classes, methods)\n    - Generates 768-dimensional embeddings using EmbeddingGemma-300m\n    - Builds FAISS vector index for fast similarity search\n    - Stores metadata in SQLite database\n\n    Args:\n        directory_path: Absolute path to project root\n        project_name: Optional name for organization (defaults to directory name)\n        file_patterns: File patterns to include (default: all supported extensions)\n        incremental: Use incremental indexing if snapshot exists (default: True)\n\n    Returns:\n        JSON with indexing statistics and success status\n\n    Note: Incremental indexing is much faster for updates. Full reindex on first run.",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef index_directory(\n    directory_path: str,\n    project_name: str = None,\n    file_patterns: List[str] = None,\n    incremental: bool = True,\n) -> str:\n    \"\"\"\n    SETUP REQUIRED: Index a codebase for semantic search. Must run this before\n    using search_code on a new project. Supports Python, JavaScript, TypeScript, JSX, TSX, and Svelte.\n\n    WHEN TO USE:\n    - First time analyzing a new codebase\n    - After significant code changes that might affect search results\n    - When switching to a different project\n\n    PROCESS:\n    - Uses Merkle trees to detect file changes efficiently\n    - Only reprocesses changed/new files (incremental mode)\n    - Parses code files using AST (Python) and tree-sitter (JS/TS/JSX/TSX/Svelte)\n    - Chunks code into semantic units (functions, classes, methods)\n    - Generates 768-dimensional embeddings using EmbeddingGemma-300m\n    - Builds FAISS vector index for fast similarity search\n    - Stores metadata in SQLite database\n\n    Args:\n        directory_path: Absolute path to project root\n        project_name: Optional name for organization (defaults to directory name)\n        file_patterns: File patterns to include (default: all supported extensions)\n        incremental: Use incremental indexing if snapshot exists (default: True)\n\n    Returns:\n        JSON with indexing statistics and success status\n\n    Note: Incremental indexing is much faster for updates. Full reindex on first run.\n    \"\"\"\n    try:\n        from search.incremental_indexer import IncrementalIndexer\n\n        # Start model preload early to overlap with Merkle/IO work\n        _maybe_start_model_preload()\n\n        directory_path = Path(directory_path).resolve()\n        if not directory_path.exists():\n            return json.dumps({\"error\": f\"Directory does not exist: {directory_path}\"})\n\n        if not directory_path.is_dir():\n            return json.dumps({\"error\": f\"Path is not a directory: {directory_path}\"})\n\n        project_name = project_name or directory_path.name\n        logger.info(f\"Indexing directory: {directory_path} (incremental={incremental})\")\n\n        # Initialize incremental indexer - use HybridSearcher if hybrid search is enabled\n        config = get_search_config()\n        if config.enable_hybrid_search:\n            # Use HybridSearcher for indexing when hybrid search is enabled\n            project_storage = get_project_storage_dir(str(directory_path))\n            storage_dir = project_storage / \"index\"\n            indexer = HybridSearcher(\n                storage_dir=str(storage_dir),\n                embedder=get_embedder(),\n                bm25_weight=config.bm25_weight,\n                dense_weight=config.dense_weight,\n                rrf_k=config.rrf_k_parameter,\n                max_workers=2,\n            )\n            logger.info(\n                \"Using HybridSearcher for indexing to populate both BM25 and dense indices\"\n            )\n        else:\n            indexer = get_index_manager(str(directory_path))\n            logger.info(\"Using CodeIndexManager for dense-only indexing\")\n\n        embedder = get_embedder()\n        chunker = MultiLanguageChunker(str(directory_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer, embedder=embedder, chunker=chunker\n        )\n\n        # Update current project tracker (CRITICAL for multi-project isolation - Bug #2 fix)\n        global _current_project\n        _current_project = str(directory_path)\n        logger.info(\n            f\"[PER_MODEL_INDICES] Updated _current_project to: {_current_project}\"\n        )\n\n        # Perform indexing\n        result = incremental_indexer.incremental_index(\n            str(directory_path), project_name, force_full=not incremental\n        )\n\n        # Get updated statistics\n        stats = incremental_indexer.get_indexing_stats(str(directory_path))\n\n        response = {\n            \"success\": result.success,\n            \"directory\": str(directory_path),\n            \"project_name\": project_name,\n            \"incremental\": incremental and result.files_modified > 0,\n            \"files_added\": result.files_added,\n            \"files_removed\": result.files_removed,\n            \"files_modified\": result.files_modified,\n            \"chunks_added\": result.chunks_added,\n            \"chunks_removed\": result.chunks_removed,\n            \"time_taken\": round(result.time_taken, 2),\n            \"index_stats\": stats,\n        }\n\n        if result.error:\n            response[\"error\"] = result.error\n\n        logger.info(\n            f\"Indexing completed. Added: {result.files_added}, Modified: {result.files_modified}, Time: {result.time_taken:.2f}s\"\n        )\n        return json.dumps(response, indent=2)\n\n    except (OSError, IOError, ValueError, TypeError, RuntimeError, MemoryError) as e:\n        error_msg = f\"Indexing failed: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})",
              "content_preview": "@mcp.tool()\ndef index_directory(\n    directory_path: str,\n    project_name: str = None,\n    file_patterns: List[str] = None,\n    incremental: bool = True,\n) -> str:\n    \"\"\"\n    SETUP REQUIRED: Index a...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01597542242703533,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "tools\\batch_index.py:19-137:function:main",
            "score": 11.178,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tools\\batch_index.py",
              "relative_path": "tools\\batch_index.py",
              "folder_structure": [
                "tools"
              ],
              "chunk_type": "function",
              "start_line": 19,
              "end_line": 137,
              "name": "main",
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Index project for semantic code search\"\n    )\n    parser.add_argument(\n        \"--path\",\n        required=True,\n        help=\"Path to project directory\"\n    )\n    parser.add_argument(\n        \"--mode\",\n        required=True,\n        choices=[\"new\", \"incremental\", \"force\"],\n        help=\"Indexing mode: new (first-time), incremental (change detection), force (full reindex)\"\n    )\n\n    args = parser.parse_args()\n\n    # Validate path\n    project_path = Path(args.path)\n    if not project_path.exists():\n        print(f\"[ERROR] Path does not exist: {project_path}\")\n        return 1\n\n    if not project_path.is_dir():\n        print(f\"[ERROR] Path is not a directory: {project_path}\")\n        return 1\n\n    # Determine incremental mode\n    if args.mode == \"incremental\":\n        incremental = True\n        mode_desc = \"Incremental (change detection with Merkle tree)\"\n    else:  # new or force\n        incremental = False\n        if args.mode == \"new\":\n            mode_desc = \"New (first-time full index)\"\n        else:\n            mode_desc = \"Force (full reindex, bypass snapshot)\"\n\n    # Display configuration\n    print(\"=\" * 70)\n    print(\"PROJECT INDEXING\")\n    print(\"=\" * 70)\n    print(f\"Path: {project_path}\")\n    print(f\"Mode: {mode_desc}\")\n    print(f\"Incremental: {incremental}\")\n    print(\"=\" * 70)\n    print()\n\n    # Start indexing\n    start_time = time.time()\n\n    try:\n        print(\"[INFO] Starting indexing...\")\n        print()\n\n        result_json = index_directory(\n            str(project_path),\n            incremental=incremental\n        )\n\n        # Parse JSON string to dictionary\n        result = json.loads(result_json)\n\n        elapsed = time.time() - start_time\n\n        # Display results\n        print()\n        print(\"=\" * 70)\n        print(\"INDEXING RESULTS\")\n        print(\"=\" * 70)\n\n        if result.get(\"success\"):\n            print(\"[OK] Indexing completed successfully\")\n            print()\n            print(f\"Project: {result.get('project_name', 'Unknown')}\")\n            print(f\"Directory: {result.get('directory', project_path)}\")\n            print(f\"Mode: {'Incremental' if result.get('incremental') else 'Full'}\")\n            print()\n            print(f\"Files added: {result.get('files_added', 0)}\")\n            print(f\"Files removed: {result.get('files_removed', 0)}\")\n            print(f\"Files modified: {result.get('files_modified', 0)}\")\n            print()\n            print(f\"Chunks added: {result.get('chunks_added', 0)}\")\n            print(f\"Chunks removed: {result.get('chunks_removed', 0)}\")\n            print()\n            print(f\"Time taken: {result.get('time_taken', elapsed):.2f} seconds\")\n\n            # Show index statistics\n            index_stats = result.get('index_stats', {})\n            if index_stats:\n                print()\n                print(\"Index Statistics:\")\n                print(f\"  Total files: {index_stats.get('total_files', 0)}\")\n                print(f\"  Supported files: {index_stats.get('supported_files', 0)}\")\n                print(f\"  Total chunks: {index_stats.get('chunks_indexed', 0)}\")\n\n            print(\"=\" * 70)\n            return 0\n\n        else:\n            print(\"[ERROR] Indexing failed\")\n            error = result.get('error', 'Unknown error')\n            print(f\"Error: {error}\")\n            print(\"=\" * 70)\n            return 1\n\n    except Exception as e:\n        elapsed = time.time() - start_time\n        print()\n        print(\"=\" * 70)\n        print(\"[ERROR] Indexing failed with exception\")\n        print(\"=\" * 70)\n        print(f\"Error: {str(e)}\")\n        print(f\"Time elapsed: {elapsed:.2f} seconds\")\n        print(\"=\" * 70)\n        import traceback\n        traceback.print_exc()\n        return 1",
              "content_preview": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Index project for semantic code search\"\n    )\n    parser.add_argument(\n        \"--path\",\n        required=True,\n        help=\"Pat...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015151515151515152,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "tests\\integration\\test_direct_indexing.py:13-101:function:test_direct_indexing",
            "score": 11.111,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_direct_indexing.py",
              "relative_path": "tests\\integration\\test_direct_indexing.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "function",
              "start_line": 13,
              "end_line": 101,
              "name": "test_direct_indexing",
              "parent_name": null,
              "docstring": "Test the incremental indexer directly.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_direct_indexing():\n    \"\"\"Test the incremental indexer directly.\"\"\"\n    from chunking.multi_language_chunker import MultiLanguageChunker\n    from search.indexer import CodeIndexManager\n\n    project_path = str(Path(__file__).parent.parent.parent / \"test_glsl_dir\")\n    project_name = \"DirectTest\"\n\n    print(\"TESTING DIRECT INCREMENTAL INDEXING\")\n    print(f\"Project path: {project_path}\")\n    print(\"=\" * 60)\n\n    try:\n        # Initialize components\n        print(\"\\n1. INITIALIZING COMPONENTS\")\n        CodeIndexManager(storage_dir=\"C:/Users/Inter/.claude_code_search\")\n        print(\"   Index manager initialized\")\n\n        # Skip embedder for now to avoid dependency issues\n        print(\"   Skipping embedder initialization for debugging\")\n\n        chunker = MultiLanguageChunker(project_path)\n        print(f\"   Chunker initialized with root: {project_path}\")\n\n        # Test the incremental indexer manually\n        print(\"\\n2. TESTING INCREMENTAL INDEXER COMPONENTS\")\n\n        # Test MerkleDAG\n        from merkle.merkle_dag import MerkleDAG\n\n        dag = MerkleDAG(project_path)\n        dag.build()\n        all_files = dag.get_all_files()\n        print(f\"   MerkleDAG found {len(all_files)} files: {all_files}\")\n\n        # Test filtering with the fixed logic\n        supported_files = [\n            f for f in all_files if chunker.is_supported(str(Path(project_path) / f))\n        ]\n        print(f\"   Supported files: {len(supported_files)}\")\n        for sf in supported_files:\n            print(f\"     - {sf}\")\n\n        # Test chunking\n        print(\"\\n3. TESTING CHUNKING\")\n        all_chunks = []\n        for file_path in supported_files:\n            full_path = Path(project_path) / file_path\n            try:\n                chunks = chunker.chunk_file(str(full_path))\n                if chunks:\n                    all_chunks.extend(chunks)\n                    print(f\"   {file_path}: {len(chunks)} chunks\")\n                    for chunk in chunks:\n                        print(f\"     - {chunk.chunk_type}: {chunk.name or 'unnamed'}\")\n                else:\n                    print(f\"   {file_path}: No chunks generated\")\n            except Exception as e:\n                print(f\"   {file_path}: ERROR - {e}\")\n                import traceback\n\n                traceback.print_exc()\n\n        print(f\"\\n   Total chunks: {len(all_chunks)}\")\n\n        # Test what would be saved to snapshot\n        print(\"\\n4. TESTING SNAPSHOT METADATA\")\n        metadata = {\n            \"project_name\": project_name,\n            \"full_index\": True,\n            \"total_files\": len(all_files),\n            \"supported_files\": len(supported_files),\n            \"chunks_indexed\": len(all_chunks),\n        }\n        print(f\"   Metadata that would be saved: {metadata}\")\n\n        return {\n            \"all_files\": all_files,\n            \"supported_files\": supported_files,\n            \"chunks\": all_chunks,\n            \"metadata\": metadata,\n        }\n\n    except Exception as e:\n        print(f\"ERROR: {e}\")\n        import traceback\n\n        traceback.print_exc()\n        return None",
              "content_preview": "def test_direct_indexing():\n    \"\"\"Test the incremental indexer directly.\"\"\"\n    from chunking.multi_language_chunker import MultiLanguageChunker\n    from search.indexer import CodeIndexManager\n\n    p...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014793678665496048,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "merkle\\__init__.py:1-8:module",
          "tools\\batch_index.py:19-137:function:main",
          "tests\\integration\\test_hybrid_search_integration.py:201-219:decorated_definition:test_incremental_indexing_with_hybrid_search",
          "mcp_server\\server.py:641-761:decorated_definition:index_directory",
          "search\\incremental_indexer.py:221-367:method:_full_index",
          "tests\\integration\\test_full_flow.py:441-516:method:test_incremental_indexing_with_merkle",
          "tests\\integration\\test_direct_indexing.py:13-101:function:test_direct_indexing",
          "search\\incremental_indexer.py:47-552:class:IncrementalIndexer",
          "tests\\integration\\test_mcp_indexing.py:98-166:method:test_incremental_indexing_mcp_path",
          "tests\\integration\\test_incremental_indexing.py:25-364:class:TestIncrementalIndexing"
        ]
      },
      "multi_hop": {
        "time_ms": 43.04,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "merkle\\__init__.py:1-8:module",
            "score": 14.555,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\merkle\\__init__.py",
              "relative_path": "merkle\\__init__.py",
              "folder_structure": [
                "merkle"
              ],
              "chunk_type": "module",
              "start_line": 1,
              "end_line": 8,
              "name": null,
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "\"\"\"Merkle tree-based change detection for efficient incremental indexing.\"\"\"\n\nfrom .change_detector import ChangeDetector\nfrom .merkle_dag import MerkleDAG, MerkleNode\nfrom .snapshot_manager import SnapshotManager\n\n__all__ = [\"MerkleNode\", \"MerkleDAG\", \"SnapshotManager\", \"ChangeDetector\"]\n",
              "content_preview": "\"\"\"Merkle tree-based change detection for efficient incremental indexing.\"\"\"\n\nfrom .change_detector import ChangeDetector\nfrom .merkle_dag import MerkleDAG, MerkleNode\nfrom .snapshot_manager import Sn...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016234796404019036,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "mcp_server\\server.py:641-761:decorated_definition:index_directory",
            "score": 13.398,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 641,
              "end_line": 761,
              "name": "index_directory",
              "parent_name": null,
              "docstring": "SETUP REQUIRED: Index a codebase for semantic search. Must run this before\n    using search_code on a new project. Supports Python, JavaScript, TypeScript, JSX, TSX, and Svelte.\n\n    WHEN TO USE:\n    - First time analyzing a new codebase\n    - After significant code changes that might affect search results\n    - When switching to a different project\n\n    PROCESS:\n    - Uses Merkle trees to detect file changes efficiently\n    - Only reprocesses changed/new files (incremental mode)\n    - Parses code files using AST (Python) and tree-sitter (JS/TS/JSX/TSX/Svelte)\n    - Chunks code into semantic units (functions, classes, methods)\n    - Generates 768-dimensional embeddings using EmbeddingGemma-300m\n    - Builds FAISS vector index for fast similarity search\n    - Stores metadata in SQLite database\n\n    Args:\n        directory_path: Absolute path to project root\n        project_name: Optional name for organization (defaults to directory name)\n        file_patterns: File patterns to include (default: all supported extensions)\n        incremental: Use incremental indexing if snapshot exists (default: True)\n\n    Returns:\n        JSON with indexing statistics and success status\n\n    Note: Incremental indexing is much faster for updates. Full reindex on first run.",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef index_directory(\n    directory_path: str,\n    project_name: str = None,\n    file_patterns: List[str] = None,\n    incremental: bool = True,\n) -> str:\n    \"\"\"\n    SETUP REQUIRED: Index a codebase for semantic search. Must run this before\n    using search_code on a new project. Supports Python, JavaScript, TypeScript, JSX, TSX, and Svelte.\n\n    WHEN TO USE:\n    - First time analyzing a new codebase\n    - After significant code changes that might affect search results\n    - When switching to a different project\n\n    PROCESS:\n    - Uses Merkle trees to detect file changes efficiently\n    - Only reprocesses changed/new files (incremental mode)\n    - Parses code files using AST (Python) and tree-sitter (JS/TS/JSX/TSX/Svelte)\n    - Chunks code into semantic units (functions, classes, methods)\n    - Generates 768-dimensional embeddings using EmbeddingGemma-300m\n    - Builds FAISS vector index for fast similarity search\n    - Stores metadata in SQLite database\n\n    Args:\n        directory_path: Absolute path to project root\n        project_name: Optional name for organization (defaults to directory name)\n        file_patterns: File patterns to include (default: all supported extensions)\n        incremental: Use incremental indexing if snapshot exists (default: True)\n\n    Returns:\n        JSON with indexing statistics and success status\n\n    Note: Incremental indexing is much faster for updates. Full reindex on first run.\n    \"\"\"\n    try:\n        from search.incremental_indexer import IncrementalIndexer\n\n        # Start model preload early to overlap with Merkle/IO work\n        _maybe_start_model_preload()\n\n        directory_path = Path(directory_path).resolve()\n        if not directory_path.exists():\n            return json.dumps({\"error\": f\"Directory does not exist: {directory_path}\"})\n\n        if not directory_path.is_dir():\n            return json.dumps({\"error\": f\"Path is not a directory: {directory_path}\"})\n\n        project_name = project_name or directory_path.name\n        logger.info(f\"Indexing directory: {directory_path} (incremental={incremental})\")\n\n        # Initialize incremental indexer - use HybridSearcher if hybrid search is enabled\n        config = get_search_config()\n        if config.enable_hybrid_search:\n            # Use HybridSearcher for indexing when hybrid search is enabled\n            project_storage = get_project_storage_dir(str(directory_path))\n            storage_dir = project_storage / \"index\"\n            indexer = HybridSearcher(\n                storage_dir=str(storage_dir),\n                embedder=get_embedder(),\n                bm25_weight=config.bm25_weight,\n                dense_weight=config.dense_weight,\n                rrf_k=config.rrf_k_parameter,\n                max_workers=2,\n            )\n            logger.info(\n                \"Using HybridSearcher for indexing to populate both BM25 and dense indices\"\n            )\n        else:\n            indexer = get_index_manager(str(directory_path))\n            logger.info(\"Using CodeIndexManager for dense-only indexing\")\n\n        embedder = get_embedder()\n        chunker = MultiLanguageChunker(str(directory_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer, embedder=embedder, chunker=chunker\n        )\n\n        # Update current project tracker (CRITICAL for multi-project isolation - Bug #2 fix)\n        global _current_project\n        _current_project = str(directory_path)\n        logger.info(\n            f\"[PER_MODEL_INDICES] Updated _current_project to: {_current_project}\"\n        )\n\n        # Perform indexing\n        result = incremental_indexer.incremental_index(\n            str(directory_path), project_name, force_full=not incremental\n        )\n\n        # Get updated statistics\n        stats = incremental_indexer.get_indexing_stats(str(directory_path))\n\n        response = {\n            \"success\": result.success,\n            \"directory\": str(directory_path),\n            \"project_name\": project_name,\n            \"incremental\": incremental and result.files_modified > 0,\n            \"files_added\": result.files_added,\n            \"files_removed\": result.files_removed,\n            \"files_modified\": result.files_modified,\n            \"chunks_added\": result.chunks_added,\n            \"chunks_removed\": result.chunks_removed,\n            \"time_taken\": round(result.time_taken, 2),\n            \"index_stats\": stats,\n        }\n\n        if result.error:\n            response[\"error\"] = result.error\n\n        logger.info(\n            f\"Indexing completed. Added: {result.files_added}, Modified: {result.files_modified}, Time: {result.time_taken:.2f}s\"\n        )\n        return json.dumps(response, indent=2)\n\n    except (OSError, IOError, ValueError, TypeError, RuntimeError, MemoryError) as e:\n        error_msg = f\"Indexing failed: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})",
              "content_preview": "@mcp.tool()\ndef index_directory(\n    directory_path: str,\n    project_name: str = None,\n    file_patterns: List[str] = None,\n    incremental: bool = True,\n) -> str:\n    \"\"\"\n    SETUP REQUIRED: Index a...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01597542242703533,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "tests\\integration\\test_full_flow.py:441-516:method:test_incremental_indexing_with_merkle",
            "score": 12.825,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_full_flow.py",
              "relative_path": "tests\\integration\\test_full_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 441,
              "end_line": 516,
              "name": "test_incremental_indexing_with_merkle",
              "parent_name": "TestFullSearchFlow",
              "docstring": "Test incremental indexing using Merkle tree change detection.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_incremental_indexing_with_merkle(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test incremental indexing using Merkle tree change detection.\"\"\"\n        # Initial indexing\n        chunker = MultiLanguageChunker(str(test_project_path))\n        initial_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            initial_chunks.extend(chunks)\n\n        initial_embeddings = self._create_embeddings_from_chunks(initial_chunks)\n\n        # Create initial index\n        index_manager = CodeIndexManager(str(mock_storage_dir))\n        index_manager.create_index(768, \"flat\")\n        index_manager.add_embeddings(initial_embeddings)\n\n        initial_count = len(index_manager._chunk_ids)\n\n        # Save the initial index\n        index_manager.save_index()\n\n        # Create Merkle snapshot manager and save initial state\n        snapshot_manager = SnapshotManager(str(mock_storage_dir))\n        merkle_dag = MerkleDAG(str(test_project_path))\n        merkle_dag.build()  # Build the DAG first\n        snapshot_manager.save_snapshot(merkle_dag)\n\n        # Simulate file changes by creating a temporary modified project\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_project = Path(temp_dir) / \"modified_project\"\n            shutil.copytree(test_project_path, temp_project)\n\n            # Modify a file to trigger incremental update\n            auth_file = temp_project / \"src\" / \"auth\" / \"authenticator.py\"\n            if auth_file.exists():\n                content = auth_file.read_text()\n                # Add a new function\n                new_function = \"\\n\\ndef new_auth_function():\\n    '''New authentication function.'''\\n    return True\\n\"\n                auth_file.write_text(content + new_function)\n\n            # Create new DAG for modified project\n            new_dag = MerkleDAG(str(temp_project))\n            new_dag.build()  # Build the DAG first\n\n            # Detect changes using ChangeDetector\n            detector = ChangeDetector()\n            changes = detector.detect_changes(merkle_dag, new_dag)\n\n            # Should detect at least one modified file\n            assert len(changes.modified) > 0 or len(changes.added) > 0\n\n            # Process only changed files (incremental indexing)\n            # Create a new chunker for the temp project\n            temp_chunker = MultiLanguageChunker(str(temp_project))\n            changed_chunks = []\n            for file_path in changes.modified + changes.added:\n                # The file_path from MerkleDAG is relative, construct full path\n                full_path = temp_project / file_path\n                if full_path.exists():\n                    chunks = temp_chunker.chunk_file(str(full_path))\n                    changed_chunks.extend(chunks)\n\n            # Should have found new chunks\n            assert len(changed_chunks) > 0\n\n            # Create embeddings for changed chunks\n            new_embeddings = self._create_embeddings_from_chunks(changed_chunks)\n\n            # Add new embeddings incrementally\n            index_manager.add_embeddings(new_embeddings)\n\n            # Should have more chunks now\n            assert len(index_manager._chunk_ids) > initial_count",
              "content_preview": "def test_incremental_indexing_with_merkle(\n        self, test_project_path, mock_storage_dir\n    ):\n        \"\"\"Test incremental indexing using Merkle tree change detection.\"\"\"\n        # Initial indexi...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016185271922976842,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\integration\\test_incremental_indexing.py:25-364:class:TestIncrementalIndexing",
            "score": 12.37,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_incremental_indexing.py",
              "relative_path": "tests\\integration\\test_incremental_indexing.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "class",
              "start_line": 25,
              "end_line": 364,
              "name": "TestIncrementalIndexing",
              "parent_name": null,
              "docstring": "Test incremental indexing functionality.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestIncrementalIndexing(TestCase):\n    \"\"\"Test incremental indexing functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.test_path = Path(self.temp_dir) / \"test_project\"\n        self.test_path.mkdir()\n\n        # Storage for snapshots\n        self.snapshot_dir = Path(self.temp_dir) / \"snapshots\"\n        self.snapshot_manager = SnapshotManager(self.snapshot_dir)\n\n        # Storage for index\n        self.index_dir = Path(self.temp_dir) / \"index\"\n        self.index_dir.mkdir()\n\n        self.create_initial_codebase()\n\n    def tearDown(self):\n        \"\"\"Clean up test environment.\"\"\"\n        import shutil\n\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def create_initial_codebase(self):\n        \"\"\"Create initial Python codebase.\"\"\"\n        # Main module\n        (self.test_path / \"main.py\").write_text(\n            '''\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Hello World\")\n    return 0\n\nif __name__ == \"__main__\":\n    main()\n'''\n        )\n\n        # Utils module\n        (self.test_path / \"utils.py\").write_text(\n            '''\ndef helper(x, y):\n    \"\"\"Helper function.\"\"\"\n    return x + y\n\nclass Calculator:\n    \"\"\"Simple calculator.\"\"\"\n\n    def add(self, a, b):\n        return a + b\n\n    def subtract(self, a, b):\n        return a - b\n'''\n        )\n\n        # Create subdirectory\n        (self.test_path / \"lib\").mkdir()\n        (self.test_path / \"lib\" / \"database.py\").write_text(\n            '''\nclass Database:\n    \"\"\"Database connection.\"\"\"\n\n    def connect(self):\n        \"\"\"Connect to database.\"\"\"\n        pass\n\n    def query(self, sql):\n        \"\"\"Execute query.\"\"\"\n        return []\n'''\n        )\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_full_index(self):\n        \"\"\"Test full indexing of a codebase.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # First index should be full\n        result = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result.success\n        assert result.files_added > 0\n        assert result.chunks_added > 0\n        assert result.files_removed == 0\n        assert result.files_modified == 0\n\n        # Verify snapshot was created\n        assert self.snapshot_manager.has_snapshot(str(self.test_path))\n\n    def test_no_changes(self):\n        \"\"\"Test indexing when no changes occur.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # First index\n        result1 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result1.success\n\n        # Second index with no changes\n        result2 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result2.success\n        assert result2.files_added == 0\n        assert result2.files_removed == 0\n        assert result2.files_modified == 0\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_file_modification(self):\n        \"\"\"Test incremental indexing when files are modified.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # Initial index\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Modify a file\n        (self.test_path / \"utils.py\").write_text(\n            '''\ndef helper(x, y):\n    \"\"\"Updated helper function.\"\"\"\n    return x * y  # Changed from addition to multiplication\n\nclass Calculator:\n    \"\"\"Enhanced calculator.\"\"\"\n\n    def add(self, a, b):\n        return a + b\n\n    def multiply(self, a, b):\n        \"\"\"New method.\"\"\"\n        return a * b\n\n    def subtract(self, a, b):\n        return a - b\n'''\n        )\n\n        # Incremental index\n        result2 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result2.success\n        assert result2.files_modified == 1\n        assert result2.files_added == 0\n        assert result2.files_removed == 0\n        # Should have removed old chunks and added new ones\n        assert result2.chunks_removed > 0\n        assert result2.chunks_added > 0\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_file_addition(self):\n        \"\"\"Test incremental indexing when files are added.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # Initial index\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Add a new file\n        (self.test_path / \"new_module.py\").write_text(\n            '''\ndef new_function():\n    \"\"\"A new function.\"\"\"\n    return \"new\"\n\nclass NewClass:\n    \"\"\"A new class.\"\"\"\n    pass\n'''\n        )\n\n        # Incremental index\n        result2 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result2.success\n        assert result2.files_added == 1\n        assert result2.files_removed == 0\n        assert result2.files_modified == 0\n        assert result2.chunks_added > 0\n\n    @pytest.mark.skipif(not _has_hf_token(), reason=\"HuggingFace token not available\")\n    def test_file_deletion(self):\n        \"\"\"Test incremental indexing when files are deleted.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # Initial index\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Delete a file\n        (self.test_path / \"utils.py\").unlink()\n\n        # Incremental index\n        result2 = incremental_indexer.incremental_index(\n            str(self.test_path), \"test_project\"\n        )\n\n        assert result2.success\n        assert result2.files_added == 0\n        assert result2.files_removed == 1\n        assert result2.files_modified == 0\n        assert result2.chunks_removed > 0\n\n    def test_change_detection(self):\n        \"\"\"Test change detection using Merkle trees.\"\"\"\n        detector = ChangeDetector(self.snapshot_manager)\n\n        # Build initial DAG\n        dag1 = MerkleDAG(str(self.test_path))\n        dag1.build()\n        self.snapshot_manager.save_snapshot(dag1)\n\n        # No changes should be detected\n        assert not detector.quick_check(str(self.test_path))\n\n        # Modify a file\n        time.sleep(0.1)  # Ensure different timestamp\n        (self.test_path / \"main.py\").write_text(\"# Modified\\n\")\n\n        # Changes should be detected\n        assert detector.quick_check(str(self.test_path))\n\n        # Get detailed changes\n        changes, new_dag = detector.detect_changes_from_snapshot(str(self.test_path))\n\n        assert changes.has_changes()\n        assert \"main.py\" in changes.modified\n\n    def test_needs_reindex(self):\n        \"\"\"Test checking if reindex is needed.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # Should need index initially\n        assert incremental_indexer.needs_reindex(str(self.test_path))\n\n        # Index the project\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Should not need reindex immediately\n        assert not incremental_indexer.needs_reindex(\n            str(self.test_path), max_age_minutes=1440\n        )  # 24 hours\n\n        # Modify a file\n        (self.test_path / \"main.py\").write_text(\"# Changed\\n\")\n\n        # Should need reindex after change\n        assert incremental_indexer.needs_reindex(str(self.test_path))\n\n    def test_indexing_stats(self):\n        \"\"\"Test getting indexing statistics.\"\"\"\n        indexer = Indexer(storage_dir=str(self.index_dir))\n        embedder = CodeEmbedder()\n        chunker = MultiLanguageChunker(str(self.test_path))\n\n        incremental_indexer = IncrementalIndexer(\n            indexer=indexer,\n            embedder=embedder,\n            chunker=chunker,\n            snapshot_manager=self.snapshot_manager,\n        )\n\n        # No stats initially\n        stats = incremental_indexer.get_indexing_stats(str(self.test_path))\n        assert stats is None\n\n        # Index the project\n        incremental_indexer.incremental_index(str(self.test_path), \"test_project\")\n\n        # Get stats\n        stats = incremental_indexer.get_indexing_stats(str(self.test_path))\n\n        assert stats is not None\n        assert stats[\"project_name\"] == \"test_project\"\n        assert stats[\"file_count\"] > 0\n        assert \"last_snapshot\" in stats\n        assert \"current_chunks\" in stats",
              "content_preview": "class TestIncrementalIndexing(TestCase):\n    \"\"\"Test incremental indexing functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test environment.\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n    ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014700704225352112,
              "appears_in_lists": 2,
              "final_rank": 8
            }
          },
          {
            "doc_id": "tools\\batch_index.py:19-137:function:main",
            "score": 11.178,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tools\\batch_index.py",
              "relative_path": "tools\\batch_index.py",
              "folder_structure": [
                "tools"
              ],
              "chunk_type": "function",
              "start_line": 19,
              "end_line": 137,
              "name": "main",
              "parent_name": null,
              "docstring": null,
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Index project for semantic code search\"\n    )\n    parser.add_argument(\n        \"--path\",\n        required=True,\n        help=\"Path to project directory\"\n    )\n    parser.add_argument(\n        \"--mode\",\n        required=True,\n        choices=[\"new\", \"incremental\", \"force\"],\n        help=\"Indexing mode: new (first-time), incremental (change detection), force (full reindex)\"\n    )\n\n    args = parser.parse_args()\n\n    # Validate path\n    project_path = Path(args.path)\n    if not project_path.exists():\n        print(f\"[ERROR] Path does not exist: {project_path}\")\n        return 1\n\n    if not project_path.is_dir():\n        print(f\"[ERROR] Path is not a directory: {project_path}\")\n        return 1\n\n    # Determine incremental mode\n    if args.mode == \"incremental\":\n        incremental = True\n        mode_desc = \"Incremental (change detection with Merkle tree)\"\n    else:  # new or force\n        incremental = False\n        if args.mode == \"new\":\n            mode_desc = \"New (first-time full index)\"\n        else:\n            mode_desc = \"Force (full reindex, bypass snapshot)\"\n\n    # Display configuration\n    print(\"=\" * 70)\n    print(\"PROJECT INDEXING\")\n    print(\"=\" * 70)\n    print(f\"Path: {project_path}\")\n    print(f\"Mode: {mode_desc}\")\n    print(f\"Incremental: {incremental}\")\n    print(\"=\" * 70)\n    print()\n\n    # Start indexing\n    start_time = time.time()\n\n    try:\n        print(\"[INFO] Starting indexing...\")\n        print()\n\n        result_json = index_directory(\n            str(project_path),\n            incremental=incremental\n        )\n\n        # Parse JSON string to dictionary\n        result = json.loads(result_json)\n\n        elapsed = time.time() - start_time\n\n        # Display results\n        print()\n        print(\"=\" * 70)\n        print(\"INDEXING RESULTS\")\n        print(\"=\" * 70)\n\n        if result.get(\"success\"):\n            print(\"[OK] Indexing completed successfully\")\n            print()\n            print(f\"Project: {result.get('project_name', 'Unknown')}\")\n            print(f\"Directory: {result.get('directory', project_path)}\")\n            print(f\"Mode: {'Incremental' if result.get('incremental') else 'Full'}\")\n            print()\n            print(f\"Files added: {result.get('files_added', 0)}\")\n            print(f\"Files removed: {result.get('files_removed', 0)}\")\n            print(f\"Files modified: {result.get('files_modified', 0)}\")\n            print()\n            print(f\"Chunks added: {result.get('chunks_added', 0)}\")\n            print(f\"Chunks removed: {result.get('chunks_removed', 0)}\")\n            print()\n            print(f\"Time taken: {result.get('time_taken', elapsed):.2f} seconds\")\n\n            # Show index statistics\n            index_stats = result.get('index_stats', {})\n            if index_stats:\n                print()\n                print(\"Index Statistics:\")\n                print(f\"  Total files: {index_stats.get('total_files', 0)}\")\n                print(f\"  Supported files: {index_stats.get('supported_files', 0)}\")\n                print(f\"  Total chunks: {index_stats.get('chunks_indexed', 0)}\")\n\n            print(\"=\" * 70)\n            return 0\n\n        else:\n            print(\"[ERROR] Indexing failed\")\n            error = result.get('error', 'Unknown error')\n            print(f\"Error: {error}\")\n            print(\"=\" * 70)\n            return 1\n\n    except Exception as e:\n        elapsed = time.time() - start_time\n        print()\n        print(\"=\" * 70)\n        print(\"[ERROR] Indexing failed with exception\")\n        print(\"=\" * 70)\n        print(f\"Error: {str(e)}\")\n        print(f\"Time elapsed: {elapsed:.2f} seconds\")\n        print(\"=\" * 70)\n        import traceback\n        traceback.print_exc()\n        return 1",
              "content_preview": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Index project for semantic code search\"\n    )\n    parser.add_argument(\n        \"--path\",\n        required=True,\n        help=\"Pat...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015151515151515152,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          }
        ],
        "all_doc_ids": [
          "merkle\\__init__.py:1-8:module",
          "tools\\batch_index.py:19-137:function:main",
          "tests\\integration\\test_hybrid_search_integration.py:201-219:decorated_definition:test_incremental_indexing_with_hybrid_search",
          "mcp_server\\server.py:641-761:decorated_definition:index_directory",
          "tests\\integration\\test_incremental_indexing.py:337-364:method:test_indexing_stats",
          "search\\incremental_indexer.py:221-367:method:_full_index",
          "tests\\integration\\test_full_flow.py:441-516:method:test_incremental_indexing_with_merkle",
          "tests\\integration\\test_direct_indexing.py:13-101:function:test_direct_indexing",
          "tests\\integration\\test_incremental_indexing.py:251-280:decorated_definition:test_file_deletion",
          "tests\\integration\\test_incremental_indexing.py:25-364:class:TestIncrementalIndexing"
        ],
        "unique_discoveries": [
          "tests\\integration\\test_incremental_indexing.py:337-364:method:test_indexing_stats",
          "tests\\integration\\test_incremental_indexing.py:251-280:decorated_definition:test_file_deletion"
        ]
      },
      "comparison": {
        "time_overhead_ms": 23.85,
        "time_overhead_pct": 124.3,
        "top5_overlap_count": 4,
        "top5_overlap_pct": 80.0,
        "unique_discovery_count": 2,
        "value_rating": "MEDIUM"
      }
    },
    {
      "query": "BM25 sparse search implementation",
      "single_hop": {
        "time_ms": 18.28,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "search\\hybrid_searcher.py:301-426:method:search",
            "score": 6.82,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 301,
              "end_line": 426,
              "name": "search",
              "parent_name": "HybridSearcher",
              "docstring": "Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)\n        \"\"\"\n        # Check if indices are ready based on search mode\n        if search_mode == \"bm25\":\n            if self.bm25_index.is_empty:\n                self._logger.warning(\"BM25 search requested but BM25 index is empty\")\n                return []\n        elif search_mode == \"semantic\":\n            if not self.dense_index.index or self.dense_index.index.ntotal == 0:\n                self._logger.warning(\n                    \"Semantic search requested but dense index is empty\"\n                )\n                return []\n        else:  # hybrid\n            if not self.is_ready:\n                self._logger.warning(\"Hybrid search not ready - indices may be empty\")\n                return []\n\n        self._logger.debug(f\"{search_mode.title()} search for: '{query}' (k={k})\")\n\n        start_time = time.time()\n\n        # Handle different search modes\n        if search_mode == \"bm25\":\n            # BM25-only search\n            bm25_results = self._search_bm25(query, k, min_bm25_score)\n            # Convert BM25 results to SearchResult format\n            final_results = self._convert_bm25_to_search_results(bm25_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        elif search_mode == \"semantic\":\n            # Dense-only search\n            dense_results = self._search_dense(query, k, filters)\n            # Convert dense results to SearchResult format\n            final_results = self._convert_dense_to_search_results(dense_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        else:  # hybrid mode\n            search_k = k * 2  # Get more results for better reranking\n\n            if use_parallel and not self._is_shutdown:\n                # Parallel execution\n                bm25_results, dense_results = self._parallel_search(\n                    query, search_k, min_bm25_score, filters\n                )\n            else:\n                # Sequential execution\n                bm25_results, dense_results = self._sequential_search(\n                    query, search_k, min_bm25_score, filters\n                )\n\n            # Rerank results\n            rerank_start = time.time()\n            self._logger.debug(\n                f\"[RERANK] Using weights: BM25={self.bm25_weight}, Dense={self.dense_weight}, \"\n                f\"BM25_results={len(bm25_results)}, Dense_results={len(dense_results)}\"\n            )\n            final_results = self.reranker.rerank_simple(\n                bm25_results=bm25_results,\n                dense_results=dense_results,\n                max_results=k,\n                bm25_weight=self.bm25_weight,\n                dense_weight=self.dense_weight,\n            )\n            rerank_time = time.time() - rerank_start\n            self._logger.debug(\n                f\"[RERANK] Produced {len(final_results)} results in {rerank_time:.3f}s\"\n            )\n\n        # Update statistics\n        total_time = time.time() - start_time\n        self._search_stats[\"total_searches\"] += 1\n        self._search_stats[\"rerank_time\"] += rerank_time\n\n        if use_parallel:\n            parallel_time = max(\n                self._search_stats.get(\"last_bm25_time\", 0),\n                self._search_stats.get(\"last_dense_time\", 0),\n            )\n            sequential_time = self._search_stats.get(\n                \"last_bm25_time\", 0\n            ) + self._search_stats.get(\"last_dense_time\", 0)\n            if sequential_time > 0:\n                efficiency = 1.0 - (parallel_time / sequential_time)\n                self._search_stats[\"parallel_efficiency\"] = efficiency\n\n        # Mode-specific logging\n        if search_mode == \"bm25\":\n            self._logger.debug(\n                f\"BM25 search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        elif search_mode == \"semantic\":\n            self._logger.debug(\n                f\"Semantic search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        else:  # hybrid\n            self._logger.debug(\n                f\"Hybrid search complete: {len(final_results)} results, \"\n                f\"BM25: {len(bm25_results)}, Dense: {len(dense_results)}, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n\n        return final_results",
              "content_preview": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015584415584415584,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:696-706:method:_sequential_search",
            "score": 6.84,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 696,
              "end_line": 706,
              "name": "_sequential_search",
              "parent_name": "HybridSearcher",
              "docstring": "Execute BM25 and dense search sequentially.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def _sequential_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"Execute BM25 and dense search sequentially.\"\"\"\n        bm25_results = self._search_bm25(query, k, min_bm25_score)\n        dense_results = self._search_dense(query, k, filters)\n        return bm25_results, dense_results",
              "content_preview": "def _sequential_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015528846153846153,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:57-1252:class:HybridSearcher",
            "score": 6.505,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "class",
              "start_line": 57,
              "end_line": 1252,
              "name": "HybridSearcher",
              "parent_name": null,
              "docstring": "Orchestrates BM25 + dense search with GPU awareness and parallel execution.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class HybridSearcher:\n    \"\"\"Orchestrates BM25 + dense search with GPU awareness and parallel execution.\"\"\"\n\n    def __init__(\n        self,\n        storage_dir: str,\n        embedder=None,\n        bm25_weight: float = 0.4,\n        dense_weight: float = 0.6,\n        rrf_k: int = 60,\n        max_workers: int = 2,\n    ):\n        \"\"\"\n        Initialize hybrid searcher.\n\n        Args:\n            storage_dir: Directory for storing indices\n            embedder: CodeEmbedder instance for semantic search (optional)\n            bm25_weight: Weight for BM25 results (0.0 to 1.0)\n            dense_weight: Weight for dense vector results (0.0 to 1.0)\n            rrf_k: RRF parameter for reranking\n            max_workers: Maximum thread pool workers for parallel execution\n        \"\"\"\n        self.storage_dir = Path(storage_dir)\n        self.storage_dir.mkdir(parents=True, exist_ok=True)\n\n        # Store embedder for semantic search\n        self.embedder = embedder\n\n        # Weights\n        self.bm25_weight = bm25_weight\n        self.dense_weight = dense_weight\n\n        # Components - use existing storage structure\n        self._logger = logging.getLogger(__name__)\n\n        # BM25 index gets its own subdirectory\n        self._logger.info(f\"[INIT] Creating BM25Index at: {self.storage_dir / 'bm25'}\")\n        try:\n            self.bm25_index = BM25Index(str(self.storage_dir / \"bm25\"))\n            self._logger.info(\"[INIT] BM25Index created successfully\")\n        except Exception as e:\n            self._logger.error(f\"[INIT] Failed to create BM25Index: {e}\")\n            raise\n\n        # Try to load existing BM25 index\n        self._logger.info(f\"[INIT] BM25 storage path: {self.storage_dir / 'bm25'}\")\n        bm25_loaded = self.bm25_index.load()\n        if bm25_loaded:\n            self._logger.info(\n                f\"[INIT] Loaded existing BM25 index with {self.bm25_index.size} documents\"\n            )\n        else:\n            self._logger.info(\"[INIT] No existing BM25 index found, starting fresh\")\n            # Log what files we're looking for\n            bm25_dir = self.storage_dir / \"bm25\"\n            self._logger.debug(f\"[INIT] BM25 directory exists: {bm25_dir.exists()}\")\n            if bm25_dir.exists():\n                files = list(bm25_dir.iterdir())\n                self._logger.debug(\n                    f\"[INIT] BM25 files found: {[f.name for f in files]}\"\n                )\n\n        # Dense index uses the main storage directory where existing indices are stored\n        self._logger.info(f\"[INIT] Initializing dense index at: {self.storage_dir}\")\n        self.dense_index = CodeIndexManager(str(self.storage_dir))\n        # Dense index loads automatically in its __init__\n        dense_count = self.dense_index.index.ntotal if self.dense_index.index else 0\n        if dense_count > 0:\n            self._logger.info(\n                f\"[INIT] Loaded existing dense index with {dense_count} vectors\"\n            )\n        else:\n            self._logger.info(\"[INIT] No existing dense index found, starting fresh\")\n\n        # Log final initialization status\n        total_bm25 = self.bm25_index.size\n        self._logger.info(\n            f\"[INIT] HybridSearcher initialized - BM25: {total_bm25} docs, Dense: {dense_count} vectors\"\n        )\n        self._logger.info(\n            f\"[INIT] Ready status: BM25={not self.bm25_index.is_empty}, Dense={dense_count > 0}, Overall={self.is_ready}\"\n        )\n\n        self.reranker = RRFReranker(k=rrf_k)\n        self.gpu_monitor = GPUMemoryMonitor()\n\n        # Threading\n        self.max_workers = max_workers\n        self._thread_pool = ThreadPoolExecutor(max_workers=max_workers)\n        self._shutdown_lock = threading.Lock()\n        self._is_shutdown = False\n\n        # Performance tracking\n        self._search_stats = {\n            \"total_searches\": 0,\n            \"bm25_time\": 0.0,\n            \"dense_time\": 0.0,\n            \"rerank_time\": 0.0,\n            \"parallel_efficiency\": 0.0,\n        }\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.shutdown()\n\n    def shutdown(self):\n        \"\"\"Shutdown the thread pool.\"\"\"\n        with self._shutdown_lock:\n            if not self._is_shutdown:\n                self._thread_pool.shutdown(wait=True)\n                self._is_shutdown = True\n                self._logger.info(\"HybridSearcher shut down\")\n\n    @property\n    def is_ready(self) -> bool:\n        \"\"\"Check if both indices are ready.\"\"\"\n        bm25_ready = not self.bm25_index.is_empty\n        dense_ready = (\n            self.dense_index.index is not None and self.dense_index.index.ntotal > 0\n        )\n\n        self._logger.debug(\n            f\"[IS_READY] BM25 ready: {bm25_ready} (size: {self.bm25_index.size})\"\n        )\n        self._logger.debug(\n            f\"[IS_READY] Dense ready: {dense_ready} (vectors: {self.dense_index.index.ntotal if self.dense_index.index else 0})\"\n        )\n\n        is_ready = bm25_ready and dense_ready\n        self._logger.debug(f\"[IS_READY] Overall ready: {is_ready}\")\n\n        return is_ready\n\n    @property\n    def stats(self) -> Dict[str, Any]:\n        \"\"\"Get search performance statistics.\"\"\"\n        stats = self._search_stats.copy()\n\n        # Add index stats\n        stats.update(\n            {\n                \"bm25_stats\": self.bm25_index.get_stats(),\n                \"dense_stats\": {\n                    \"total_vectors\": (\n                        self.dense_index.index.ntotal if self.dense_index.index else 0\n                    ),\n                    \"on_gpu\": getattr(self.dense_index, \"_on_gpu\", False),\n                },\n                \"gpu_memory\": self.gpu_monitor.get_available_memory(),\n            }\n        )\n\n        return stats\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get index statistics in the format expected by MCP server.\"\"\"\n        bm25_count = self.bm25_index.size\n        dense_count = self.dense_index.index.ntotal if self.dense_index.index else 0\n        total_chunks = max(bm25_count, dense_count)  # Use the higher count as total\n\n        return {\n            \"total_chunks\": total_chunks,\n            \"bm25_documents\": bm25_count,\n            \"dense_vectors\": dense_count,\n            \"is_ready\": self.is_ready,\n            \"bm25_ready\": not self.bm25_index.is_empty,\n            \"dense_ready\": dense_count > 0,\n        }\n\n    def get_index_size(self) -> int:\n        \"\"\"Get total index size (compatible with incremental indexer interface).\"\"\"\n        bm25_count = self.bm25_index.size\n        dense_count = self.dense_index.index.ntotal if self.dense_index.index else 0\n        return max(bm25_count, dense_count)  # Return the higher count\n\n    def index_documents(\n        self,\n        documents: List[str],\n        doc_ids: List[str],\n        embeddings: List[List[float]],\n        metadata: Optional[Dict[str, Dict]] = None,\n    ) -> None:\n        \"\"\"Index documents in both BM25 and dense indices.\"\"\"\n        if len(documents) != len(doc_ids) or len(documents) != len(embeddings):\n            raise ValueError(\"All input lists must have the same length\")\n\n        self._logger.info(f\"[INDEX_DOCUMENTS] Called with {len(documents)} documents\")\n\n        # Index in BM25 (CPU)\n        self._logger.info(\"[BM25] Starting BM25 indexing...\")\n        start_time = time.time()\n        bm25_size_before = self.bm25_index.size\n        self._logger.info(f\"[BM25] Before indexing - size: {bm25_size_before}\")\n\n        self.bm25_index.index_documents(documents, doc_ids, metadata)\n\n        bm25_time = time.time() - start_time\n        bm25_size_after = self.bm25_index.size\n        self._logger.info(f\"[BM25] After indexing - size: {bm25_size_after}\")\n\n        self._logger.debug(\n            f\"[BM25] Indexing completed: {bm25_size_before} -> {bm25_size_after} documents ({bm25_time:.2f}s)\"\n        )\n        self._logger.debug(f\"[BM25] Index directory: {self.bm25_index.storage_dir}\")\n        self._logger.debug(\n            f\"[BM25] Index files will be saved as: {[str(p) for p in [self.bm25_index.index_path, self.bm25_index.docs_path, self.bm25_index.metadata_path]]}\"\n        )\n\n        # Verify BM25 indexing worked\n        if bm25_size_after == bm25_size_before:\n            self._logger.error(\"[BM25] ERROR: No documents were indexed!\")\n            self._logger.debug(f\"[BM25] Documents provided: {len(documents)}\")\n            self._logger.debug(\n                f\"[BM25] First document: {documents[0][:200] if documents else 'EMPTY'}\"\n            )\n\n        # Index in dense (potentially GPU)\n        start_time = time.time()\n        # Convert embeddings to EmbeddingResult format\n        import numpy as np\n\n        from embeddings.embedder import EmbeddingResult\n\n        embedding_results = []\n        for _i, (doc_id, embedding) in enumerate(\n            zip(doc_ids, embeddings, strict=False)\n        ):\n            result = EmbeddingResult(\n                embedding=np.array(embedding, dtype=np.float32),\n                chunk_id=doc_id,\n                metadata=metadata.get(doc_id, {}) if metadata else {},\n            )\n            embedding_results.append(result)\n\n        self.dense_index.add_embeddings(embedding_results)\n        dense_time = time.time() - start_time\n\n        self._logger.info(\n            f\"Hybrid indexing complete: BM25 {bm25_time:.2f}s, Dense {dense_time:.2f}s\"\n        )\n\n    def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)\n        \"\"\"\n        # Check if indices are ready based on search mode\n        if search_mode == \"bm25\":\n            if self.bm25_index.is_empty:\n                self._logger.warning(\"BM25 search requested but BM25 index is empty\")\n                return []\n        elif search_mode == \"semantic\":\n            if not self.dense_index.index or self.dense_index.index.ntotal == 0:\n                self._logger.warning(\n                    \"Semantic search requested but dense index is empty\"\n                )\n                return []\n        else:  # hybrid\n            if not self.is_ready:\n                self._logger.warning(\"Hybrid search not ready - indices may be empty\")\n                return []\n\n        self._logger.debug(f\"{search_mode.title()} search for: '{query}' (k={k})\")\n\n        start_time = time.time()\n\n        # Handle different search modes\n        if search_mode == \"bm25\":\n            # BM25-only search\n            bm25_results = self._search_bm25(query, k, min_bm25_score)\n            # Convert BM25 results to SearchResult format\n            final_results = self._convert_bm25_to_search_results(bm25_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        elif search_mode == \"semantic\":\n            # Dense-only search\n            dense_results = self._search_dense(query, k, filters)\n            # Convert dense results to SearchResult format\n            final_results = self._convert_dense_to_search_results(dense_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        else:  # hybrid mode\n            search_k = k * 2  # Get more results for better reranking\n\n            if use_parallel and not self._is_shutdown:\n                # Parallel execution\n                bm25_results, dense_results = self._parallel_search(\n                    query, search_k, min_bm25_score, filters\n                )\n            else:\n                # Sequential execution\n                bm25_results, dense_results = self._sequential_search(\n                    query, search_k, min_bm25_score, filters\n                )\n\n            # Rerank results\n            rerank_start = time.time()\n            self._logger.debug(\n                f\"[RERANK] Using weights: BM25={self.bm25_weight}, Dense={self.dense_weight}, \"\n                f\"BM25_results={len(bm25_results)}, Dense_results={len(dense_results)}\"\n            )\n            final_results = self.reranker.rerank_simple(\n                bm25_results=bm25_results,\n                dense_results=dense_results,\n                max_results=k,\n                bm25_weight=self.bm25_weight,\n                dense_weight=self.dense_weight,\n            )\n            rerank_time = time.time() - rerank_start\n            self._logger.debug(\n                f\"[RERANK] Produced {len(final_results)} results in {rerank_time:.3f}s\"\n            )\n\n        # Update statistics\n        total_time = time.time() - start_time\n        self._search_stats[\"total_searches\"] += 1\n        self._search_stats[\"rerank_time\"] += rerank_time\n\n        if use_parallel:\n            parallel_time = max(\n                self._search_stats.get(\"last_bm25_time\", 0),\n                self._search_stats.get(\"last_dense_time\", 0),\n            )\n            sequential_time = self._search_stats.get(\n                \"last_bm25_time\", 0\n            ) + self._search_stats.get(\"last_dense_time\", 0)\n            if sequential_time > 0:\n                efficiency = 1.0 - (parallel_time / sequential_time)\n                self._search_stats[\"parallel_efficiency\"] = efficiency\n\n        # Mode-specific logging\n        if search_mode == \"bm25\":\n            self._logger.debug(\n                f\"BM25 search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        elif search_mode == \"semantic\":\n            self._logger.debug(\n                f\"Semantic search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        else:  # hybrid\n            self._logger.debug(\n                f\"Hybrid search complete: {len(final_results)} results, \"\n                f\"BM25: {len(bm25_results)}, Dense: {len(dense_results)}, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n\n        return final_results\n\n    def find_similar_to_chunk(self, chunk_id: str, k: int = 5) -> List:\n        \"\"\"\n        Find chunks similar to a given chunk using dense semantic search.\n\n        Args:\n            chunk_id: The ID of the reference chunk\n            k: Number of similar chunks to return\n\n        Returns:\n            List of SearchResult objects with similar chunks\n        \"\"\"\n        from .searcher import SearchResult\n\n        # Use dense index for semantic similarity\n        similar_chunks = self.dense_index.get_similar_chunks(chunk_id, k)\n\n        # Convert to SearchResult format expected by MCP tool\n        results = []\n        for cid, similarity, metadata in similar_chunks:\n            result = SearchResult(\n                chunk_id=cid,\n                similarity_score=similarity,\n                content_preview=metadata.get(\"content_preview\", \"\"),\n                file_path=metadata.get(\"file_path\", \"\"),\n                relative_path=metadata.get(\"relative_path\", \"\"),\n                folder_structure=metadata.get(\"folder_structure\", []),\n                chunk_type=metadata.get(\"chunk_type\", \"unknown\"),\n                name=metadata.get(\"name\"),\n                parent_name=metadata.get(\"parent_name\"),\n                start_line=metadata.get(\"start_line\", 0),\n                end_line=metadata.get(\"end_line\", 0),\n                docstring=metadata.get(\"docstring\"),\n                tags=metadata.get(\"tags\", []),\n                context_info={},\n            )\n            results.append(result)\n\n        return results\n\n    def multi_hop_search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        hops: int = 2,\n        expansion_factor: float = 0.3,\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List:\n        \"\"\"\n        Multi-hop semantic search for discovering interconnected code relationships.\n\n        Performs iterative search by:\n        1. Initial query-based search (Hop 1)\n        2. Finding similar chunks for each result (Hop 2+)\n        3. Re-ranking all discovered chunks by query relevance\n\n        Args:\n            query: Search query\n            k: Number of final results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            hops: Number of search hops (default: 2)\n            expansion_factor: Fraction of k to expand per hop (default: 0.3)\n            use_parallel: Whether to use parallel search\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for search\n\n        Returns:\n            List of SearchResult objects with discovered related code\n        \"\"\"\n        from .searcher import SearchResult\n\n        # Validate parameters\n        if hops < 1:\n            self._logger.warning(f\"Invalid hops={hops}, using 1\")\n            hops = 1\n        if expansion_factor < 0 or expansion_factor > 2.0:\n            self._logger.warning(f\"Invalid expansion_factor={expansion_factor}, using 0.3\")\n            expansion_factor = 0.3\n\n        self._logger.info(\n            f\"[MULTI_HOP] Starting {hops}-hop search for '{query}' \"\n            f\"(k={k}, expansion={expansion_factor}, mode={search_mode})\"\n        )\n\n        # Hop 1: Initial query-based search\n        initial_k = k * 2  # Get more initial results for better expansion\n        initial_results = self.search(\n            query=query,\n            k=initial_k,\n            search_mode=search_mode,\n            use_parallel=use_parallel,\n            min_bm25_score=min_bm25_score,\n            filters=filters,\n        )\n\n        if not initial_results:\n            self._logger.info(\"[MULTI_HOP] No initial results found\")\n            return []\n\n        self._logger.info(\n            f\"[MULTI_HOP] Hop 1: Found {len(initial_results)} initial results\"\n        )\n\n        # Track all discovered chunks (avoid duplicates)\n        # Note: HybridSearcher.search() returns reranker.SearchResult with doc_id\n        all_doc_ids = {r.doc_id for r in initial_results}\n        all_results = {r.doc_id: r for r in initial_results}\n\n        # If only 1 hop requested, return initial results\n        if hops == 1:\n            return initial_results[:k]\n\n        # Hop 2+: Expand from each initial result\n        expansion_k = max(1, int(k * expansion_factor))\n\n        for hop in range(2, hops + 1):\n            hop_discovered = 0\n\n            # Expand from top initial results only (not from previously expanded)\n            source_results = initial_results[:k]  # Use top k as expansion sources\n\n            for result in source_results:\n                try:\n                    # Find similar chunks to this result\n                    # find_similar_to_chunk returns searcher.SearchResult with chunk_id\n                    similar_chunks = self.find_similar_to_chunk(\n                        chunk_id=result.doc_id,  # doc_id is the same as chunk_id\n                        k=expansion_k\n                    )\n\n                    # Add new chunks\n                    for sim_result in similar_chunks:\n                        # Convert searcher.SearchResult chunk_id to doc_id for consistency\n                        doc_id = sim_result.chunk_id\n                        if doc_id not in all_doc_ids:\n                            all_doc_ids.add(doc_id)\n                            # Convert to reranker.SearchResult format\n                            from .reranker import SearchResult as RerankerSearchResult\n                            reranker_result = RerankerSearchResult(\n                                doc_id=doc_id,\n                                score=sim_result.similarity_score,\n                                metadata=sim_result.__dict__,\n                                source=\"multi_hop\"\n                            )\n                            all_results[doc_id] = reranker_result\n                            hop_discovered += 1\n\n                except Exception as e:\n                    self._logger.warning(\n                        f\"[MULTI_HOP] Failed to find similar chunks for {result.doc_id}: {e}\"\n                    )\n                    continue\n\n            self._logger.info(\n                f\"[MULTI_HOP] Hop {hop}: Discovered {hop_discovered} new chunks \"\n                f\"(total: {len(all_results)})\"\n            )\n\n        # Re-rank all discovered results by query relevance\n        self._logger.info(\n            f\"[MULTI_HOP] Re-ranking {len(all_results)} total chunks by query relevance\"\n        )\n\n        final_results = self._rerank_by_query(\n            query=query,\n            results=list(all_results.values()),\n            k=k,\n            search_mode=search_mode\n        )\n\n        self._logger.info(\n            f\"[MULTI_HOP] Returning top {len(final_results)} results after re-ranking\"\n        )\n\n        return final_results\n\n    def _rerank_by_query(\n        self,\n        query: str,\n        results: List,\n        k: int,\n        search_mode: str = \"hybrid\"\n    ) -> List:\n        \"\"\"\n        Re-rank results by computing fresh relevance scores against the original query.\n\n        Args:\n            query: Original search query\n            results: List of SearchResult objects to re-rank\n            k: Number of top results to return\n            search_mode: Search mode for re-ranking strategy\n\n        Returns:\n            Top k results sorted by query relevance\n        \"\"\"\n        if not results:\n            return []\n\n        # For semantic/hybrid modes: re-score using dense similarity\n        if search_mode in (\"semantic\", \"hybrid\") and self.embedder:\n            try:\n                # Get query embedding\n                query_embedding = self.embedder.embed_query(query)\n\n                # Re-score each result by cosine similarity to query\n                import numpy as np\n\n                for result in results:\n                    # Get chunk embedding from dense index\n                    # reranker.SearchResult uses doc_id, not chunk_id\n                    doc_id = result.doc_id\n                    chunk_metadata = self.dense_index.metadata_db.get(doc_id)\n                    if chunk_metadata and \"embedding\" in chunk_metadata:\n                        chunk_emb = np.array(chunk_metadata[\"embedding\"])\n                        # Compute cosine similarity\n                        similarity = np.dot(query_embedding, chunk_emb) / (\n                            np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb)\n                        )\n                        # reranker.SearchResult uses score, not similarity_score\n                        result.score = float(similarity)\n                    # Keep original score if embedding not found\n\n            except Exception as e:\n                self._logger.warning(\n                    f\"[MULTI_HOP] Failed to re-score with embeddings: {e}, \"\n                    \"keeping original scores\"\n                )\n\n        # Sort by score (descending)\n        # reranker.SearchResult uses score, not similarity_score\n        sorted_results = sorted(\n            results,\n            key=lambda r: r.score,\n            reverse=True\n        )\n\n        return sorted_results[:k]\n\n    def _parallel_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"Execute BM25 and dense search in parallel.\"\"\"\n        try:\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                # Submit both searches\n                bm25_future = executor.submit(\n                    self._search_bm25, query, k, min_bm25_score\n                )\n                dense_future = executor.submit(self._search_dense, query, k, filters)\n\n                # Wait for results\n                bm25_results = bm25_future.result()\n                dense_results = dense_future.result()\n\n                return bm25_results, dense_results\n\n        except Exception as e:\n            self._logger.warning(\n                f\"Parallel search failed, falling back to sequential: {e}\"\n            )\n            return self._sequential_search(query, k, min_bm25_score, filters)\n\n    def _sequential_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"Execute BM25 and dense search sequentially.\"\"\"\n        bm25_results = self._search_bm25(query, k, min_bm25_score)\n        dense_results = self._search_dense(query, k, filters)\n        return bm25_results, dense_results\n\n    def _search_bm25(self, query: str, k: int, min_score: float) -> List[Tuple]:\n        \"\"\"Search using BM25 index.\"\"\"\n        start_time = time.time()\n        try:\n            results = self.bm25_index.search(query, k, min_score)\n            search_time = time.time() - start_time\n\n            self._search_stats[\"bm25_time\"] += search_time\n            self._search_stats[\"last_bm25_time\"] = search_time\n\n            self._logger.debug(\n                f\"BM25 search: {len(results)} results in {search_time:.3f}s\"\n            )\n            return results\n\n        except Exception as e:\n            self._logger.error(f\"BM25 search failed: {e}\")\n            return []\n\n    def _search_dense(self, query: str, k: int, filters: Optional[Dict]) -> List[Tuple]:\n        \"\"\"Search using dense vector index.\"\"\"\n        start_time = time.time()\n        try:\n            # Use stored embedder or create one if not provided\n            if self.embedder is None:\n                self._logger.warning(\n                    \"No embedder provided to HybridSearcher, creating new instance\"\n                )\n                from pathlib import Path\n\n                from embeddings.embedder import CodeEmbedder\n\n                # Use same cache directory as main embedder\n                cache_dir = Path.home() / \".claude_code_search\" / \"models\"\n                cache_dir.mkdir(parents=True, exist_ok=True)\n                self.embedder = CodeEmbedder(cache_dir=str(cache_dir))\n                self._logger.info(\n                    \"Created new CodeEmbedder instance for semantic search\"\n                )\n\n            query_embedding = self.embedder.embed_query(query)\n\n            # Search in dense index\n            results = self.dense_index.search(query_embedding, k, filters)\n\n            search_time = time.time() - start_time\n            self._search_stats[\"dense_time\"] += search_time\n            self._search_stats[\"last_dense_time\"] = search_time\n\n            self._logger.debug(\n                f\"Dense search: {len(results)} results in {search_time:.3f}s\"\n            )\n            return results\n\n        except Exception as e:\n            self._logger.error(f\"Dense search failed: {e}\")\n            import traceback\n\n            self._logger.error(\n                f\"Dense search exception details: {traceback.format_exc()}\"\n            )\n            return []\n\n    def _convert_bm25_to_search_results(\n        self, bm25_results: List[Tuple]\n    ) -> List[SearchResult]:\n        \"\"\"Convert BM25 search results to SearchResult format.\"\"\"\n        search_results = []\n        for i, (doc_id, score, metadata) in enumerate(bm25_results):\n            search_result = SearchResult(\n                doc_id=doc_id, score=score, metadata=metadata, source=\"bm25\", rank=i\n            )\n            search_results.append(search_result)\n        return search_results\n\n    def _convert_dense_to_search_results(\n        self, dense_results: List[Tuple]\n    ) -> List[SearchResult]:\n        \"\"\"Convert dense search results to SearchResult format.\"\"\"\n        search_results = []\n        for i, (doc_id, score, metadata) in enumerate(dense_results):\n            search_result = SearchResult(\n                doc_id=doc_id, score=score, metadata=metadata, source=\"semantic\", rank=i\n            )\n            search_results.append(search_result)\n        return search_results\n\n    def get_search_mode_stats(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about search mode performance.\"\"\"\n        total_searches = self._search_stats[\"total_searches\"]\n        if total_searches == 0:\n            return {\"message\": \"No searches performed yet\"}\n\n        avg_bm25_time = self._search_stats[\"bm25_time\"] / total_searches\n        avg_dense_time = self._search_stats[\"dense_time\"] / total_searches\n        avg_rerank_time = self._search_stats[\"rerank_time\"] / total_searches\n\n        return {\n            \"total_searches\": total_searches,\n            \"average_times\": {\n                \"bm25\": avg_bm25_time,\n                \"dense\": avg_dense_time,\n                \"reranking\": avg_rerank_time,\n                \"total\": avg_bm25_time + avg_dense_time + avg_rerank_time,\n            },\n            \"parallel_efficiency\": self._search_stats.get(\"parallel_efficiency\", 0.0),\n            \"gpu_utilization\": self.gpu_monitor.get_available_memory(),\n            \"search_distribution\": {\n                \"bm25_contribution\": self.bm25_weight,\n                \"dense_contribution\": self.dense_weight,\n            },\n        }\n\n    def optimize_weights(\n        self, test_queries: List[str], ground_truth: Optional[List[List[str]]] = None\n    ) -> Dict[str, float]:\n        \"\"\"\n        Optimize BM25/dense weights based on test queries.\n\n        Args:\n            test_queries: List of test queries\n            ground_truth: Optional ground truth results for each query\n\n        Returns:\n            Optimized weights\n        \"\"\"\n        self._logger.info(f\"Optimizing weights with {len(test_queries)} test queries\")\n\n        weight_combinations = [\n            (0.2, 0.8),\n            (0.3, 0.7),\n            (0.4, 0.6),\n            (0.5, 0.5),\n            (0.6, 0.4),\n            (0.7, 0.3),\n            (0.8, 0.2),\n        ]\n\n        best_weights = (self.bm25_weight, self.dense_weight)\n        best_score = 0.0\n\n        for bm25_w, dense_w in weight_combinations:\n            # Temporarily set weights\n            orig_bm25_w, orig_dense_w = self.bm25_weight, self.dense_weight\n            self.bm25_weight, self.dense_weight = bm25_w, dense_w\n\n            total_score = 0.0\n            for _i, query in enumerate(test_queries):\n                results = self.search(query, k=10, use_parallel=False)\n\n                # Score based on result quality metrics\n                if results:\n                    analysis = self.reranker.analyze_fusion_quality(results)\n                    score = (\n                        analysis[\"diversity_score\"] * 0.4\n                        + analysis[\"coverage_balance\"] * 0.3\n                        + analysis[\"high_quality_ratio\"] * 0.3\n                    )\n                    total_score += score\n\n            avg_score = total_score / len(test_queries) if test_queries else 0.0\n\n            if avg_score > best_score:\n                best_score = avg_score\n                best_weights = (bm25_w, dense_w)\n\n            # Restore original weights\n            self.bm25_weight, self.dense_weight = orig_bm25_w, orig_dense_w\n\n        # Set optimal weights\n        self.bm25_weight, self.dense_weight = best_weights\n\n        self._logger.info(\n            f\"Optimized weights: BM25={self.bm25_weight:.2f}, \"\n            f\"Dense={self.dense_weight:.2f} (score: {best_score:.3f})\"\n        )\n\n        return {\n            \"bm25_weight\": self.bm25_weight,\n            \"dense_weight\": self.dense_weight,\n            \"optimization_score\": best_score,\n            \"tested_combinations\": len(weight_combinations),\n        }\n\n    def save_indices(self) -> None:\n        \"\"\"Save both BM25 and dense indices.\"\"\"\n        try:\n            self._logger.info(\"[SAVE] Starting save operation\")\n\n            # Log comprehensive state before save\n            bm25_dir = self.storage_dir / \"bm25\"\n            dense_size = self.dense_index.index.ntotal if self.dense_index.index else 0\n\n            self._logger.info(\"[SAVE] === PRE-SAVE STATE ===\")\n            self._logger.info(f\"[SAVE] BM25 directory exists: {bm25_dir.exists()}\")\n            self._logger.info(\n                f\"[SAVE] BM25 index size: {self.bm25_index.size} documents\"\n            )\n            self._logger.info(\n                f\"[SAVE] BM25 has index: {self.bm25_index._bm25 is not None}\"\n            )\n            self._logger.info(\n                f\"[SAVE] BM25 tokenized docs: {len(self.bm25_index._tokenized_docs)}\"\n            )\n            self._logger.info(f\"[SAVE] Dense index size: {dense_size} vectors\")\n            self._logger.info(\n                f\"[SAVE] Dense has index: {self.dense_index.index is not None}\"\n            )\n            self._logger.info(f\"[SAVE] Overall ready state: {self.is_ready}\")\n            self._logger.info(\"[SAVE] === END PRE-SAVE STATE ===\")\n\n            # Log BM25 state before save (keep original logging for compatibility)\n            self._logger.info(f\"[SAVE] BM25 size before save: {self.bm25_index.size}\")\n\n            # Save BM25 index\n            if hasattr(self.bm25_index, \"save\"):\n                self._logger.info(\"[SAVE] Calling BM25 index save...\")\n                self.bm25_index.save()\n                self._logger.info(\"[SAVE] BM25 index save completed\")\n            else:\n                self._logger.warning(\"[SAVE] BM25 index does not support saving\")\n\n            # Save dense index\n            if hasattr(self.dense_index, \"save_index\"):\n                self._logger.info(\"[SAVE] Calling dense index save_index...\")\n                self.dense_index.save_index()\n                self._logger.info(\"[SAVE] Dense index save completed\")\n            elif hasattr(self.dense_index, \"save\"):\n                self._logger.info(\"[SAVE] Calling dense index save...\")\n                self.dense_index.save()\n                self._logger.info(\"[SAVE] Dense index save completed\")\n            else:\n                self._logger.warning(\"[SAVE] Dense index does not support saving\")\n\n            # Verify files after save\n            self._verify_bm25_files()\n\n            # Log comprehensive state after save\n            bm25_dir = self.storage_dir / \"bm25\"\n            dense_size_after = (\n                self.dense_index.index.ntotal if self.dense_index.index else 0\n            )\n\n            self._logger.info(\"[SAVE] === POST-SAVE STATE ===\")\n            self._logger.info(f\"[SAVE] BM25 directory exists: {bm25_dir.exists()}\")\n            if bm25_dir.exists():\n                files = list(bm25_dir.iterdir())\n                self._logger.info(f\"[SAVE] BM25 files: {[f.name for f in files]}\")\n            self._logger.info(\n                f\"[SAVE] BM25 index size: {self.bm25_index.size} documents\"\n            )\n            self._logger.info(\n                f\"[SAVE] BM25 has index: {self.bm25_index._bm25 is not None}\"\n            )\n            self._logger.info(f\"[SAVE] Dense index size: {dense_size_after} vectors\")\n            self._logger.info(\n                f\"[SAVE] Dense has index: {self.dense_index.index is not None}\"\n            )\n            self._logger.info(f\"[SAVE] Overall ready state: {self.is_ready}\")\n            self._logger.info(\"[SAVE] === END POST-SAVE STATE ===\")\n\n            self._logger.info(\"[SAVE] Hybrid indices saved successfully\")\n        except Exception as e:\n            self._logger.error(f\"[SAVE] Failed to save indices: {e}\")\n            raise\n\n    def load_indices(self) -> bool:\n        \"\"\"Load both BM25 and dense indices.\"\"\"\n        try:\n            bm25_loaded = self.bm25_index.load()\n            dense_loaded = self.dense_index.load()\n\n            success = bm25_loaded and dense_loaded\n            if success:\n                self._logger.info(\"Hybrid indices loaded successfully\")\n            else:\n                self._logger.warning(\n                    f\"Index loading partial: BM25={bm25_loaded}, Dense={dense_loaded}\"\n                )\n\n            return success\n\n        except Exception as e:\n            self._logger.error(f\"Failed to load indices: {e}\")\n            return False\n\n    def add_embeddings(self, embedding_results) -> None:\n        \"\"\"\n        Add embeddings to both BM25 and dense indices.\n        Compatible with incremental indexer interface.\n\n        Args:\n            embedding_results: List of EmbeddingResult objects\n        \"\"\"\n        if not embedding_results:\n            self._logger.debug(\"[ADD_EMBEDDINGS] No embedding results provided\")\n            return\n\n        self._logger.info(\n            f\"[ADD_EMBEDDINGS] Called with {len(embedding_results)} results\"\n        )\n        self._logger.debug(f\"[ADD_EMBEDDINGS] Storage directory: {self.storage_dir}\")\n        self._logger.debug(\n            f\"[ADD_EMBEDDINGS] BM25 index path: {self.storage_dir / 'bm25'}\"\n        )\n        self._logger.debug(f\"[ADD_EMBEDDINGS] Dense index path: {self.storage_dir}\")\n\n        # Extract data for both indices\n        documents = []\n        doc_ids = []\n        embeddings = []\n        metadata = {}\n\n        for result in embedding_results:\n            doc_id = result.chunk_id\n            doc_ids.append(doc_id)\n\n            # Extract text content for BM25 (from metadata or content)\n            content = result.metadata.get(\"content\", \"\")\n            if not content:\n                # Fallback: try other content fields\n                content = (\n                    result.metadata.get(\"content_preview\", \"\")\n                    or result.metadata.get(\"raw_content\", \"\")\n                    or \"\"\n                )\n            documents.append(content)\n\n            # Embeddings for dense index\n            if hasattr(result.embedding, \"tolist\"):\n                embeddings.append(result.embedding.tolist())\n            else:\n                embeddings.append(list(result.embedding))\n\n            # Metadata for both indices\n            metadata[doc_id] = result.metadata\n\n        # Log data extraction\n        self._logger.debug(f\"[ADD_EMBEDDINGS] Extracted {len(documents)} documents\")\n        self._logger.debug(\n            f\"[ADD_EMBEDDINGS] First doc sample: {documents[0][:100] if documents else 'EMPTY'}...\"\n        )\n\n        # Index in both systems using existing method\n        try:\n            # Log before calling index_documents\n            self._logger.info(\n                f\"[ADD_EMBEDDINGS] Calling index_documents with {len(documents)} docs\"\n            )\n\n            self.index_documents(documents, doc_ids, embeddings, metadata)\n\n            self._logger.info(\n                f\"[ADD_EMBEDDINGS] Successfully added {len(embedding_results)} embeddings to hybrid index\"\n            )\n            self._logger.debug(\n                f\"[ADD_EMBEDDINGS] BM25 index size after adding: {self.bm25_index.size}\"\n            )\n            self._logger.debug(\n                f\"[ADD_EMBEDDINGS] Dense index size after adding: {self.dense_index.index.ntotal if self.dense_index.index else 0}\"\n            )\n\n        except Exception as e:\n            self._logger.error(\n                f\"[ADD_EMBEDDINGS] Failed to add embeddings to hybrid index: {e}\"\n            )\n            raise\n\n    def clear_index(self) -> None:\n        \"\"\"\n        Clear both BM25 and dense indices.\n        Compatible with incremental indexer interface.\n        \"\"\"\n        self._logger.info(\"Clearing hybrid indices\")\n\n        try:\n            # Clear BM25 index by recreating it\n            self.bm25_index = BM25Index(str(self.storage_dir / \"bm25\"))\n\n            # Clear dense index by recreating it\n            self.dense_index = CodeIndexManager(str(self.storage_dir))\n\n            self._logger.info(\"Successfully cleared hybrid indices\")\n        except Exception as e:\n            self._logger.error(f\"Failed to clear hybrid indices: {e}\")\n            raise\n\n    def remove_file_chunks(self, file_path: str, project_name: str) -> int:\n        \"\"\"\n        Remove chunks for a specific file from both indices.\n        Compatible with incremental indexer interface.\n\n        Args:\n            file_path: Relative path of the file\n            project_name: Name of the project\n\n        Returns:\n            Number of chunks removed\n        \"\"\"\n        self._logger.debug(f\"Removing chunks for file: {file_path}\")\n\n        try:\n            removed_count = 0\n\n            # Remove from dense index\n            if hasattr(self.dense_index, \"remove_file_chunks\"):\n                removed_dense = self.dense_index.remove_file_chunks(\n                    file_path, project_name\n                )\n                removed_count += removed_dense\n                self._logger.debug(f\"Removed {removed_dense} chunks from dense index\")\n\n            # Remove from BM25 index\n            if hasattr(self.bm25_index, \"remove_file_chunks\"):\n                removed_bm25 = self.bm25_index.remove_file_chunks(\n                    file_path, project_name\n                )\n                removed_count += removed_bm25\n                self._logger.debug(f\"Removed {removed_bm25} chunks from BM25 index\")\n            else:\n                self._logger.warning(\"BM25 index does not support file chunk removal\")\n\n            self._logger.info(\n                f\"Removed {removed_count} total chunks for file: {file_path}\"\n            )\n            return removed_count\n\n        except Exception as e:\n            self._logger.error(f\"Failed to remove chunks for file {file_path}: {e}\")\n            return 0\n\n    def remove_multiple_files(self, file_paths: set, project_name: str) -> int:\n        \"\"\"\n        Remove chunks for multiple files from both indices in a single pass.\n        Much faster than calling remove_file_chunks repeatedly.\n\n        IMPORTANT: This method properly removes chunks from both FAISS and BM25 indices,\n        preventing index corruption.\n\n        Args:\n            file_paths: Set of file paths to remove\n            project_name: Name of the project\n\n        Returns:\n            Total number of chunks removed\n        \"\"\"\n        self._logger.info(\n            f\"Batch removing chunks for {len(file_paths)} files from hybrid indices\"\n        )\n\n        removed_count = 0\n        dense_failed = False\n        bm25_failed = False\n\n        # Remove from dense index\n        if hasattr(self.dense_index, \"remove_multiple_files\"):\n            try:\n                removed_dense = self.dense_index.remove_multiple_files(\n                    file_paths, project_name\n                )\n                removed_count += removed_dense\n                self._logger.info(\n                    f\"Batch removed {removed_dense} chunks from dense (FAISS) index\"\n                )\n            except Exception as e:\n                self._logger.error(f\"Failed to batch remove from dense index: {e}\")\n                import traceback\n                self._logger.error(traceback.format_exc())\n                dense_failed = True\n\n        # Remove from BM25 index\n        if hasattr(self.bm25_index, \"remove_multiple_files\"):\n            try:\n                removed_bm25 = self.bm25_index.remove_multiple_files(\n                    file_paths, project_name\n                )\n                removed_count += removed_bm25\n                self._logger.info(\n                    f\"Batch removed {removed_bm25} chunks from BM25 index\"\n                )\n            except Exception as e:\n                self._logger.error(f\"Failed to batch remove from BM25 index: {e}\")\n                import traceback\n                self._logger.error(traceback.format_exc())\n                bm25_failed = True\n        else:\n            self._logger.warning(\n                \"BM25 index does not support batch file chunk removal\"\n            )\n\n        # If both failed, raise exception to trigger error recovery\n        if dense_failed and bm25_failed:\n            raise RuntimeError(\n                \"Batch removal failed for both dense and BM25 indices. \"\n                \"Indices may be in corrupted state.\"\n            )\n\n        self._logger.info(\n            f\"Batch removed {removed_count} total chunks for {len(file_paths)} files\"\n        )\n        return removed_count\n\n    def save_index(self) -> None:\n        \"\"\"\n        Save both BM25 and dense indices to disk.\n        Compatible with incremental indexer interface.\n        \"\"\"\n        self._logger.info(\"Saving hybrid indices\")\n\n        try:\n            # Save both indices\n            self.save_indices()\n            self._logger.info(\"Successfully saved hybrid indices\")\n        except Exception as e:\n            self._logger.error(f\"Failed to save hybrid indices: {e}\")\n            raise\n\n    def _verify_bm25_files(self):\n        \"\"\"Verify BM25 files exist and are non-empty.\"\"\"\n        bm25_dir = Path(self.bm25_index.storage_dir)\n        expected_files = [\"bm25.index\", \"bm25_docs.json\", \"bm25_metadata.json\"]\n\n        self._logger.info(f\"[VERIFY] Checking BM25 files in: {bm25_dir}\")\n\n        for filename in expected_files:\n            filepath = bm25_dir / filename\n            if filepath.exists():\n                size = filepath.stat().st_size\n                if size == 0:\n                    self._logger.error(f\"[VERIFY] {filename} exists but is EMPTY\")\n                else:\n                    self._logger.info(f\"[VERIFY] {filename}: {size} bytes\")\n            else:\n                self._logger.error(f\"[VERIFY] {filename} does NOT exist\")\n\n        # Log overall BM25 directory status\n        if bm25_dir.exists():\n            files = list(bm25_dir.iterdir())\n            self._logger.info(\n                f\"[VERIFY] BM25 files after save: {[f.name for f in files]}\"\n            )\n            for f in files:\n                self._logger.info(f\"[VERIFY] {f.name}: {f.stat().st_size} bytes\")\n        else:\n            self._logger.error(\"[VERIFY] BM25 directory does not exist after save!\")",
              "content_preview": "class HybridSearcher:\n    \"\"\"Orchestrates BM25 + dense search with GPU awareness and parallel execution.\"\"\"\n\n    def __init__(\n        self,\n        storage_dir: str,\n        embedder=None,\n        bm...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015391705069124424,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
            "score": 10.156,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 1284,
              "end_line": 1363,
              "name": "configure_search_mode",
              "parent_name": null,
              "docstring": "Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes\n    \"\"\"\n    try:\n        # Validate search mode\n        valid_modes = [\"hybrid\", \"semantic\", \"bm25\", \"auto\"]\n        if search_mode not in valid_modes:\n            return json.dumps(\n                {\n                    \"error\": f\"Invalid search_mode '{search_mode}'. Must be one of: {valid_modes}\"\n                }\n            )\n\n        # Validate weights\n        if not (0.0 <= bm25_weight <= 1.0) or not (0.0 <= dense_weight <= 1.0):\n            return json.dumps({\"error\": \"Weights must be between 0.0 and 1.0\"})\n\n        # Normalize weights if they don't sum to 1.0\n        total_weight = bm25_weight + dense_weight\n        if total_weight > 0:\n            bm25_weight = bm25_weight / total_weight\n            dense_weight = dense_weight / total_weight\n\n        # Get current config and update\n        config_manager = get_config_manager()\n        config = config_manager.load_config()\n\n        # Update configuration\n        config.default_search_mode = search_mode\n        config.enable_hybrid_search = search_mode == \"hybrid\"\n        config.bm25_weight = bm25_weight\n        config.dense_weight = dense_weight\n        config.use_parallel_search = enable_parallel\n\n        # Save configuration\n        config_manager.save_config(config)\n\n        # Reset searcher to pick up new configuration\n        global _searcher\n        _searcher = None\n\n        response = {\n            \"success\": True,\n            \"message\": \"Search configuration updated successfully\",\n            \"new_config\": {\n                \"search_mode\": search_mode,\n                \"enable_hybrid_search\": config.enable_hybrid_search,\n                \"bm25_weight\": round(bm25_weight, 3),\n                \"dense_weight\": round(dense_weight, 3),\n                \"use_parallel_search\": enable_parallel,\n            },\n            \"note\": \"Changes will take effect on next search. Searcher will be reinitialized.\",\n        }\n\n        logger.info(\n            f\"Search configuration updated: mode={search_mode}, \"\n            f\"weights=({bm25_weight:.3f}, {dense_weight:.3f}), parallel={enable_parallel}\"\n        )\n\n        return json.dumps(response, indent=2)\n\n    except Exception as e:\n        logger.error(f\"Error configuring search mode: {e}\")\n        return json.dumps({\"error\": str(e)})",
              "content_preview": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure s...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015147265077138851,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:668-694:method:_parallel_search",
            "score": 6.854,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 668,
              "end_line": 694,
              "name": "_parallel_search",
              "parent_name": "HybridSearcher",
              "docstring": "Execute BM25 and dense search in parallel.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def _parallel_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"Execute BM25 and dense search in parallel.\"\"\"\n        try:\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                # Submit both searches\n                bm25_future = executor.submit(\n                    self._search_bm25, query, k, min_bm25_score\n                )\n                dense_future = executor.submit(self._search_dense, query, k, filters)\n\n                # Wait for results\n                bm25_results = bm25_future.result()\n                dense_results = dense_future.result()\n\n                return bm25_results, dense_results\n\n        except Exception as e:\n            self._logger.warning(\n                f\"Parallel search failed, falling back to sequential: {e}\"\n            )\n            return self._sequential_search(query, k, min_bm25_score, filters)",
              "content_preview": "def _parallel_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"E...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01482142857142857,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
          "search\\hybrid_searcher.py:696-706:method:_sequential_search",
          "search\\hybrid_searcher.py:668-694:method:_parallel_search",
          "search\\hybrid_searcher.py:301-426:method:search",
          "tests\\integration\\test_hybrid_search_integration.py:289-321:decorated_definition:test_bm25_vs_dense_results_differ",
          "search\\bm25_index.py:114-717:class:BM25Index",
          "search\\hybrid_searcher.py:57-1252:class:HybridSearcher",
          "tests\\unit\\test_hybrid_search.py:273-294:decorated_definition:test_search_error_handling",
          "search\\hybrid_searcher.py:708-725:method:_search_bm25",
          "search\\bm25_index.py:252-297:method:search"
        ]
      },
      "multi_hop": {
        "time_ms": 41.01,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "mcp_server\\server.py:1366-1418:decorated_definition:get_search_config_status",
            "score": 10.364,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 1366,
              "end_line": 1418,
              "name": "get_search_config_status",
              "parent_name": null,
              "docstring": "Get current search configuration status and available options.\n\n    Returns:\n        JSON with current configuration and available settings",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef get_search_config_status() -> str:\n    \"\"\"\n    Get current search configuration status and available options.\n\n    Returns:\n        JSON with current configuration and available settings\n    \"\"\"\n    try:\n        config = get_search_config()\n        config_manager = get_config_manager()\n\n        # Check current searcher type\n        current_searcher_type = \"None\"\n        if _searcher:\n            current_searcher_type = type(_searcher).__name__\n\n        response = {\n            \"current_configuration\": {\n                \"default_search_mode\": config.default_search_mode,\n                \"enable_hybrid_search\": config.enable_hybrid_search,\n                \"bm25_weight\": config.bm25_weight,\n                \"dense_weight\": config.dense_weight,\n                \"use_parallel_search\": config.use_parallel_search,\n                \"rrf_k_parameter\": config.rrf_k_parameter,\n                \"prefer_gpu\": config.prefer_gpu,\n                \"enable_auto_reindex\": config.enable_auto_reindex,\n            },\n            \"runtime_status\": {\n                \"current_searcher_type\": current_searcher_type,\n                \"active_project\": _current_project or \"None\",\n                \"config_file\": config_manager.config_file,\n            },\n            \"available_modes\": {\n                \"hybrid\": \"BM25 + Dense vector search with RRF reranking (recommended)\",\n                \"semantic\": \"Dense vector search only\",\n                \"bm25\": \"Text-based sparse search only\",\n                \"auto\": \"Automatically choose based on query characteristics\",\n            },\n            \"environment_variables\": {\n                \"CLAUDE_SEARCH_MODE\": \"Override default search mode\",\n                \"CLAUDE_ENABLE_HYBRID\": \"Enable/disable hybrid search (true/false)\",\n                \"CLAUDE_BM25_WEIGHT\": \"BM25 weight (0.0-1.0)\",\n                \"CLAUDE_DENSE_WEIGHT\": \"Dense weight (0.0-1.0)\",\n                \"CLAUDE_USE_PARALLEL\": \"Enable parallel search (true/false)\",\n            },\n        }\n\n        return json.dumps(response, indent=2)\n\n    except Exception as e:\n        logger.error(f\"Error getting search config status: {e}\")\n        return json.dumps({\"error\": str(e)})",
              "content_preview": "@mcp.tool()\ndef get_search_config_status() -> str:\n    \"\"\"\n    Get current search configuration status and available options.\n\n    Returns:\n        JSON with current configuration and available settin...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012679826028772165,
              "appears_in_lists": 2,
              "final_rank": 11
            }
          },
          {
            "doc_id": "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
            "score": 10.156,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 1284,
              "end_line": 1363,
              "name": "configure_search_mode",
              "parent_name": null,
              "docstring": "Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure search mode and hybrid search parameters.\n\n    Args:\n        search_mode: Default search mode - \"hybrid\", \"semantic\", \"bm25\", or \"auto\"\n        bm25_weight: Weight for BM25 sparse search (0.0 to 1.0)\n        dense_weight: Weight for dense vector search (0.0 to 1.0)\n        enable_parallel: Enable parallel BM25 + Dense search execution\n\n    Returns:\n        JSON confirmation of configuration changes\n    \"\"\"\n    try:\n        # Validate search mode\n        valid_modes = [\"hybrid\", \"semantic\", \"bm25\", \"auto\"]\n        if search_mode not in valid_modes:\n            return json.dumps(\n                {\n                    \"error\": f\"Invalid search_mode '{search_mode}'. Must be one of: {valid_modes}\"\n                }\n            )\n\n        # Validate weights\n        if not (0.0 <= bm25_weight <= 1.0) or not (0.0 <= dense_weight <= 1.0):\n            return json.dumps({\"error\": \"Weights must be between 0.0 and 1.0\"})\n\n        # Normalize weights if they don't sum to 1.0\n        total_weight = bm25_weight + dense_weight\n        if total_weight > 0:\n            bm25_weight = bm25_weight / total_weight\n            dense_weight = dense_weight / total_weight\n\n        # Get current config and update\n        config_manager = get_config_manager()\n        config = config_manager.load_config()\n\n        # Update configuration\n        config.default_search_mode = search_mode\n        config.enable_hybrid_search = search_mode == \"hybrid\"\n        config.bm25_weight = bm25_weight\n        config.dense_weight = dense_weight\n        config.use_parallel_search = enable_parallel\n\n        # Save configuration\n        config_manager.save_config(config)\n\n        # Reset searcher to pick up new configuration\n        global _searcher\n        _searcher = None\n\n        response = {\n            \"success\": True,\n            \"message\": \"Search configuration updated successfully\",\n            \"new_config\": {\n                \"search_mode\": search_mode,\n                \"enable_hybrid_search\": config.enable_hybrid_search,\n                \"bm25_weight\": round(bm25_weight, 3),\n                \"dense_weight\": round(dense_weight, 3),\n                \"use_parallel_search\": enable_parallel,\n            },\n            \"note\": \"Changes will take effect on next search. Searcher will be reinitialized.\",\n        }\n\n        logger.info(\n            f\"Search configuration updated: mode={search_mode}, \"\n            f\"weights=({bm25_weight:.3f}, {dense_weight:.3f}), parallel={enable_parallel}\"\n        )\n\n        return json.dumps(response, indent=2)\n\n    except Exception as e:\n        logger.error(f\"Error configuring search mode: {e}\")\n        return json.dumps({\"error\": str(e)})",
              "content_preview": "@mcp.tool()\ndef configure_search_mode(\n    search_mode: str = \"hybrid\",\n    bm25_weight: float = 0.4,\n    dense_weight: float = 0.6,\n    enable_parallel: bool = True,\n) -> str:\n    \"\"\"\n    Configure s...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015147265077138851,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:668-694:method:_parallel_search",
            "score": 6.854,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 668,
              "end_line": 694,
              "name": "_parallel_search",
              "parent_name": "HybridSearcher",
              "docstring": "Execute BM25 and dense search in parallel.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def _parallel_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"Execute BM25 and dense search in parallel.\"\"\"\n        try:\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                # Submit both searches\n                bm25_future = executor.submit(\n                    self._search_bm25, query, k, min_bm25_score\n                )\n                dense_future = executor.submit(self._search_dense, query, k, filters)\n\n                # Wait for results\n                bm25_results = bm25_future.result()\n                dense_results = dense_future.result()\n\n                return bm25_results, dense_results\n\n        except Exception as e:\n            self._logger.warning(\n                f\"Parallel search failed, falling back to sequential: {e}\"\n            )\n            return self._sequential_search(query, k, min_bm25_score, filters)",
              "content_preview": "def _parallel_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"E...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01482142857142857,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:696-706:method:_sequential_search",
            "score": 6.84,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 696,
              "end_line": 706,
              "name": "_sequential_search",
              "parent_name": "HybridSearcher",
              "docstring": "Execute BM25 and dense search sequentially.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def _sequential_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"\"Execute BM25 and dense search sequentially.\"\"\"\n        bm25_results = self._search_bm25(query, k, min_bm25_score)\n        dense_results = self._search_dense(query, k, filters)\n        return bm25_results, dense_results",
              "content_preview": "def _sequential_search(\n        self,\n        query: str,\n        k: int,\n        min_bm25_score: float,\n        filters: Optional[Dict[str, Any]],\n    ) -> Tuple[List[Tuple], List[Tuple]]:\n        \"\"...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015528846153846153,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "search\\hybrid_searcher.py:301-426:method:search",
            "score": 6.82,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\search\\hybrid_searcher.py",
              "relative_path": "search\\hybrid_searcher.py",
              "folder_structure": [
                "search"
              ],
              "chunk_type": "method",
              "start_line": 301,
              "end_line": 426,
              "name": "search",
              "parent_name": "HybridSearcher",
              "docstring": "Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Search using configurable approach (hybrid, semantic-only, or BM25-only).\n\n        Args:\n            query: Search query\n            k: Number of results to return\n            search_mode: Search mode - \"hybrid\", \"semantic\", or \"bm25\"\n            use_parallel: Whether to run BM25 and dense search in parallel (hybrid mode only)\n            min_bm25_score: Minimum BM25 score threshold\n            filters: Optional filters for dense search\n\n        Returns:\n            Search results (reranked for hybrid mode, direct for single modes)\n        \"\"\"\n        # Check if indices are ready based on search mode\n        if search_mode == \"bm25\":\n            if self.bm25_index.is_empty:\n                self._logger.warning(\"BM25 search requested but BM25 index is empty\")\n                return []\n        elif search_mode == \"semantic\":\n            if not self.dense_index.index or self.dense_index.index.ntotal == 0:\n                self._logger.warning(\n                    \"Semantic search requested but dense index is empty\"\n                )\n                return []\n        else:  # hybrid\n            if not self.is_ready:\n                self._logger.warning(\"Hybrid search not ready - indices may be empty\")\n                return []\n\n        self._logger.debug(f\"{search_mode.title()} search for: '{query}' (k={k})\")\n\n        start_time = time.time()\n\n        # Handle different search modes\n        if search_mode == \"bm25\":\n            # BM25-only search\n            bm25_results = self._search_bm25(query, k, min_bm25_score)\n            # Convert BM25 results to SearchResult format\n            final_results = self._convert_bm25_to_search_results(bm25_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        elif search_mode == \"semantic\":\n            # Dense-only search\n            dense_results = self._search_dense(query, k, filters)\n            # Convert dense results to SearchResult format\n            final_results = self._convert_dense_to_search_results(dense_results)\n            rerank_time = 0.0  # No reranking for single mode\n\n        else:  # hybrid mode\n            search_k = k * 2  # Get more results for better reranking\n\n            if use_parallel and not self._is_shutdown:\n                # Parallel execution\n                bm25_results, dense_results = self._parallel_search(\n                    query, search_k, min_bm25_score, filters\n                )\n            else:\n                # Sequential execution\n                bm25_results, dense_results = self._sequential_search(\n                    query, search_k, min_bm25_score, filters\n                )\n\n            # Rerank results\n            rerank_start = time.time()\n            self._logger.debug(\n                f\"[RERANK] Using weights: BM25={self.bm25_weight}, Dense={self.dense_weight}, \"\n                f\"BM25_results={len(bm25_results)}, Dense_results={len(dense_results)}\"\n            )\n            final_results = self.reranker.rerank_simple(\n                bm25_results=bm25_results,\n                dense_results=dense_results,\n                max_results=k,\n                bm25_weight=self.bm25_weight,\n                dense_weight=self.dense_weight,\n            )\n            rerank_time = time.time() - rerank_start\n            self._logger.debug(\n                f\"[RERANK] Produced {len(final_results)} results in {rerank_time:.3f}s\"\n            )\n\n        # Update statistics\n        total_time = time.time() - start_time\n        self._search_stats[\"total_searches\"] += 1\n        self._search_stats[\"rerank_time\"] += rerank_time\n\n        if use_parallel:\n            parallel_time = max(\n                self._search_stats.get(\"last_bm25_time\", 0),\n                self._search_stats.get(\"last_dense_time\", 0),\n            )\n            sequential_time = self._search_stats.get(\n                \"last_bm25_time\", 0\n            ) + self._search_stats.get(\"last_dense_time\", 0)\n            if sequential_time > 0:\n                efficiency = 1.0 - (parallel_time / sequential_time)\n                self._search_stats[\"parallel_efficiency\"] = efficiency\n\n        # Mode-specific logging\n        if search_mode == \"bm25\":\n            self._logger.debug(\n                f\"BM25 search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        elif search_mode == \"semantic\":\n            self._logger.debug(\n                f\"Semantic search complete: {len(final_results)} results, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n        else:  # hybrid\n            self._logger.debug(\n                f\"Hybrid search complete: {len(final_results)} results, \"\n                f\"BM25: {len(bm25_results)}, Dense: {len(dense_results)}, \"\n                f\"Total time: {total_time:.3f}s\"\n            )\n\n        return final_results",
              "content_preview": "def search(\n        self,\n        query: str,\n        k: int = 5,\n        search_mode: str = \"hybrid\",\n        use_parallel: bool = True,\n        min_bm25_score: float = 0.0,\n        filters: Optional...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015584415584415584,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          }
        ],
        "all_doc_ids": [
          "search\\reranker.py:139-176:method:rerank_simple",
          "mcp_server\\server.py:1284-1363:decorated_definition:configure_search_mode",
          "search\\hybrid_searcher.py:696-706:method:_sequential_search",
          "search\\hybrid_searcher.py:668-694:method:_parallel_search",
          "search\\hybrid_searcher.py:301-426:method:search",
          "tests\\integration\\test_hybrid_search_integration.py:289-321:decorated_definition:test_bm25_vs_dense_results_differ",
          "search\\hybrid_searcher.py:57-1252:class:HybridSearcher",
          "tests\\unit\\test_hybrid_search.py:414-450:decorated_definition:test_performance_tracking",
          "tests\\unit\\test_hybrid_search.py:273-294:decorated_definition:test_search_error_handling",
          "mcp_server\\server.py:1366-1418:decorated_definition:get_search_config_status"
        ],
        "unique_discoveries": [
          "search\\reranker.py:139-176:method:rerank_simple",
          "tests\\unit\\test_hybrid_search.py:414-450:decorated_definition:test_performance_tracking",
          "mcp_server\\server.py:1366-1418:decorated_definition:get_search_config_status"
        ]
      },
      "comparison": {
        "time_overhead_ms": 22.72,
        "time_overhead_pct": 124.3,
        "top5_overlap_count": 4,
        "top5_overlap_pct": 80.0,
        "unique_discovery_count": 3,
        "value_rating": "MEDIUM"
      }
    },
    {
      "query": "model dimension detection and validation",
      "single_hop": {
        "time_ms": 9.76,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\unit\\test_model_selection.py:188-208:class:TestModelDimensionValidation",
            "score": 14.856,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_model_selection.py",
              "relative_path": "tests\\unit\\test_model_selection.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "class",
              "start_line": 188,
              "end_line": 208,
              "name": "TestModelDimensionValidation",
              "parent_name": null,
              "docstring": "Test dimension validation in search configuration.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestModelDimensionValidation:\n    \"\"\"Test dimension validation in search configuration.\"\"\"\n\n    def test_gemma_dimension(self):\n        \"\"\"Test that Gemma uses 768 dimensions.\"\"\"\n        config = get_model_config(\"google/embeddinggemma-300m\")\n        assert config[\"dimension\"] == 768\n\n    def test_bge_m3_dimension(self):\n        \"\"\"Test that BGE-M3 uses 1024 dimensions.\"\"\"\n        config = get_model_config(\"BAAI/bge-m3\")\n        assert config[\"dimension\"] == 1024\n\n    def test_dimension_mismatch_detection(self):\n        \"\"\"Test that different models have different dimensions.\"\"\"\n        gemma_config = get_model_config(\"google/embeddinggemma-300m\")\n        bge_config = get_model_config(\"BAAI/bge-m3\")\n\n        assert gemma_config[\"dimension\"] != bge_config[\"dimension\"]\n        assert gemma_config[\"dimension\"] == 768\n        assert bge_config[\"dimension\"] == 1024",
              "content_preview": "class TestModelDimensionValidation:\n    \"\"\"Test dimension validation in search configuration.\"\"\"\n\n    def test_gemma_dimension(self):\n        \"\"\"Test that Gemma uses 768 dimensions.\"\"\"\n        config ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01639344262295082,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\unit\\test_model_selection.py:151-157:method:test_model_config_detection_gemma",
            "score": 9.774,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_model_selection.py",
              "relative_path": "tests\\unit\\test_model_selection.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "method",
              "start_line": 151,
              "end_line": 157,
              "name": "test_model_config_detection_gemma",
              "parent_name": "TestCodeEmbedderModelSupport",
              "docstring": "Test automatic config detection for Gemma.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_model_config_detection_gemma(self):\n        \"\"\"Test automatic config detection for Gemma.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n        config = embedder._get_model_config()\n\n        assert config[\"dimension\"] == 768\n        assert config[\"prompt_name\"] == \"Retrieval-document\"",
              "content_preview": "def test_model_config_detection_gemma(self):\n        \"\"\"Test automatic config detection for Gemma.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n        config = embedder...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014864572047670641,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\integration\\test_model_switching.py:184-202:method:test_dimension_mismatch_detection",
            "score": 8.769,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_model_switching.py",
              "relative_path": "tests\\integration\\test_model_switching.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 184,
              "end_line": 202,
              "name": "test_dimension_mismatch_detection",
              "parent_name": "TestIndexManagerWithDifferentModels",
              "docstring": "Test that dimension mismatch is detected when loading index.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_dimension_mismatch_detection(self):\n        \"\"\"Test that dimension mismatch is detected when loading index.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Create index with Gemma (768 dim)\n            embedder_gemma = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n            indexer_gemma = CodeIndexManager(tmpdir, embedder=embedder_gemma)\n            indexer_gemma.create_index(768)\n            indexer_gemma.save_index()\n\n            # Try to load with BGE-M3 (1024 dim) - should detect mismatch\n            embedder_bge = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n            indexer_bge = CodeIndexManager(tmpdir, embedder=embedder_bge)\n\n            # Access index property to trigger loading\n            index = indexer_bge.index\n\n            # Index should be None or empty due to dimension mismatch\n            # The mismatch detection clears the index internally\n            assert index is None or index.ntotal == 0  # No vectors loaded",
              "content_preview": "def test_dimension_mismatch_detection(self):\n        \"\"\"Test that dimension mismatch is detected when loading index.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Create index...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014503205128205129,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "tests\\unit\\test_model_selection.py:131-185:class:TestCodeEmbedderModelSupport",
            "score": 9.747,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_model_selection.py",
              "relative_path": "tests\\unit\\test_model_selection.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "class",
              "start_line": 131,
              "end_line": 185,
              "name": "TestCodeEmbedderModelSupport",
              "parent_name": null,
              "docstring": "Test CodeEmbedder multi-model support.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestCodeEmbedderModelSupport:\n    \"\"\"Test CodeEmbedder multi-model support.\"\"\"\n\n    def test_default_model_is_gemma(self):\n        \"\"\"Test that default model is Gemma (backward compatibility).\"\"\"\n        embedder = CodeEmbedder()\n        assert \"gemma\" in embedder.model_name.lower()\n\n    def test_embedder_with_bge_m3(self):\n        \"\"\"Test creating embedder with BGE-M3.\"\"\"\n        embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n        assert embedder.model_name == \"BAAI/bge-m3\"\n\n    def test_get_supported_models(self):\n        \"\"\"Test getting list of supported models.\"\"\"\n        models = CodeEmbedder.get_supported_models()\n        assert isinstance(models, dict)\n        assert \"google/embeddinggemma-300m\" in models\n        assert \"BAAI/bge-m3\" in models\n\n    def test_model_config_detection_gemma(self):\n        \"\"\"Test automatic config detection for Gemma.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n        config = embedder._get_model_config()\n\n        assert config[\"dimension\"] == 768\n        assert config[\"prompt_name\"] == \"Retrieval-document\"\n\n    def test_model_config_detection_bge_m3(self):\n        \"\"\"Test automatic config detection for BGE-M3.\"\"\"\n        embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n        config = embedder._get_model_config()\n\n        assert config[\"dimension\"] == 1024\n        assert config[\"prompt_name\"] is None  # No prompts for BGE-M3\n\n    def test_model_config_detection_unknown_model(self):\n        \"\"\"Test fallback config for unknown models.\"\"\"\n        embedder = CodeEmbedder(model_name=\"unknown/test-model\")\n        config = embedder._get_model_config()\n\n        # Should fall back to defaults\n        assert config[\"dimension\"] == 768\n        assert config[\"prompt_name\"] is None\n\n    def test_model_config_caching(self):\n        \"\"\"Test that model config is cached after first access.\"\"\"\n        embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n\n        # First access\n        config1 = embedder._get_model_config()\n        # Second access should return cached version\n        config2 = embedder._get_model_config()\n\n        assert config1 is config2  # Same object reference",
              "content_preview": "class TestCodeEmbedderModelSupport:\n    \"\"\"Test CodeEmbedder multi-model support.\"\"\"\n\n    def test_default_model_is_gemma(self):\n        \"\"\"Test that default model is Gemma (backward compatibility).\"\"...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014251207729468598,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "tests\\integration\\test_model_switching.py:159-232:class:TestIndexManagerWithDifferentModels",
            "score": 10.987,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_model_switching.py",
              "relative_path": "tests\\integration\\test_model_switching.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "class",
              "start_line": 159,
              "end_line": 232,
              "name": "TestIndexManagerWithDifferentModels",
              "parent_name": null,
              "docstring": "Test CodeIndexManager with different embedding models.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestIndexManagerWithDifferentModels:\n    \"\"\"Test CodeIndexManager with different embedding models.\"\"\"\n\n    def test_index_manager_with_gemma(self):\n        \"\"\"Test creating index manager with Gemma embedder.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n            indexer = CodeIndexManager(tmpdir, embedder=embedder)\n\n            # Create index with correct dimension\n            indexer.create_index(768)\n            assert indexer._index is not None\n            assert indexer._index.d == 768\n\n    def test_index_manager_with_bge_m3(self):\n        \"\"\"Test creating index manager with BGE-M3 embedder.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n            indexer = CodeIndexManager(tmpdir, embedder=embedder)\n\n            # Create index with correct dimension\n            indexer.create_index(1024)\n            assert indexer._index is not None\n            assert indexer._index.d == 1024\n\n    def test_dimension_mismatch_detection(self):\n        \"\"\"Test that dimension mismatch is detected when loading index.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Create index with Gemma (768 dim)\n            embedder_gemma = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n            indexer_gemma = CodeIndexManager(tmpdir, embedder=embedder_gemma)\n            indexer_gemma.create_index(768)\n            indexer_gemma.save_index()\n\n            # Try to load with BGE-M3 (1024 dim) - should detect mismatch\n            embedder_bge = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n            indexer_bge = CodeIndexManager(tmpdir, embedder=embedder_bge)\n\n            # Access index property to trigger loading\n            index = indexer_bge.index\n\n            # Index should be None or empty due to dimension mismatch\n            # The mismatch detection clears the index internally\n            assert index is None or index.ntotal == 0  # No vectors loaded\n\n    def test_model_metadata_saved(self, sample_code_chunk):\n        \"\"\"Test that model metadata is saved with index.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n            indexer = CodeIndexManager(tmpdir, embedder=embedder)\n            indexer.create_index(768)\n\n            # Add a sample embedding\n            result = embedder.embed_chunk(sample_code_chunk)\n            indexer.add_embeddings([result])\n            indexer.save_index()\n\n            # Explicitly close the metadata database before cleanup\n            if indexer._metadata_db is not None:\n                indexer._metadata_db.close()\n                indexer._metadata_db = None\n\n            # Check that model_info.json was created\n            model_info_path = Path(tmpdir) / \"model_info.json\"\n            assert model_info_path.exists()\n\n            # Verify contents\n            import json\n\n            with open(model_info_path) as f:\n                model_info = json.load(f)\n\n            assert model_info[\"model_name\"] == \"google/embeddinggemma-300m\"\n            assert model_info[\"embedding_dimension\"] == 768",
              "content_preview": "class TestIndexManagerWithDifferentModels:\n    \"\"\"Test CodeIndexManager with different embedding models.\"\"\"\n\n    def test_index_manager_with_gemma(self):\n        \"\"\"Test creating index manager with Ge...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014243943191311611,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "tests\\integration\\test_model_switching.py:159-232:class:TestIndexManagerWithDifferentModels",
          "tests\\integration\\test_model_switching.py:137-147:method:test_dimension_difference",
          "tests\\integration\\test_model_switching.py:184-202:method:test_dimension_mismatch_detection",
          "tests\\unit\\test_model_selection.py:159-165:method:test_model_config_detection_bge_m3",
          "tests\\unit\\test_model_selection.py:131-185:class:TestCodeEmbedderModelSupport",
          "tests\\unit\\test_model_selection.py:201-208:method:test_dimension_mismatch_detection",
          "tests\\integration\\test_model_switching.py:134-156:class:TestModelSwitching",
          "tests\\unit\\test_model_selection.py:188-208:class:TestModelDimensionValidation",
          "tests\\unit\\test_model_selection.py:196-199:method:test_bge_m3_dimension",
          "tests\\unit\\test_model_selection.py:151-157:method:test_model_config_detection_gemma"
        ]
      },
      "multi_hop": {
        "time_ms": 49.23,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\unit\\test_model_selection.py:188-208:class:TestModelDimensionValidation",
            "score": 14.856,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_model_selection.py",
              "relative_path": "tests\\unit\\test_model_selection.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "class",
              "start_line": 188,
              "end_line": 208,
              "name": "TestModelDimensionValidation",
              "parent_name": null,
              "docstring": "Test dimension validation in search configuration.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestModelDimensionValidation:\n    \"\"\"Test dimension validation in search configuration.\"\"\"\n\n    def test_gemma_dimension(self):\n        \"\"\"Test that Gemma uses 768 dimensions.\"\"\"\n        config = get_model_config(\"google/embeddinggemma-300m\")\n        assert config[\"dimension\"] == 768\n\n    def test_bge_m3_dimension(self):\n        \"\"\"Test that BGE-M3 uses 1024 dimensions.\"\"\"\n        config = get_model_config(\"BAAI/bge-m3\")\n        assert config[\"dimension\"] == 1024\n\n    def test_dimension_mismatch_detection(self):\n        \"\"\"Test that different models have different dimensions.\"\"\"\n        gemma_config = get_model_config(\"google/embeddinggemma-300m\")\n        bge_config = get_model_config(\"BAAI/bge-m3\")\n\n        assert gemma_config[\"dimension\"] != bge_config[\"dimension\"]\n        assert gemma_config[\"dimension\"] == 768\n        assert bge_config[\"dimension\"] == 1024",
              "content_preview": "class TestModelDimensionValidation:\n    \"\"\"Test dimension validation in search configuration.\"\"\"\n\n    def test_gemma_dimension(self):\n        \"\"\"Test that Gemma uses 768 dimensions.\"\"\"\n        config ...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01639344262295082,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_model_switching.py:159-232:class:TestIndexManagerWithDifferentModels",
            "score": 10.987,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_model_switching.py",
              "relative_path": "tests\\integration\\test_model_switching.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "class",
              "start_line": 159,
              "end_line": 232,
              "name": "TestIndexManagerWithDifferentModels",
              "parent_name": null,
              "docstring": "Test CodeIndexManager with different embedding models.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestIndexManagerWithDifferentModels:\n    \"\"\"Test CodeIndexManager with different embedding models.\"\"\"\n\n    def test_index_manager_with_gemma(self):\n        \"\"\"Test creating index manager with Gemma embedder.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n            indexer = CodeIndexManager(tmpdir, embedder=embedder)\n\n            # Create index with correct dimension\n            indexer.create_index(768)\n            assert indexer._index is not None\n            assert indexer._index.d == 768\n\n    def test_index_manager_with_bge_m3(self):\n        \"\"\"Test creating index manager with BGE-M3 embedder.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            embedder = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n            indexer = CodeIndexManager(tmpdir, embedder=embedder)\n\n            # Create index with correct dimension\n            indexer.create_index(1024)\n            assert indexer._index is not None\n            assert indexer._index.d == 1024\n\n    def test_dimension_mismatch_detection(self):\n        \"\"\"Test that dimension mismatch is detected when loading index.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Create index with Gemma (768 dim)\n            embedder_gemma = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n            indexer_gemma = CodeIndexManager(tmpdir, embedder=embedder_gemma)\n            indexer_gemma.create_index(768)\n            indexer_gemma.save_index()\n\n            # Try to load with BGE-M3 (1024 dim) - should detect mismatch\n            embedder_bge = CodeEmbedder(model_name=\"BAAI/bge-m3\")\n            indexer_bge = CodeIndexManager(tmpdir, embedder=embedder_bge)\n\n            # Access index property to trigger loading\n            index = indexer_bge.index\n\n            # Index should be None or empty due to dimension mismatch\n            # The mismatch detection clears the index internally\n            assert index is None or index.ntotal == 0  # No vectors loaded\n\n    def test_model_metadata_saved(self, sample_code_chunk):\n        \"\"\"Test that model metadata is saved with index.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            embedder = CodeEmbedder(model_name=\"google/embeddinggemma-300m\")\n            indexer = CodeIndexManager(tmpdir, embedder=embedder)\n            indexer.create_index(768)\n\n            # Add a sample embedding\n            result = embedder.embed_chunk(sample_code_chunk)\n            indexer.add_embeddings([result])\n            indexer.save_index()\n\n            # Explicitly close the metadata database before cleanup\n            if indexer._metadata_db is not None:\n                indexer._metadata_db.close()\n                indexer._metadata_db = None\n\n            # Check that model_info.json was created\n            model_info_path = Path(tmpdir) / \"model_info.json\"\n            assert model_info_path.exists()\n\n            # Verify contents\n            import json\n\n            with open(model_info_path) as f:\n                model_info = json.load(f)\n\n            assert model_info[\"model_name\"] == \"google/embeddinggemma-300m\"\n            assert model_info[\"embedding_dimension\"] == 768",
              "content_preview": "class TestIndexManagerWithDifferentModels:\n    \"\"\"Test CodeIndexManager with different embedding models.\"\"\"\n\n    def test_index_manager_with_gemma(self):\n        \"\"\"Test creating index manager with Ge...",
              "project_name": "claude-context-local",
              "rrf_score": 0.014243943191311611,
              "appears_in_lists": 2,
              "final_rank": 6
            }
          },
          {
            "doc_id": "mcp_server\\server.py:1458-1560:decorated_definition:switch_embedding_model",
            "score": 10.511,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "decorated_definition",
              "start_line": 1458,
              "end_line": 1560,
              "name": "switch_embedding_model",
              "parent_name": null,
              "docstring": "Switch to a different embedding model without deleting existing indices.\n\n    Per-model indices enable instant switching - if you've already indexed a project\n    with a model, switching back to it requires no re-indexing.\n\n    Args:\n        model_name: Model identifier from MODEL_REGISTRY (e.g., \"BAAI/bge-m3\")\n\n    Returns:\n        JSON confirmation with model info and existing indices status",
              "decorators": [
                "@mcp.tool()"
              ],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "@mcp.tool()\ndef switch_embedding_model(model_name: str) -> str:\n    \"\"\"\n    Switch to a different embedding model without deleting existing indices.\n\n    Per-model indices enable instant switching - if you've already indexed a project\n    with a model, switching back to it requires no re-indexing.\n\n    Args:\n        model_name: Model identifier from MODEL_REGISTRY (e.g., \"BAAI/bge-m3\")\n\n    Returns:\n        JSON confirmation with model info and existing indices status\n    \"\"\"\n    try:\n        from search.config import MODEL_REGISTRY\n\n        # Validate model name\n        if model_name not in MODEL_REGISTRY:\n            available_models = list(MODEL_REGISTRY.keys())\n            return json.dumps(\n                {\n                    \"error\": f\"Invalid model '{model_name}'. Available models: {available_models}\",\n                    \"available_models\": available_models,\n                }\n            )\n\n        # Get model specs\n        old_config = get_search_config()\n        old_model = old_config.embedding_model_name\n        old_dimension = old_config.model_dimension\n\n        new_specs = MODEL_REGISTRY[model_name]\n        new_dimension = new_specs[\"dimension\"]\n\n        # Update configuration\n        config_manager = get_config_manager()\n        config = config_manager.load_config()\n        config.embedding_model_name = model_name\n        config.model_dimension = new_dimension\n        config_manager.save_config(config)\n\n        # Reset global components to pick up new model\n        global _embedder, _index_manager, _searcher\n        _embedder = None\n        _index_manager = None\n        _searcher = None\n\n        # Check for existing indices in new dimension\n        base_dir = get_storage_dir()\n        projects_dir = base_dir / \"projects\"\n\n        existing_projects = []\n        if projects_dir.exists():\n            # Find all project directories with the new dimension suffix\n            for project_dir in projects_dir.iterdir():\n                if project_dir.is_dir() and project_dir.name.endswith(\n                    f\"_{new_dimension}d\"\n                ):\n                    existing_projects.append(project_dir.name)\n\n        response = {\n            \"success\": True,\n            \"message\": f\"Switched embedding model from {old_model} ({old_dimension}d) to {model_name} ({new_dimension}d)\",\n            \"old_model\": {\n                \"name\": old_model,\n                \"dimension\": old_dimension,\n            },\n            \"new_model\": {\n                \"name\": model_name,\n                \"dimension\": new_dimension,\n                \"max_context\": new_specs[\"max_context\"],\n                \"vram_gb\": new_specs.get(\"vram_gb\", \"Unknown\"),\n                \"description\": new_specs[\"description\"],\n            },\n            \"existing_indices\": {\n                \"count\": len(existing_projects),\n                \"projects\": existing_projects,\n                \"note\": (\n                    f\"{len(existing_projects)} projects already indexed with {new_dimension}d - no re-indexing needed!\"\n                    if existing_projects\n                    else f\"No existing {new_dimension}d indices found - projects will need indexing\"\n                ),\n            },\n            \"per_model_indices_info\": (\n                f\"Old {old_dimension}d indices preserved. \"\n                f\"Switching back to {old_model} will be instant (no re-indexing).\"\n            ),\n        }\n\n        logger.info(\n            f\"[PER_MODEL_INDICES] Switched model: {old_model} ({old_dimension}d) \u2192 {model_name} ({new_dimension}d)\"\n        )\n        if existing_projects:\n            logger.info(\n                f\"[PER_MODEL_INDICES] Found {len(existing_projects)} existing {new_dimension}d projects - instant activation!\"\n            )\n\n        return json.dumps(response, indent=2)\n\n    except Exception as e:\n        logger.error(f\"Error switching embedding model: {e}\")\n        return json.dumps({\"error\": str(e)})",
              "content_preview": "@mcp.tool()\ndef switch_embedding_model(model_name: str) -> str:\n    \"\"\"\n    Switch to a different embedding model without deleting existing indices.\n\n    Per-model indices enable instant switching - i...",
              "project_name": "claude-context-local",
              "rrf_score": 0.013478915662650602,
              "appears_in_lists": 2,
              "final_rank": 8
            }
          },
          {
            "doc_id": "mcp_server\\server.py:78-119:function:get_project_storage_dir",
            "score": 10.398,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\mcp_server\\server.py",
              "relative_path": "mcp_server\\server.py",
              "folder_structure": [
                "mcp_server"
              ],
              "chunk_type": "function",
              "start_line": 78,
              "end_line": 119,
              "name": "get_project_storage_dir",
              "parent_name": null,
              "docstring": "Get or create project-specific storage directory with per-model dimension suffix.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def get_project_storage_dir(project_path: str) -> Path:\n    \"\"\"Get or create project-specific storage directory with per-model dimension suffix.\"\"\"\n    base_dir = get_storage_dir()\n\n    # Create a safe directory name from project path\n    import hashlib\n    from datetime import datetime\n\n    project_path = Path(project_path).resolve()\n    project_name = project_path.name\n    project_hash = hashlib.md5(str(project_path).encode()).hexdigest()[:8]\n\n    # Detect embedding dimension from current config for per-model index storage\n    from search.config import MODEL_REGISTRY, get_search_config\n\n    config = get_search_config()\n    model_name = config.embedding_model_name\n    dimension = MODEL_REGISTRY.get(model_name, {}).get(\"dimension\", 768)\n\n    # Use project name + hash + dimension for per-model index separation\n    project_dir = base_dir / \"projects\" / f\"{project_name}_{project_hash}_{dimension}d\"\n    project_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(\n        f\"[PER_MODEL_INDICES] Using storage: {project_dir.name} (model: {model_name}, dimension: {dimension}d)\"\n    )\n\n    # Store project info with model metadata\n    project_info_file = project_dir / \"project_info.json\"\n    if not project_info_file.exists():\n        project_info = {\n            \"project_name\": project_name,\n            \"project_path\": str(project_path),\n            \"project_hash\": project_hash,\n            \"embedding_model\": model_name,\n            \"model_dimension\": dimension,\n            \"created_at\": datetime.now().isoformat(),\n        }\n        with open(project_info_file, \"w\") as f:\n            json.dump(project_info, f, indent=2)\n\n    return project_dir",
              "content_preview": "def get_project_storage_dir(project_path: str) -> Path:\n    \"\"\"Get or create project-specific storage directory with per-model dimension suffix.\"\"\"\n    base_dir = get_storage_dir()\n\n    # Create a saf...",
              "project_name": "claude-context-local",
              "rrf_score": 0.013212669683257919,
              "appears_in_lists": 2,
              "final_rank": 11
            }
          },
          {
            "doc_id": "tests\\unit\\test_model_selection.py:48-82:class:TestSearchConfigModelFields",
            "score": 10.058,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\unit\\test_model_selection.py",
              "relative_path": "tests\\unit\\test_model_selection.py",
              "folder_structure": [
                "tests",
                "unit"
              ],
              "chunk_type": "class",
              "start_line": 48,
              "end_line": 82,
              "name": "TestSearchConfigModelFields",
              "parent_name": null,
              "docstring": "Test SearchConfig dataclass with model fields.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestSearchConfigModelFields:\n    \"\"\"Test SearchConfig dataclass with model fields.\"\"\"\n\n    def test_default_config_uses_gemma(self):\n        \"\"\"Test that default config uses Gemma (backward compatibility).\"\"\"\n        config = SearchConfig()\n        assert config.embedding_model_name == \"google/embeddinggemma-300m\"\n        assert config.model_dimension == 768\n\n    def test_config_with_bge_m3(self):\n        \"\"\"Test creating config with BGE-M3.\"\"\"\n        config = SearchConfig(\n            embedding_model_name=\"BAAI/bge-m3\",\n            model_dimension=1024,\n        )\n        assert config.embedding_model_name == \"BAAI/bge-m3\"\n        assert config.model_dimension == 1024\n\n    def test_config_to_dict(self):\n        \"\"\"Test config serialization.\"\"\"\n        config = SearchConfig(embedding_model_name=\"BAAI/bge-m3\")\n        config_dict = config.to_dict()\n        assert \"embedding_model_name\" in config_dict\n        assert config_dict[\"embedding_model_name\"] == \"BAAI/bge-m3\"\n\n    def test_config_from_dict(self):\n        \"\"\"Test config deserialization.\"\"\"\n        config_dict = {\n            \"embedding_model_name\": \"BAAI/bge-m3\",\n            \"model_dimension\": 1024,\n            \"default_search_mode\": \"hybrid\",\n        }\n        config = SearchConfig.from_dict(config_dict)\n        assert config.embedding_model_name == \"BAAI/bge-m3\"\n        assert config.model_dimension == 1024",
              "content_preview": "class TestSearchConfigModelFields:\n    \"\"\"Test SearchConfig dataclass with model fields.\"\"\"\n\n    def test_default_config_uses_gemma(self):\n        \"\"\"Test that default config uses Gemma (backward comp...",
              "project_name": "claude-context-local",
              "rrf_score": 0.012788331071913161,
              "appears_in_lists": 2,
              "final_rank": 13
            }
          }
        ],
        "all_doc_ids": [
          "tests\\integration\\test_model_switching.py:159-232:class:TestIndexManagerWithDifferentModels",
          "tests\\unit\\test_model_selection.py:48-82:class:TestSearchConfigModelFields",
          "tests\\unit\\test_model_selection.py:159-165:method:test_model_config_detection_bge_m3",
          "tests\\unit\\test_model_selection.py:131-185:class:TestCodeEmbedderModelSupport",
          "embeddings\\embedder.py:392-403:method:get_model_info",
          "tests\\unit\\test_model_selection.py:73-82:method:test_config_from_dict",
          "mcp_server\\server.py:1458-1560:decorated_definition:switch_embedding_model",
          "tests\\unit\\test_model_selection.py:188-208:class:TestModelDimensionValidation",
          "tests\\unit\\test_model_selection.py:151-157:method:test_model_config_detection_gemma",
          "mcp_server\\server.py:78-119:function:get_project_storage_dir"
        ],
        "unique_discoveries": [
          "tests\\unit\\test_model_selection.py:48-82:class:TestSearchConfigModelFields",
          "embeddings\\embedder.py:392-403:method:get_model_info",
          "tests\\unit\\test_model_selection.py:73-82:method:test_config_from_dict",
          "mcp_server\\server.py:1458-1560:decorated_definition:switch_embedding_model",
          "mcp_server\\server.py:78-119:function:get_project_storage_dir"
        ]
      },
      "comparison": {
        "time_overhead_ms": 39.47,
        "time_overhead_pct": 404.5,
        "top5_overlap_count": 2,
        "top5_overlap_pct": 40.0,
        "unique_discovery_count": 5,
        "value_rating": "HIGH"
      }
    },
    {
      "query": "multi-hop semantic search",
      "single_hop": {
        "time_ms": 19.05,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:118-178:method:test_multi_hop_basic_functionality",
            "score": 10.962,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 118,
              "end_line": 178,
              "name": "test_multi_hop_basic_functionality",
              "parent_name": "TestMultiHopSearchFlow",
              "docstring": "Test basic multi-hop search with 2 hops.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_hop_basic_functionality(self, test_project_path, mock_storage_dir):\n        \"\"\"Test basic multi-hop search with 2 hops.\"\"\"\n        # Setup: Chunk project\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        # Use subset for faster testing\n        test_chunks = all_chunks[:15]\n        assert len(test_chunks) >= 10, \"Need at least 10 chunks for multi-hop test\"\n\n        # Create embeddings (deterministic for speed)\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index the chunks\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(\n            storage_dir=str(storage_dir),\n            embedder=None,  # Don't need embedder for deterministic test\n            bm25_weight=0.4,\n            dense_weight=0.6,\n        )\n\n        # Index documents\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Test multi-hop search\n        query = \"authentication user login\"\n\n        # Single-hop search for comparison\n        single_hop_results = hybrid_searcher.search(\n            query=query,\n            k=3,\n            search_mode=\"hybrid\"\n        )\n\n        # Multi-hop search\n        multi_hop_results = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=3,\n            search_mode=\"hybrid\",\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        # Verify results\n        assert len(single_hop_results) > 0, \"Single-hop should return results\"\n        assert len(multi_hop_results) > 0, \"Multi-hop should return results\"\n\n        # Multi-hop should potentially find more related code\n        # (or at least same amount as single-hop)\n        assert len(multi_hop_results) >= len(single_hop_results) or True, \\\n            \"Multi-hop should discover related code\"",
              "content_preview": "def test_multi_hop_basic_functionality(self, test_project_path, mock_storage_dir):\n        \"\"\"Test basic multi-hop search with 2 hops.\"\"\"\n        # Setup: Chunk project\n        chunker = MultiLanguage...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016234796404019036,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:18-401:class:TestMultiHopSearchFlow",
            "score": 9.47,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "class",
              "start_line": 18,
              "end_line": 401,
              "name": "TestMultiHopSearchFlow",
              "parent_name": null,
              "docstring": "Integration tests for multi-hop semantic search.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestMultiHopSearchFlow:\n    \"\"\"Integration tests for multi-hop semantic search.\"\"\"\n\n    @pytest.fixture\n    def test_project_path(self):\n        \"\"\"Path to the test Python project.\"\"\"\n        return Path(__file__).parent.parent / \"test_data\" / \"python_project\"\n\n    @pytest.fixture\n    def mock_storage_dir(self):\n        \"\"\"Create a temporary storage directory for tests.\"\"\"\n        temp_dir = tempfile.mkdtemp()\n        yield temp_dir\n        shutil.rmtree(temp_dir, ignore_errors=True)\n\n    @pytest.fixture\n    def session_embedder(self):\n        \"\"\"Reusable embedder for all tests.\"\"\"\n        cache_dir = tempfile.mkdtemp()\n        embedder = CodeEmbedder(\n            model_name=\"google/embeddinggemma-300m\",\n            cache_dir=cache_dir\n        )\n        yield embedder\n        # Cleanup\n        shutil.rmtree(cache_dir, ignore_errors=True)\n\n    def _generate_chunk_id(self, chunk):\n        \"\"\"Generate chunk ID like the embedder does.\"\"\"\n        chunk_id = f\"{chunk.relative_path}:{chunk.start_line}-{chunk.end_line}:{chunk.chunk_type}\"\n        if chunk.name:\n            chunk_id += f\":{chunk.name}\"\n        return chunk_id\n\n    def _create_embeddings_from_chunks(self, chunks, embedder=None):\n        \"\"\"Create embeddings from chunks using deterministic approach or real embedder.\"\"\"\n        embeddings = []\n\n        # If real embedder provided, use it\n        if embedder:\n            texts = [chunk.content for chunk in chunks]\n            chunk_ids = [self._generate_chunk_id(chunk) for chunk in chunks]\n\n            # Use embedder to generate real embeddings\n            embed_results = embedder.embed_batch(\n                texts=texts,\n                chunk_ids=chunk_ids,\n                metadata=[{\n                    \"name\": chunk.name,\n                    \"chunk_type\": chunk.chunk_type,\n                    \"file_path\": chunk.file_path,\n                    \"relative_path\": chunk.relative_path,\n                    \"folder_structure\": chunk.folder_structure,\n                    \"start_line\": chunk.start_line,\n                    \"end_line\": chunk.end_line,\n                    \"docstring\": chunk.docstring,\n                    \"tags\": chunk.tags,\n                    \"complexity_score\": chunk.complexity_score,\n                    \"content_preview\": (\n                        chunk.content[:200] + \"...\"\n                        if len(chunk.content) > 200\n                        else chunk.content\n                    ),\n                } for chunk in chunks]\n            )\n            return embed_results\n\n        # Otherwise use deterministic embeddings for fast tests\n        for chunk in chunks:\n            content_hash = abs(hash(chunk.content)) % 10000\n            embedding = (\n                np.random.RandomState(content_hash).random(768).astype(np.float32)\n            )\n\n            chunk_id = self._generate_chunk_id(chunk)\n            metadata = {\n                \"name\": chunk.name,\n                \"chunk_type\": chunk.chunk_type,\n                \"file_path\": chunk.file_path,\n                \"relative_path\": chunk.relative_path,\n                \"folder_structure\": chunk.folder_structure,\n                \"start_line\": chunk.start_line,\n                \"end_line\": chunk.end_line,\n                \"docstring\": chunk.docstring,\n                \"tags\": chunk.tags,\n                \"complexity_score\": chunk.complexity_score,\n                \"content_preview\": (\n                    chunk.content[:200] + \"...\"\n                    if len(chunk.content) > 200\n                    else chunk.content\n                ),\n            }\n\n            result = EmbeddingResult(\n                embedding=embedding, chunk_id=chunk_id, metadata=metadata\n            )\n            embeddings.append(result)\n\n        return embeddings\n\n    def test_multi_hop_basic_functionality(self, test_project_path, mock_storage_dir):\n        \"\"\"Test basic multi-hop search with 2 hops.\"\"\"\n        # Setup: Chunk project\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        # Use subset for faster testing\n        test_chunks = all_chunks[:15]\n        assert len(test_chunks) >= 10, \"Need at least 10 chunks for multi-hop test\"\n\n        # Create embeddings (deterministic for speed)\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index the chunks\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(\n            storage_dir=str(storage_dir),\n            embedder=None,  # Don't need embedder for deterministic test\n            bm25_weight=0.4,\n            dense_weight=0.6,\n        )\n\n        # Index documents\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Test multi-hop search\n        query = \"authentication user login\"\n\n        # Single-hop search for comparison\n        single_hop_results = hybrid_searcher.search(\n            query=query,\n            k=3,\n            search_mode=\"hybrid\"\n        )\n\n        # Multi-hop search\n        multi_hop_results = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=3,\n            search_mode=\"hybrid\",\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        # Verify results\n        assert len(single_hop_results) > 0, \"Single-hop should return results\"\n        assert len(multi_hop_results) > 0, \"Multi-hop should return results\"\n\n        # Multi-hop should potentially find more related code\n        # (or at least same amount as single-hop)\n        assert len(multi_hop_results) >= len(single_hop_results) or True, \\\n            \"Multi-hop should discover related code\"\n\n    def test_multi_hop_expansion_factor(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that expansion factor affects number of discovered chunks.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:20]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(\n            storage_dir=str(storage_dir),\n            embedder=None,\n            bm25_weight=0.4,\n            dense_weight=0.6,\n        )\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Test different expansion factors\n        query = \"database connection\"\n        k = 5\n\n        # Low expansion\n        low_expansion_results = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=2,\n            expansion_factor=0.2\n        )\n\n        # High expansion\n        high_expansion_results = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=2,\n            expansion_factor=0.8\n        )\n\n        # Both should return results\n        assert len(low_expansion_results) > 0\n        assert len(high_expansion_results) > 0\n\n        # Results should be properly ranked (scores descending)\n        for i in range(len(low_expansion_results) - 1):\n            assert low_expansion_results[i].score >= \\\n                   low_expansion_results[i + 1].score\n\n    def test_multi_hop_hop_count(self, test_project_path, mock_storage_dir):\n        \"\"\"Test multi-hop search with different hop counts.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(\n            storage_dir=str(storage_dir),\n            embedder=None,\n        )\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Test different hop counts\n        query = \"user authentication\"\n        k = 3\n\n        # 1 hop (should be same as regular search)\n        results_1_hop = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=1,\n            expansion_factor=0.3\n        )\n\n        # 2 hops (default)\n        results_2_hops = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        # 3 hops\n        results_3_hops = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=3,\n            expansion_factor=0.3\n        )\n\n        # All should return results\n        assert len(results_1_hop) > 0\n        assert len(results_2_hops) > 0\n        assert len(results_3_hops) > 0\n\n        # Verify results are properly structured\n        for result in results_2_hops:\n            assert hasattr(result, 'doc_id')\n            assert hasattr(result, 'score')\n            assert hasattr(result, 'metadata')\n\n    def test_multi_hop_config_integration(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop respects configuration settings.\"\"\"\n        # Create test config with multi-hop enabled\n        config = SearchConfig(\n            enable_multi_hop=True,\n            multi_hop_count=2,\n            multi_hop_expansion=0.3\n        )\n\n        # Verify config values\n        assert config.enable_multi_hop is True\n        assert config.multi_hop_count == 2\n        assert config.multi_hop_expansion == 0.3\n\n        # Test config with multi-hop disabled\n        config_disabled = SearchConfig(\n            enable_multi_hop=False\n        )\n\n        assert config_disabled.enable_multi_hop is False\n\n    def test_multi_hop_deduplication(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly deduplicates results.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(storage_dir=str(storage_dir))\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Multi-hop search\n        results = hybrid_searcher.multi_hop_search(\n            query=\"database query\",\n            k=5,\n            hops=2,\n            expansion_factor=0.5\n        )\n\n        # Verify no duplicate doc_ids\n        doc_ids = [r.doc_id for r in results]\n        unique_doc_ids = set(doc_ids)\n\n        assert len(doc_ids) == len(unique_doc_ids), \\\n            \"Multi-hop should deduplicate results\"\n\n    def test_multi_hop_reranking(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly re-ranks results by query relevance.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(storage_dir=str(storage_dir))\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Multi-hop search\n        results = hybrid_searcher.multi_hop_search(\n            query=\"user authentication validation\",\n            k=5,\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        assert len(results) > 0\n\n        # Verify results are sorted by score (descending)\n        for i in range(len(results) - 1):\n            assert results[i].score >= results[i + 1].score, \\\n                \"Results should be sorted by relevance score\"",
              "content_preview": "class TestMultiHopSearchFlow:\n    \"\"\"Integration tests for multi-hop semantic search.\"\"\"\n\n    @pytest.fixture\n    def test_project_path(self):\n        \"\"\"Path to the test Python project.\"\"\"\n        re...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01572420634920635,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:304-323:method:test_multi_hop_config_integration",
            "score": 10.513,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 304,
              "end_line": 323,
              "name": "test_multi_hop_config_integration",
              "parent_name": "TestMultiHopSearchFlow",
              "docstring": "Test that multi-hop respects configuration settings.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_hop_config_integration(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop respects configuration settings.\"\"\"\n        # Create test config with multi-hop enabled\n        config = SearchConfig(\n            enable_multi_hop=True,\n            multi_hop_count=2,\n            multi_hop_expansion=0.3\n        )\n\n        # Verify config values\n        assert config.enable_multi_hop is True\n        assert config.multi_hop_count == 2\n        assert config.multi_hop_expansion == 0.3\n\n        # Test config with multi-hop disabled\n        config_disabled = SearchConfig(\n            enable_multi_hop=False\n        )\n\n        assert config_disabled.enable_multi_hop is False",
              "content_preview": "def test_multi_hop_config_integration(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop respects configuration settings.\"\"\"\n        # Create test config with multi-hop enabled...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01540683678382282,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:364-401:method:test_multi_hop_reranking",
            "score": 8.213,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 364,
              "end_line": 401,
              "name": "test_multi_hop_reranking",
              "parent_name": "TestMultiHopSearchFlow",
              "docstring": "Test that multi-hop properly re-ranks results by query relevance.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_hop_reranking(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly re-ranks results by query relevance.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(storage_dir=str(storage_dir))\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Multi-hop search\n        results = hybrid_searcher.multi_hop_search(\n            query=\"user authentication validation\",\n            k=5,\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        assert len(results) > 0\n\n        # Verify results are sorted by score (descending)\n        for i in range(len(results) - 1):\n            assert results[i].score >= results[i + 1].score, \\\n                \"Results should be sorted by relevance score\"",
              "content_preview": "def test_multi_hop_reranking(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly re-ranks results by query relevance.\"\"\"\n        # Setup\n        chunker = MultiLanguage...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015384615384615385,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          },
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:325-362:method:test_multi_hop_deduplication",
            "score": 9.266,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 325,
              "end_line": 362,
              "name": "test_multi_hop_deduplication",
              "parent_name": "TestMultiHopSearchFlow",
              "docstring": "Test that multi-hop properly deduplicates results.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_hop_deduplication(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly deduplicates results.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(storage_dir=str(storage_dir))\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Multi-hop search\n        results = hybrid_searcher.multi_hop_search(\n            query=\"database query\",\n            k=5,\n            hops=2,\n            expansion_factor=0.5\n        )\n\n        # Verify no duplicate doc_ids\n        doc_ids = [r.doc_id for r in results]\n        unique_doc_ids = set(doc_ids)\n\n        assert len(doc_ids) == len(unique_doc_ids), \\\n            \"Multi-hop should deduplicate results\"",
              "content_preview": "def test_multi_hop_deduplication(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly deduplicates results.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01534090909090909,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          }
        ],
        "all_doc_ids": [
          "tests\\integration\\test_semantic_search.py:11-56:function:test_semantic_search",
          "tests\\integration\\test_multi_hop_flow.py:325-362:method:test_multi_hop_deduplication",
          "tests\\integration\\test_multi_hop_flow.py:118-178:method:test_multi_hop_basic_functionality",
          "mcp_server\\server.py:337-638:decorated_definition:search_code",
          "tests\\integration\\test_multi_hop_flow.py:304-323:method:test_multi_hop_config_integration",
          "search\\searcher.py:97-121:method:search",
          "search\\hybrid_searcher.py:467-604:method:multi_hop_search",
          "search\\config.py:30-98:decorated_definition:SearchConfig",
          "tests\\integration\\test_multi_hop_flow.py:18-401:class:TestMultiHopSearchFlow",
          "tests\\integration\\test_multi_hop_flow.py:364-401:method:test_multi_hop_reranking"
        ]
      },
      "multi_hop": {
        "time_ms": 42.39,
        "result_count": 10,
        "top_5": [
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:118-178:method:test_multi_hop_basic_functionality",
            "score": 10.962,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 118,
              "end_line": 178,
              "name": "test_multi_hop_basic_functionality",
              "parent_name": "TestMultiHopSearchFlow",
              "docstring": "Test basic multi-hop search with 2 hops.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_hop_basic_functionality(self, test_project_path, mock_storage_dir):\n        \"\"\"Test basic multi-hop search with 2 hops.\"\"\"\n        # Setup: Chunk project\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        # Use subset for faster testing\n        test_chunks = all_chunks[:15]\n        assert len(test_chunks) >= 10, \"Need at least 10 chunks for multi-hop test\"\n\n        # Create embeddings (deterministic for speed)\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index the chunks\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(\n            storage_dir=str(storage_dir),\n            embedder=None,  # Don't need embedder for deterministic test\n            bm25_weight=0.4,\n            dense_weight=0.6,\n        )\n\n        # Index documents\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Test multi-hop search\n        query = \"authentication user login\"\n\n        # Single-hop search for comparison\n        single_hop_results = hybrid_searcher.search(\n            query=query,\n            k=3,\n            search_mode=\"hybrid\"\n        )\n\n        # Multi-hop search\n        multi_hop_results = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=3,\n            search_mode=\"hybrid\",\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        # Verify results\n        assert len(single_hop_results) > 0, \"Single-hop should return results\"\n        assert len(multi_hop_results) > 0, \"Multi-hop should return results\"\n\n        # Multi-hop should potentially find more related code\n        # (or at least same amount as single-hop)\n        assert len(multi_hop_results) >= len(single_hop_results) or True, \\\n            \"Multi-hop should discover related code\"",
              "content_preview": "def test_multi_hop_basic_functionality(self, test_project_path, mock_storage_dir):\n        \"\"\"Test basic multi-hop search with 2 hops.\"\"\"\n        # Setup: Chunk project\n        chunker = MultiLanguage...",
              "project_name": "claude-context-local",
              "rrf_score": 0.016234796404019036,
              "appears_in_lists": 2,
              "final_rank": 1
            }
          },
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:304-323:method:test_multi_hop_config_integration",
            "score": 10.513,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 304,
              "end_line": 323,
              "name": "test_multi_hop_config_integration",
              "parent_name": "TestMultiHopSearchFlow",
              "docstring": "Test that multi-hop respects configuration settings.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_hop_config_integration(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop respects configuration settings.\"\"\"\n        # Create test config with multi-hop enabled\n        config = SearchConfig(\n            enable_multi_hop=True,\n            multi_hop_count=2,\n            multi_hop_expansion=0.3\n        )\n\n        # Verify config values\n        assert config.enable_multi_hop is True\n        assert config.multi_hop_count == 2\n        assert config.multi_hop_expansion == 0.3\n\n        # Test config with multi-hop disabled\n        config_disabled = SearchConfig(\n            enable_multi_hop=False\n        )\n\n        assert config_disabled.enable_multi_hop is False",
              "content_preview": "def test_multi_hop_config_integration(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop respects configuration settings.\"\"\"\n        # Create test config with multi-hop enabled...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01540683678382282,
              "appears_in_lists": 2,
              "final_rank": 3
            }
          },
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:18-401:class:TestMultiHopSearchFlow",
            "score": 9.47,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "class",
              "start_line": 18,
              "end_line": 401,
              "name": "TestMultiHopSearchFlow",
              "parent_name": null,
              "docstring": "Integration tests for multi-hop semantic search.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "class TestMultiHopSearchFlow:\n    \"\"\"Integration tests for multi-hop semantic search.\"\"\"\n\n    @pytest.fixture\n    def test_project_path(self):\n        \"\"\"Path to the test Python project.\"\"\"\n        return Path(__file__).parent.parent / \"test_data\" / \"python_project\"\n\n    @pytest.fixture\n    def mock_storage_dir(self):\n        \"\"\"Create a temporary storage directory for tests.\"\"\"\n        temp_dir = tempfile.mkdtemp()\n        yield temp_dir\n        shutil.rmtree(temp_dir, ignore_errors=True)\n\n    @pytest.fixture\n    def session_embedder(self):\n        \"\"\"Reusable embedder for all tests.\"\"\"\n        cache_dir = tempfile.mkdtemp()\n        embedder = CodeEmbedder(\n            model_name=\"google/embeddinggemma-300m\",\n            cache_dir=cache_dir\n        )\n        yield embedder\n        # Cleanup\n        shutil.rmtree(cache_dir, ignore_errors=True)\n\n    def _generate_chunk_id(self, chunk):\n        \"\"\"Generate chunk ID like the embedder does.\"\"\"\n        chunk_id = f\"{chunk.relative_path}:{chunk.start_line}-{chunk.end_line}:{chunk.chunk_type}\"\n        if chunk.name:\n            chunk_id += f\":{chunk.name}\"\n        return chunk_id\n\n    def _create_embeddings_from_chunks(self, chunks, embedder=None):\n        \"\"\"Create embeddings from chunks using deterministic approach or real embedder.\"\"\"\n        embeddings = []\n\n        # If real embedder provided, use it\n        if embedder:\n            texts = [chunk.content for chunk in chunks]\n            chunk_ids = [self._generate_chunk_id(chunk) for chunk in chunks]\n\n            # Use embedder to generate real embeddings\n            embed_results = embedder.embed_batch(\n                texts=texts,\n                chunk_ids=chunk_ids,\n                metadata=[{\n                    \"name\": chunk.name,\n                    \"chunk_type\": chunk.chunk_type,\n                    \"file_path\": chunk.file_path,\n                    \"relative_path\": chunk.relative_path,\n                    \"folder_structure\": chunk.folder_structure,\n                    \"start_line\": chunk.start_line,\n                    \"end_line\": chunk.end_line,\n                    \"docstring\": chunk.docstring,\n                    \"tags\": chunk.tags,\n                    \"complexity_score\": chunk.complexity_score,\n                    \"content_preview\": (\n                        chunk.content[:200] + \"...\"\n                        if len(chunk.content) > 200\n                        else chunk.content\n                    ),\n                } for chunk in chunks]\n            )\n            return embed_results\n\n        # Otherwise use deterministic embeddings for fast tests\n        for chunk in chunks:\n            content_hash = abs(hash(chunk.content)) % 10000\n            embedding = (\n                np.random.RandomState(content_hash).random(768).astype(np.float32)\n            )\n\n            chunk_id = self._generate_chunk_id(chunk)\n            metadata = {\n                \"name\": chunk.name,\n                \"chunk_type\": chunk.chunk_type,\n                \"file_path\": chunk.file_path,\n                \"relative_path\": chunk.relative_path,\n                \"folder_structure\": chunk.folder_structure,\n                \"start_line\": chunk.start_line,\n                \"end_line\": chunk.end_line,\n                \"docstring\": chunk.docstring,\n                \"tags\": chunk.tags,\n                \"complexity_score\": chunk.complexity_score,\n                \"content_preview\": (\n                    chunk.content[:200] + \"...\"\n                    if len(chunk.content) > 200\n                    else chunk.content\n                ),\n            }\n\n            result = EmbeddingResult(\n                embedding=embedding, chunk_id=chunk_id, metadata=metadata\n            )\n            embeddings.append(result)\n\n        return embeddings\n\n    def test_multi_hop_basic_functionality(self, test_project_path, mock_storage_dir):\n        \"\"\"Test basic multi-hop search with 2 hops.\"\"\"\n        # Setup: Chunk project\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        # Use subset for faster testing\n        test_chunks = all_chunks[:15]\n        assert len(test_chunks) >= 10, \"Need at least 10 chunks for multi-hop test\"\n\n        # Create embeddings (deterministic for speed)\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index the chunks\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(\n            storage_dir=str(storage_dir),\n            embedder=None,  # Don't need embedder for deterministic test\n            bm25_weight=0.4,\n            dense_weight=0.6,\n        )\n\n        # Index documents\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Test multi-hop search\n        query = \"authentication user login\"\n\n        # Single-hop search for comparison\n        single_hop_results = hybrid_searcher.search(\n            query=query,\n            k=3,\n            search_mode=\"hybrid\"\n        )\n\n        # Multi-hop search\n        multi_hop_results = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=3,\n            search_mode=\"hybrid\",\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        # Verify results\n        assert len(single_hop_results) > 0, \"Single-hop should return results\"\n        assert len(multi_hop_results) > 0, \"Multi-hop should return results\"\n\n        # Multi-hop should potentially find more related code\n        # (or at least same amount as single-hop)\n        assert len(multi_hop_results) >= len(single_hop_results) or True, \\\n            \"Multi-hop should discover related code\"\n\n    def test_multi_hop_expansion_factor(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that expansion factor affects number of discovered chunks.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:20]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(\n            storage_dir=str(storage_dir),\n            embedder=None,\n            bm25_weight=0.4,\n            dense_weight=0.6,\n        )\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Test different expansion factors\n        query = \"database connection\"\n        k = 5\n\n        # Low expansion\n        low_expansion_results = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=2,\n            expansion_factor=0.2\n        )\n\n        # High expansion\n        high_expansion_results = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=2,\n            expansion_factor=0.8\n        )\n\n        # Both should return results\n        assert len(low_expansion_results) > 0\n        assert len(high_expansion_results) > 0\n\n        # Results should be properly ranked (scores descending)\n        for i in range(len(low_expansion_results) - 1):\n            assert low_expansion_results[i].score >= \\\n                   low_expansion_results[i + 1].score\n\n    def test_multi_hop_hop_count(self, test_project_path, mock_storage_dir):\n        \"\"\"Test multi-hop search with different hop counts.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(\n            storage_dir=str(storage_dir),\n            embedder=None,\n        )\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Test different hop counts\n        query = \"user authentication\"\n        k = 3\n\n        # 1 hop (should be same as regular search)\n        results_1_hop = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=1,\n            expansion_factor=0.3\n        )\n\n        # 2 hops (default)\n        results_2_hops = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        # 3 hops\n        results_3_hops = hybrid_searcher.multi_hop_search(\n            query=query,\n            k=k,\n            hops=3,\n            expansion_factor=0.3\n        )\n\n        # All should return results\n        assert len(results_1_hop) > 0\n        assert len(results_2_hops) > 0\n        assert len(results_3_hops) > 0\n\n        # Verify results are properly structured\n        for result in results_2_hops:\n            assert hasattr(result, 'doc_id')\n            assert hasattr(result, 'score')\n            assert hasattr(result, 'metadata')\n\n    def test_multi_hop_config_integration(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop respects configuration settings.\"\"\"\n        # Create test config with multi-hop enabled\n        config = SearchConfig(\n            enable_multi_hop=True,\n            multi_hop_count=2,\n            multi_hop_expansion=0.3\n        )\n\n        # Verify config values\n        assert config.enable_multi_hop is True\n        assert config.multi_hop_count == 2\n        assert config.multi_hop_expansion == 0.3\n\n        # Test config with multi-hop disabled\n        config_disabled = SearchConfig(\n            enable_multi_hop=False\n        )\n\n        assert config_disabled.enable_multi_hop is False\n\n    def test_multi_hop_deduplication(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly deduplicates results.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(storage_dir=str(storage_dir))\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Multi-hop search\n        results = hybrid_searcher.multi_hop_search(\n            query=\"database query\",\n            k=5,\n            hops=2,\n            expansion_factor=0.5\n        )\n\n        # Verify no duplicate doc_ids\n        doc_ids = [r.doc_id for r in results]\n        unique_doc_ids = set(doc_ids)\n\n        assert len(doc_ids) == len(unique_doc_ids), \\\n            \"Multi-hop should deduplicate results\"\n\n    def test_multi_hop_reranking(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly re-ranks results by query relevance.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(storage_dir=str(storage_dir))\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Multi-hop search\n        results = hybrid_searcher.multi_hop_search(\n            query=\"user authentication validation\",\n            k=5,\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        assert len(results) > 0\n\n        # Verify results are sorted by score (descending)\n        for i in range(len(results) - 1):\n            assert results[i].score >= results[i + 1].score, \\\n                \"Results should be sorted by relevance score\"",
              "content_preview": "class TestMultiHopSearchFlow:\n    \"\"\"Integration tests for multi-hop semantic search.\"\"\"\n\n    @pytest.fixture\n    def test_project_path(self):\n        \"\"\"Path to the test Python project.\"\"\"\n        re...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01572420634920635,
              "appears_in_lists": 2,
              "final_rank": 2
            }
          },
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:325-362:method:test_multi_hop_deduplication",
            "score": 9.266,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 325,
              "end_line": 362,
              "name": "test_multi_hop_deduplication",
              "parent_name": "TestMultiHopSearchFlow",
              "docstring": "Test that multi-hop properly deduplicates results.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_hop_deduplication(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly deduplicates results.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(storage_dir=str(storage_dir))\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Multi-hop search\n        results = hybrid_searcher.multi_hop_search(\n            query=\"database query\",\n            k=5,\n            hops=2,\n            expansion_factor=0.5\n        )\n\n        # Verify no duplicate doc_ids\n        doc_ids = [r.doc_id for r in results]\n        unique_doc_ids = set(doc_ids)\n\n        assert len(doc_ids) == len(unique_doc_ids), \\\n            \"Multi-hop should deduplicate results\"",
              "content_preview": "def test_multi_hop_deduplication(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly deduplicates results.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str...",
              "project_name": "claude-context-local",
              "rrf_score": 0.01534090909090909,
              "appears_in_lists": 2,
              "final_rank": 5
            }
          },
          {
            "doc_id": "tests\\integration\\test_multi_hop_flow.py:364-401:method:test_multi_hop_reranking",
            "score": 8.213,
            "metadata": {
              "file_path": "F:\\RD_PROJECTS\\COMPONENTS\\claude-context-local\\tests\\integration\\test_multi_hop_flow.py",
              "relative_path": "tests\\integration\\test_multi_hop_flow.py",
              "folder_structure": [
                "tests",
                "integration"
              ],
              "chunk_type": "method",
              "start_line": 364,
              "end_line": 401,
              "name": "test_multi_hop_reranking",
              "parent_name": "TestMultiHopSearchFlow",
              "docstring": "Test that multi-hop properly re-ranks results by query relevance.",
              "decorators": [],
              "imports": [],
              "complexity_score": 0,
              "tags": [
                "python"
              ],
              "content": "def test_multi_hop_reranking(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly re-ranks results by query relevance.\"\"\"\n        # Setup\n        chunker = MultiLanguageChunker(str(test_project_path))\n        all_chunks = []\n\n        for py_file in test_project_path.rglob(\"*.py\"):\n            chunks = chunker.chunk_file(str(py_file))\n            all_chunks.extend(chunks)\n\n        test_chunks = all_chunks[:15]\n        embeddings = self._create_embeddings_from_chunks(test_chunks)\n\n        # Index\n        storage_dir = Path(mock_storage_dir)\n        hybrid_searcher = HybridSearcher(storage_dir=str(storage_dir))\n\n        documents = [chunk.content for chunk in test_chunks]\n        doc_ids = [emb.chunk_id for emb in embeddings]\n        embeddings_list = [emb.embedding.tolist() for emb in embeddings]\n        metadata = {emb.chunk_id: emb.metadata for emb in embeddings}\n\n        hybrid_searcher.index_documents(documents, doc_ids, embeddings_list, metadata)\n\n        # Multi-hop search\n        results = hybrid_searcher.multi_hop_search(\n            query=\"user authentication validation\",\n            k=5,\n            hops=2,\n            expansion_factor=0.3\n        )\n\n        assert len(results) > 0\n\n        # Verify results are sorted by score (descending)\n        for i in range(len(results) - 1):\n            assert results[i].score >= results[i + 1].score, \\\n                \"Results should be sorted by relevance score\"",
              "content_preview": "def test_multi_hop_reranking(self, test_project_path, mock_storage_dir):\n        \"\"\"Test that multi-hop properly re-ranks results by query relevance.\"\"\"\n        # Setup\n        chunker = MultiLanguage...",
              "project_name": "claude-context-local",
              "rrf_score": 0.015384615384615385,
              "appears_in_lists": 2,
              "final_rank": 4
            }
          }
        ],
        "all_doc_ids": [
          "tests\\integration\\test_semantic_search.py:11-56:function:test_semantic_search",
          "tests\\integration\\test_multi_hop_flow.py:325-362:method:test_multi_hop_deduplication",
          "tests\\integration\\test_multi_hop_flow.py:118-178:method:test_multi_hop_basic_functionality",
          "mcp_server\\server.py:337-638:decorated_definition:search_code",
          "tests\\integration\\test_multi_hop_flow.py:304-323:method:test_multi_hop_config_integration",
          "search\\searcher.py:97-121:method:search",
          "tests\\unit\\test_token_efficiency.py:300-319:decorated_definition:test_search",
          "search\\config.py:30-98:decorated_definition:SearchConfig",
          "tests\\integration\\test_multi_hop_flow.py:18-401:class:TestMultiHopSearchFlow",
          "tests\\integration\\test_multi_hop_flow.py:364-401:method:test_multi_hop_reranking"
        ],
        "unique_discoveries": [
          "tests\\unit\\test_token_efficiency.py:300-319:decorated_definition:test_search"
        ]
      },
      "comparison": {
        "time_overhead_ms": 23.34,
        "time_overhead_pct": 122.5,
        "top5_overlap_count": 5,
        "top5_overlap_pct": 100.0,
        "unique_discovery_count": 1,
        "value_rating": "LOW"
      }
    }
  ],
  "aggregate": {
    "total_queries": 15,
    "avg_single_time_ms": 172.93,
    "avg_multi_time_ms": 47.04,
    "avg_overhead_ms": -125.89,
    "avg_overhead_pct": 144.0,
    "avg_unique_discoveries": 3.2,
    "avg_top5_overlap": 3.0,
    "queries_with_benefits": 14,
    "queries_with_benefits_pct": 93.3,
    "value_distribution": {
      "HIGH": 5,
      "MEDIUM": 7,
      "LOW": 2,
      "NONE": 1
    }
  }
}